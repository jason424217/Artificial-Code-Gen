
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/finetuning_gpt2.ipynb

from exp.nb_embedding import generate_embeddings_from_files, generate_embeddings_from_list, generate_embeddings_from_text_files
from tensorflow.keras import layers
import pickle
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf


tf.__version__

def prepare_dataset(pickle_path, MAX_LEN = 1024):
    with open(pickle_path, 'rb') as f:
            features = pickle.load(f)

    features = np.asarray(features)
    features = tf.keras.preprocessing.sequence.pad_sequences(features, MAX_LEN)
    return features

def get_model(input_shape):
    model = tf.keras.Sequential()
#     model.add(layers.Flatten(input_shape = input_shape))
    model.add(layers.Dropout(0.1))
    model.add(layers.Dense(2, activation = "softmax"))

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def get_gru(input_shape):
    # 1D convolution with 64 output channels (filters) and five kernel size
    model = tf.keras.Sequential()
#     model.add(layers.Flatten(input_shape = input_shape))
    model.add(layers.Conv1D(64, 5))
#     x = Conv1D(64, 5)(embedded_sequences)
    # MaxPool divides the length of the sequence by 5
    model.add(layers.MaxPooling1D(5))
    model.add(layers.Conv1D(64, 5))
    model.add(layers.MaxPooling1D(5))
    # LSTM layer with a hidden size of 64
    model.add(layers.GRU(64))

    #Regularization
    model.add(layers.Dropout(0.5))
    model.add(Dense(2, activation='softmax'))

    model.compile(
        optimizer='adam',
        loss='categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

def finetune_model(x, y, val_x, val_y, model, callbacks, class_weight, epochs = 100, bs = 128):
    history = model.fit(
        x, y,
        epochs = epochs,
        batch_size = bs,
        validation_data = (val_x, val_y),
        callbacks = callbacks,
        class_weight = class_weight
    )

    return history

def evaluate_model(history):
    #Evaluation
    acc = history['accuracy']
    val_acc = history['val_accuracy']
    loss = history['loss']
    val_loss = history['val_loss']

    epochs2 = range(len(acc))

    plt.plot(epochs2, acc, 'b', label='Training')
    plt.plot(epochs2, val_acc, 'r', label='Validation')
    plt.title('Training and validation accuracy')
    plt.ylabel('acc')
    plt.xlabel('epoch')
    plt.legend()

    plt.figure()

    plt.plot(epochs2, loss, 'b', label='Training')
    plt.plot(epochs2, val_loss, 'r', label='Validation')
    plt.title('Training and validation loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend()

    plt.show()
