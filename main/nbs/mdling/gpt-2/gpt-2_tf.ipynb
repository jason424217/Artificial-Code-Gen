{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/src/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd /tf/src/data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
      "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
      "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
      "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 download_model.py 117M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from statistics import median\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1)\n",
    "        ]\n",
    "\n",
    "    past, prev, output = body(None, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "    _, _, tokens = tf.while_loop(\n",
    "        cond=cond, body=body,\n",
    "        maximum_iterations=length - 1,\n",
    "        loop_vars=[\n",
    "            past,\n",
    "            prev,\n",
    "            output\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "        ],\n",
    "        back_prop=False,\n",
    "    )\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "        \n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "#         if i >= 10000: break\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "                raw_text += '<|endoftext|>'\n",
    "            tokens = np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(enc, ds):\n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, method in enumerate(tqdm.tqdm(ds)):\n",
    "        try:\n",
    "            raw_text += method\n",
    "            raw_text += '<|endoftext|>'\n",
    "            tokens = np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    return token_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, trn_dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every, pretrained, iterations):\n",
    "        self.trn_dataset = trn_dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every\n",
    "        self.pretrained = pretrained\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                trn_dataset=\"/tf/src/data/datasets/security-training.csv\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1, # DO NOT TOUCH. INCREASING THIS WILL RAIN DOWN HELL FIRE ONTO YOUR COMPUTER.\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=40,\n",
    "                top_p=0.0,\n",
    "                run_name=\"m1_vulnerability\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=\"/tf/src/data/datasets/security-validation.csv\",\n",
    "                val_batch_size=1,\n",
    "                val_batch_count=40,\n",
    "                val_every=100,\n",
    "                pretrained=True,\n",
    "                iterations=100000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 729/972771 [00:13<6:50:51, 39.43it/s]"
     ]
    }
   ],
   "source": [
    "# enc = get_encoder(args.model_name, \"models\")\n",
    "# trn_set = load_dataset(enc, args.trn_dataset)\n",
    "# val_set = load_dataset(enc, args.val_dataset)\n",
    "# len(trn_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 781086/1019471 [21:16<06:15, 634.11it/s]"
     ]
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "trn_set = pd.read_csv(args.trn_dataset)['code']\n",
    "val_set = pd.read_csv(args.val_dataset)['code']\n",
    "trn_set = prepare_dataset(enc, x_trn)\n",
    "val_set = prepare_dataset(enc, x_val)\n",
    "len(trn_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1019471, 127476)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_set = x_trn\n",
    "val_set = x_val\n",
    "len(trn_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished loading dataset\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET_SIZE = len(dataset)\n",
    "# TRN_SET_SIZE = int(DATASET_SIZE * 0.8)\n",
    "# VAL_SET_SIZE = int(DATASET_SIZE * 0.1)\n",
    "# TST_SET_SIZE = int(DATASET_SIZE * 0.1)\n",
    "\n",
    "# trn_set = dataset[:TRN_SET_SIZE]\n",
    "# val_set = dataset[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "# tst_set = dataset[-TST_SET_SIZE:]\n",
    "# DATASET_SIZE, len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "SAMPLE_DIR = 'samples'\n",
    "\n",
    "trn_losses = [0.0]\n",
    "trn_avgs   = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore previous metrics \n",
    "with open(os.path.join(CHECKPOINT_DIR, args.run_name, 'metrics.pickle'), 'rb') as f:\n",
    "    loss_dict = pickle.load(f)\n",
    "    \n",
    "trn_losses = loss_dict[\"trn_losses\"]\n",
    "trn_avgs   = loss_dict[\"avg_trn_losses\"]\n",
    "val_losses = loss_dict[\"val_losses\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0, 0)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trn_losses), len(trn_avgs), len(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/m1/model-1000000\n",
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 390748987 tokens\n",
      "Training...\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:02<00:00, 16.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 | 3.13] validation loss = 3.70\n",
      "[1 | 7.76] loss=2.12 avg=1.93\n",
      "[2 | 7.82] loss=3.28 avg=2.57\n",
      "[3 | 7.87] loss=3.32 avg=2.82\n",
      "[4 | 7.93] loss=5.84 avg=3.56\n",
      "[5 | 7.99] loss=6.16 avg=4.08\n",
      "[6 | 8.03] loss=6.14 avg=4.43\n",
      "[7 | 8.08] loss=2.76 avg=4.19\n",
      "[8 | 8.13] loss=3.66 avg=4.12\n",
      "[9 | 8.18] loss=7.47 avg=4.50\n",
      "[10 | 8.23] loss=5.61 avg=4.62\n",
      "[11 | 8.28] loss=4.17 avg=4.58\n",
      "[12 | 8.33] loss=1.14 avg=4.28\n",
      "[13 | 8.38] loss=6.39 avg=4.45\n",
      "[14 | 8.43] loss=4.90 avg=4.48\n",
      "[15 | 8.48] loss=2.56 avg=4.35\n",
      "[16 | 8.53] loss=1.03 avg=4.12\n",
      "[17 | 8.58] loss=1.73 avg=3.97\n",
      "[18 | 8.63] loss=5.16 avg=4.04\n",
      "[19 | 8.68] loss=1.71 avg=3.91\n",
      "[20 | 8.73] loss=1.13 avg=3.76\n",
      "[21 | 8.78] loss=5.71 avg=3.86\n",
      "[22 | 8.83] loss=3.14 avg=3.82\n",
      "[23 | 8.88] loss=2.63 avg=3.77\n",
      "[24 | 8.94] loss=3.95 avg=3.77\n",
      "[25 | 8.99] loss=4.43 avg=3.80\n",
      "[26 | 9.04] loss=4.41 avg=3.83\n",
      "[27 | 9.09] loss=1.46 avg=3.73\n",
      "[28 | 9.14] loss=1.45 avg=3.64\n",
      "[29 | 9.19] loss=5.33 avg=3.70\n",
      "[30 | 9.24] loss=1.34 avg=3.61\n",
      "[31 | 9.29] loss=1.37 avg=3.53\n",
      "[32 | 9.34] loss=4.91 avg=3.58\n",
      "[33 | 9.40] loss=5.52 avg=3.65\n",
      "[34 | 9.45] loss=6.59 avg=3.75\n",
      "[35 | 9.50] loss=5.20 avg=3.80\n",
      "[36 | 9.55] loss=1.26 avg=3.72\n",
      "[37 | 9.60] loss=1.40 avg=3.64\n",
      "[38 | 9.65] loss=1.95 avg=3.59\n",
      "[39 | 9.70] loss=3.64 avg=3.59\n",
      "[40 | 9.75] loss=2.12 avg=3.55\n",
      "[41 | 9.81] loss=3.66 avg=3.55\n",
      "[42 | 9.86] loss=1.83 avg=3.50\n",
      "[43 | 9.91] loss=2.58 avg=3.47\n",
      "[44 | 9.96] loss=5.59 avg=3.53\n",
      "[45 | 10.01] loss=5.60 avg=3.59\n",
      "[46 | 10.06] loss=5.25 avg=3.63\n",
      "[47 | 10.12] loss=1.10 avg=3.57\n",
      "[48 | 10.17] loss=2.27 avg=3.53\n",
      "[49 | 10.22] loss=1.32 avg=3.48\n",
      "[50 | 10.28] loss=3.17 avg=3.47\n",
      "[51 | 10.33] loss=0.78 avg=3.40\n",
      "[52 | 10.38] loss=4.37 avg=3.42\n",
      "[53 | 10.44] loss=1.61 avg=3.38\n",
      "[54 | 10.49] loss=5.68 avg=3.44\n",
      "[55 | 10.54] loss=2.91 avg=3.42\n",
      "[56 | 10.59] loss=5.16 avg=3.46\n",
      "[57 | 10.64] loss=2.89 avg=3.45\n",
      "[58 | 10.69] loss=2.35 avg=3.43\n",
      "[59 | 10.74] loss=1.87 avg=3.39\n",
      "[60 | 10.79] loss=4.09 avg=3.41\n",
      "[61 | 10.85] loss=1.54 avg=3.37\n",
      "[62 | 10.90] loss=2.58 avg=3.35\n",
      "[63 | 10.95] loss=1.93 avg=3.32\n",
      "[64 | 11.00] loss=5.34 avg=3.36\n",
      "[65 | 11.06] loss=2.01 avg=3.33\n",
      "[66 | 11.11] loss=1.56 avg=3.30\n",
      "[67 | 11.16] loss=2.74 avg=3.28\n",
      "[68 | 11.21] loss=3.23 avg=3.28\n",
      "[69 | 11.26] loss=1.76 avg=3.25\n",
      "[70 | 11.31] loss=4.28 avg=3.27\n",
      "[71 | 11.36] loss=2.28 avg=3.25\n",
      "[72 | 11.41] loss=2.89 avg=3.25\n",
      "[73 | 11.46] loss=1.64 avg=3.22\n",
      "[74 | 11.51] loss=1.84 avg=3.19\n",
      "[75 | 11.56] loss=3.32 avg=3.19\n",
      "[76 | 11.61] loss=5.01 avg=3.23\n",
      "[77 | 11.66] loss=5.60 avg=3.27\n",
      "[78 | 11.71] loss=4.70 avg=3.30\n",
      "[79 | 11.76] loss=2.99 avg=3.29\n",
      "[80 | 11.81] loss=2.36 avg=3.27\n",
      "[81 | 11.86] loss=2.12 avg=3.25\n",
      "[82 | 11.91] loss=1.64 avg=3.22\n",
      "[83 | 11.96] loss=1.68 avg=3.20\n",
      "[84 | 12.02] loss=3.32 avg=3.20\n",
      "[85 | 12.07] loss=1.62 avg=3.17\n",
      "[86 | 12.12] loss=2.05 avg=3.15\n",
      "[87 | 12.17] loss=1.09 avg=3.12\n",
      "[88 | 12.23] loss=1.99 avg=3.10\n",
      "[89 | 12.28] loss=1.64 avg=3.07\n",
      "[90 | 12.33] loss=1.50 avg=3.05\n",
      "[91 | 12.38] loss=2.94 avg=3.05\n",
      "[92 | 12.43] loss=4.28 avg=3.07\n",
      "[93 | 12.48] loss=2.23 avg=3.05\n",
      "[94 | 12.53] loss=2.03 avg=3.04\n",
      "[95 | 12.59] loss=4.83 avg=3.06\n",
      "[96 | 12.64] loss=4.87 avg=3.09\n",
      "[97 | 12.69] loss=2.38 avg=3.08\n",
      "[98 | 12.74] loss=1.20 avg=3.05\n",
      "[99 | 12.79] loss=5.07 avg=3.08\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "_data[4].id);\n",
      "               }else\n",
      "                 if ((f[3].numBits) - 1) {\n",
      "                    com.googlemaps.eventpool.eventpool.event.event.event.view.Adapter(com.googlemaps.eventpool.event.event.view.EventViewAdapter.class, android.content.Context).setContentView(R.layout.activity_actions_view);\n",
      "                    com.googlemaps.eventpool.eventpool.event.event.view.OverrideOnClickListener android.view.OnClickListener currentChannels = new com.googlemaps.eventpool.eventpool.eventpool.eventpool.view.View.onClick(getView().setContext(android.context.controls.view)) + \" \" + (currentChannel.getComponentName().getContext()) + \" \");\n",
      "            }\n",
      "         }else\n",
      "                if ((p == null) && (f[3].numBits) - 1) {\n",
      "                  if (((f[3].getName()).hasSelectedNode(f[3].getName().isSequential()) || ((com.googlemaps.eventpool.api.view.ViewController.class == android.content.Context.FIELD_VIEW) == null)) {\n",
      "                com.googlemaps.eventpool.api.view.ViewController viewController = android.view.ViewViewController();\n",
      "              viewController.setContentView(R.layout.activity_actions_view);\n",
      "             com.googlemaps.eventpool.api.view.ViewController viewController1 = null;\n",
      "              viewController1.setContentView(R.layout.activity_actions_view);\n",
      "               android.view.LayoutParams.setVisibility(Bounds.INVISIBLE);\n",
      "               com.googlemaps.eventpool.eventpool.api.view.UserView viewUser1 = com.googlemaps.eventpool.api.view.UserView.findViewById(R.id.usersController_view1);\n",
      "               android.widget.ViewView id = android.widget.View.createView(id);\n",
      "               viewController1.setOnClickListener(new android.view.OnClickListener() {\n",
      "                 android.widget.View view = android.widget.View.getChildrenByUser(view);\n",
      "               viewController1.setOnClickListener(new android.widget.OnClickListener() {\n",
      "               }));\n",
      "             View android.view.Item lv_view = null;\n",
      "               viewController1.setOnClickListener(new android.widget.OnClickListener() {\n",
      "                  android.widget.ViewViewView view;\n",
      "                  viewController1.setOnClickListener(new android.app.View.OnClickListener() {\n",
      "                  android.widget.ViewViewView lv_view = com.googlemaps.eventpool.api.view.TextViewViewView.findViewById(R.id.view);\n",
      "                android.widget.ViewViewView v_view.setOnClickListener(new android.widget.OnClickListener() {\n",
      "                   viewViewView\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 | 31.45] validation loss = 3.17\n",
      "[100 | 31.51] loss=2.82 avg=3.08\n",
      "[101 | 31.56] loss=3.10 avg=3.08\n",
      "[102 | 31.62] loss=2.34 avg=3.07\n",
      "[103 | 31.67] loss=4.87 avg=3.10\n",
      "[104 | 31.72] loss=5.04 avg=3.13\n",
      "[105 | 31.77] loss=1.44 avg=3.10\n",
      "[106 | 31.83] loss=2.59 avg=3.09\n",
      "[107 | 31.88] loss=1.86 avg=3.07\n",
      "[108 | 31.94] loss=1.96 avg=3.06\n",
      "[109 | 31.99] loss=2.02 avg=3.04\n",
      "[110 | 32.05] loss=4.73 avg=3.07\n",
      "[111 | 32.10] loss=2.70 avg=3.06\n",
      "[112 | 32.15] loss=1.77 avg=3.04\n",
      "[113 | 32.21] loss=2.39 avg=3.03\n",
      "[114 | 32.26] loss=1.17 avg=3.01\n",
      "[115 | 32.31] loss=3.70 avg=3.02\n",
      "[116 | 32.36] loss=3.13 avg=3.02\n",
      "[117 | 32.42] loss=1.99 avg=3.00\n",
      "[118 | 32.47] loss=5.91 avg=3.04\n",
      "[119 | 32.52] loss=3.86 avg=3.06\n",
      "[120 | 32.57] loss=2.01 avg=3.04\n",
      "[121 | 32.62] loss=2.65 avg=3.04\n",
      "[122 | 32.67] loss=1.23 avg=3.01\n",
      "[123 | 32.72] loss=3.78 avg=3.02\n",
      "[124 | 32.77] loss=2.30 avg=3.01\n",
      "[125 | 32.82] loss=5.39 avg=3.04\n",
      "[126 | 32.87] loss=2.90 avg=3.04\n",
      "[127 | 32.92] loss=4.69 avg=3.07\n",
      "[128 | 32.97] loss=4.59 avg=3.09\n",
      "[129 | 33.03] loss=2.89 avg=3.08\n",
      "[130 | 33.08] loss=4.30 avg=3.10\n",
      "[131 | 33.12] loss=5.38 avg=3.13\n",
      "[132 | 33.18] loss=4.33 avg=3.15\n",
      "[133 | 33.23] loss=3.97 avg=3.16\n",
      "[134 | 33.28] loss=1.80 avg=3.14\n",
      "[135 | 33.33] loss=2.33 avg=3.13\n",
      "[136 | 33.38] loss=3.46 avg=3.13\n",
      "[137 | 33.43] loss=5.35 avg=3.16\n",
      "[138 | 33.48] loss=1.70 avg=3.14\n",
      "[139 | 33.53] loss=0.97 avg=3.12\n",
      "[140 | 33.58] loss=1.41 avg=3.09\n",
      "[141 | 33.63] loss=3.90 avg=3.10\n",
      "[142 | 33.68] loss=1.86 avg=3.09\n",
      "[143 | 33.74] loss=1.50 avg=3.07\n",
      "[144 | 33.79] loss=4.19 avg=3.08\n",
      "[145 | 33.85] loss=4.50 avg=3.10\n",
      "[146 | 33.90] loss=1.13 avg=3.07\n",
      "[147 | 33.95] loss=2.52 avg=3.07\n",
      "[148 | 34.00] loss=3.73 avg=3.08\n",
      "[149 | 34.05] loss=3.45 avg=3.08\n",
      "[150 | 34.11] loss=2.52 avg=3.07\n",
      "[151 | 34.16] loss=3.65 avg=3.08\n",
      "[152 | 34.21] loss=1.96 avg=3.07\n",
      "[153 | 34.26] loss=1.58 avg=3.05\n",
      "[154 | 34.32] loss=1.27 avg=3.02\n",
      "[155 | 34.37] loss=1.92 avg=3.01\n",
      "[156 | 34.42] loss=2.83 avg=3.01\n",
      "[157 | 34.48] loss=4.48 avg=3.03\n",
      "[158 | 34.53] loss=2.69 avg=3.02\n",
      "[159 | 34.59] loss=4.94 avg=3.05\n",
      "[160 | 34.64] loss=3.34 avg=3.05\n",
      "[161 | 34.69] loss=4.21 avg=3.06\n",
      "[162 | 34.74] loss=3.62 avg=3.07\n",
      "[163 | 34.79] loss=3.99 avg=3.08\n",
      "[164 | 34.84] loss=2.47 avg=3.08\n",
      "[165 | 34.90] loss=2.52 avg=3.07\n",
      "[166 | 34.95] loss=1.91 avg=3.05\n",
      "[167 | 35.01] loss=1.70 avg=3.04\n",
      "[168 | 35.06] loss=2.68 avg=3.03\n",
      "[169 | 35.11] loss=3.31 avg=3.04\n",
      "[170 | 35.16] loss=4.37 avg=3.05\n",
      "[171 | 35.21] loss=1.56 avg=3.03\n",
      "[172 | 35.26] loss=1.82 avg=3.02\n",
      "[173 | 35.32] loss=4.50 avg=3.04\n",
      "[174 | 35.37] loss=3.35 avg=3.04\n",
      "[175 | 35.42] loss=1.66 avg=3.02\n",
      "[176 | 35.48] loss=2.08 avg=3.01\n",
      "[177 | 35.53] loss=2.31 avg=3.01\n",
      "[178 | 35.58] loss=2.81 avg=3.00\n",
      "[179 | 35.63] loss=3.02 avg=3.00\n",
      "[180 | 35.68] loss=0.72 avg=2.98\n",
      "[181 | 35.73] loss=3.17 avg=2.98\n",
      "[182 | 35.79] loss=1.62 avg=2.96\n",
      "[183 | 35.84] loss=1.80 avg=2.95\n",
      "[184 | 35.90] loss=1.67 avg=2.93\n",
      "[185 | 35.95] loss=3.03 avg=2.93\n",
      "[186 | 36.00] loss=2.22 avg=2.93\n",
      "[187 | 36.05] loss=4.86 avg=2.95\n",
      "[188 | 36.10] loss=1.85 avg=2.94\n",
      "[189 | 36.15] loss=2.12 avg=2.93\n",
      "[190 | 36.20] loss=3.79 avg=2.94\n",
      "[191 | 36.25] loss=1.62 avg=2.92\n",
      "[192 | 36.30] loss=0.48 avg=2.89\n",
      "[193 | 36.35] loss=2.81 avg=2.89\n",
      "[194 | 36.40] loss=3.61 avg=2.90\n",
      "[195 | 36.45] loss=3.30 avg=2.90\n",
      "[196 | 36.50] loss=4.90 avg=2.93\n",
      "[197 | 36.55] loss=1.20 avg=2.91\n",
      "[198 | 36.60] loss=5.11 avg=2.93\n",
      "[199 | 36.66] loss=3.11 avg=2.93\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "             int result = null;\n",
      "                  long\t = 100;\n",
      "         }\n",
      "    }\n",
      "    com.android.support.app/wpa_supplicant/impl, com.android.support.apps.xnx.gwpa.WPAContext base = new com.android.support.app/wpa_supplicant.impl(userAgent, WPA_SERVER_SETTINGS, RATE_CONTERING, RATES_TO_MATCHABLE, RATES_TO_MARKING, RATES_TO_EXPERIMENTAL_CASES, RATES_TO_NOT_PROTECTED, RATES_TO_MATCHABLE, RATES_TO_EXAMPLE_AUTHORIZATION, RATES_TO_NOT_PROTECTED, RATES_TO_NOT_EXPLICIT_STATE))\n",
      "    if (((base == android.app.Context.ENVIRONMENT_PREFERRED) && ((base != android.app.ApplicationConstants.GENERAL_DELAY_FOR_COMMAND)))\n",
      "            i = base.get(new com.jspad.android.util.StringBuilder()[0]))\n",
      "             com.jspad.core.util.StringBuilder base = new com.jspad.core.util.StringBuilder()[0] = base;\n",
      "          \n",
      "    if (result != null)\n",
      "           {\n",
      "           android.widget.Widgets.remove(base, result);\n",
      "        }\n",
      "    java.util.StringField org.jspad.core.util.ListNodeType org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField\n",
      "         org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField\n",
      "         org.jspad.core.util.StringField java.util.StringField android.widget.Widgets.remove(base, result);\n",
      "         \n",
      "    if (result != null)\n",
      "           android.widget.Widgets.remove(base, result);\n",
      "   }\n",
      "   java.util.List<com.android.support.util.ArrayList> javaList = new javaList();\n",
      "   if (com.jspad.core.util.stringField org.jspad.core.util.StringField java.util.StringField stringField);\n",
      "   if ( org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField java.util.StringField\n",
      "  (org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField org.jspad.core.util.StringField java.util.StringField\n",
      "       javaList.remove(base, result, javaList.getString(org.jspad.core.util.StringField)));\n",
      "   if (org.jspad.core.util.StringField org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField com.jspad.core.util.StringField java.util.StringField\n",
      "        javaList.remove(base, result, org-util.StringField.getString(org.jspad.core.util.StringField)));\n",
      "   }else\n",
      "   if (com.jspad.core.util.StringField org.jspad.core.util.StringField org.jspad.core.util.StringField java.util.StringField\n",
      "       org.jspad.core.util.StringField java.util.StringField javaConstants javaConstants javaConstants javaConstants org.jspad.core.util.StringField javaConstants javaStringField org.jspad\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 | 52.78] validation loss = 2.98\n",
      "[200 | 52.84] loss=3.06 avg=2.94\n",
      "[201 | 52.89] loss=5.12 avg=2.96\n",
      "[202 | 52.94] loss=2.77 avg=2.96\n",
      "[203 | 52.99] loss=2.90 avg=2.96\n",
      "[204 | 53.05] loss=5.15 avg=2.98\n",
      "[205 | 53.10] loss=2.02 avg=2.97\n",
      "[206 | 53.15] loss=3.68 avg=2.98\n",
      "[207 | 53.21] loss=1.65 avg=2.97\n",
      "[208 | 53.26] loss=1.20 avg=2.95\n",
      "[209 | 53.31] loss=4.38 avg=2.96\n",
      "[210 | 53.36] loss=3.87 avg=2.97\n",
      "[211 | 53.41] loss=1.97 avg=2.96\n",
      "[212 | 53.46] loss=1.30 avg=2.94\n",
      "[213 | 53.52] loss=4.20 avg=2.96\n",
      "[214 | 53.57] loss=2.48 avg=2.95\n",
      "[215 | 53.62] loss=2.98 avg=2.95\n",
      "[216 | 53.67] loss=3.16 avg=2.95\n",
      "[217 | 53.72] loss=1.72 avg=2.94\n",
      "[218 | 53.78] loss=1.48 avg=2.92\n",
      "[219 | 53.83] loss=2.21 avg=2.92\n",
      "[220 | 53.87] loss=4.34 avg=2.93\n",
      "[221 | 53.92] loss=3.60 avg=2.94\n",
      "[222 | 53.98] loss=1.34 avg=2.92\n",
      "[223 | 54.03] loss=2.55 avg=2.92\n",
      "[224 | 54.08] loss=1.97 avg=2.91\n",
      "[225 | 54.13] loss=4.72 avg=2.93\n",
      "[226 | 54.18] loss=2.41 avg=2.92\n",
      "[227 | 54.23] loss=2.39 avg=2.91\n",
      "[228 | 54.28] loss=2.30 avg=2.91\n",
      "[229 | 54.33] loss=2.27 avg=2.90\n",
      "[230 | 54.38] loss=2.10 avg=2.89\n",
      "[231 | 54.43] loss=1.62 avg=2.88\n",
      "[232 | 54.48] loss=2.82 avg=2.88\n",
      "[233 | 54.54] loss=3.47 avg=2.88\n",
      "[234 | 54.59] loss=4.53 avg=2.90\n",
      "[235 | 54.64] loss=3.90 avg=2.91\n",
      "[236 | 54.69] loss=1.95 avg=2.90\n",
      "[237 | 54.74] loss=4.56 avg=2.92\n",
      "[238 | 54.79] loss=3.70 avg=2.93\n",
      "[239 | 54.84] loss=3.32 avg=2.93\n",
      "[240 | 54.89] loss=2.57 avg=2.93\n",
      "[241 | 54.95] loss=1.87 avg=2.92\n",
      "[242 | 55.00] loss=2.21 avg=2.91\n",
      "[243 | 55.05] loss=1.46 avg=2.89\n",
      "[244 | 55.10] loss=1.68 avg=2.88\n",
      "[245 | 55.15] loss=2.03 avg=2.87\n",
      "[246 | 55.21] loss=1.44 avg=2.86\n",
      "[247 | 55.26] loss=0.91 avg=2.83\n",
      "[248 | 55.31] loss=1.49 avg=2.82\n",
      "[249 | 55.37] loss=2.26 avg=2.81\n",
      "[250 | 55.42] loss=1.74 avg=2.80\n",
      "[251 | 55.47] loss=4.53 avg=2.82\n",
      "[252 | 55.53] loss=1.74 avg=2.81\n",
      "[253 | 55.58] loss=2.09 avg=2.80\n",
      "[254 | 55.63] loss=3.42 avg=2.81\n",
      "[255 | 55.67] loss=3.85 avg=2.82\n",
      "[256 | 55.72] loss=3.50 avg=2.83\n",
      "[257 | 55.78] loss=3.84 avg=2.84\n",
      "[258 | 55.83] loss=1.71 avg=2.83\n",
      "[259 | 55.88] loss=3.65 avg=2.83\n",
      "[260 | 55.93] loss=4.47 avg=2.85\n",
      "[261 | 55.98] loss=1.75 avg=2.84\n",
      "[262 | 56.03] loss=3.67 avg=2.85\n",
      "[263 | 56.08] loss=2.56 avg=2.85\n",
      "[264 | 56.13] loss=1.98 avg=2.84\n",
      "[265 | 56.18] loss=5.19 avg=2.86\n",
      "[266 | 56.23] loss=4.03 avg=2.87\n",
      "[267 | 56.28] loss=2.36 avg=2.87\n",
      "[268 | 56.34] loss=3.15 avg=2.87\n",
      "[269 | 56.39] loss=4.25 avg=2.89\n",
      "[270 | 56.44] loss=1.93 avg=2.88\n",
      "[271 | 56.49] loss=2.02 avg=2.87\n",
      "[272 | 56.54] loss=2.25 avg=2.86\n",
      "[273 | 56.59] loss=4.44 avg=2.88\n",
      "[274 | 56.64] loss=1.01 avg=2.86\n",
      "[275 | 56.69] loss=2.16 avg=2.85\n",
      "[276 | 56.74] loss=3.84 avg=2.86\n",
      "[277 | 56.80] loss=3.46 avg=2.87\n",
      "[278 | 56.85] loss=2.72 avg=2.87\n",
      "[279 | 56.90] loss=2.00 avg=2.86\n",
      "[280 | 56.96] loss=2.98 avg=2.86\n",
      "[281 | 57.01] loss=1.56 avg=2.84\n",
      "[282 | 57.06] loss=2.17 avg=2.84\n",
      "[283 | 57.12] loss=2.27 avg=2.83\n",
      "[284 | 57.17] loss=3.81 avg=2.84\n",
      "[285 | 57.22] loss=2.70 avg=2.84\n",
      "[286 | 57.27] loss=1.60 avg=2.83\n",
      "[287 | 57.33] loss=3.68 avg=2.84\n",
      "[288 | 57.38] loss=4.02 avg=2.85\n",
      "[289 | 57.43] loss=1.97 avg=2.84\n",
      "[290 | 57.48] loss=2.39 avg=2.83\n",
      "[291 | 57.53] loss=3.53 avg=2.84\n",
      "[292 | 57.58] loss=1.60 avg=2.83\n",
      "[293 | 57.63] loss=1.51 avg=2.81\n",
      "[294 | 57.69] loss=4.17 avg=2.83\n",
      "[295 | 57.74] loss=2.19 avg=2.82\n",
      "[296 | 57.79] loss=3.51 avg=2.83\n",
      "[297 | 57.84] loss=2.65 avg=2.83\n",
      "[298 | 57.90] loss=2.80 avg=2.83\n",
      "[299 | 57.95] loss=4.90 avg=2.85\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "                                             end\t;\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300 | 73.89] validation loss = 2.85\n",
      "[300 | 73.95] loss=3.11 avg=2.85\n",
      "[301 | 74.00] loss=1.74 avg=2.84\n",
      "[302 | 74.05] loss=4.61 avg=2.86\n",
      "[303 | 74.10] loss=2.54 avg=2.86\n",
      "[304 | 74.16] loss=3.47 avg=2.86\n",
      "[305 | 74.21] loss=3.63 avg=2.87\n",
      "[306 | 74.26] loss=4.05 avg=2.88\n",
      "[307 | 74.31] loss=1.86 avg=2.87\n",
      "[308 | 74.36] loss=2.68 avg=2.87\n",
      "[309 | 74.41] loss=5.54 avg=2.90\n",
      "[310 | 74.47] loss=1.85 avg=2.89\n",
      "[311 | 74.52] loss=3.51 avg=2.89\n",
      "[312 | 74.57] loss=1.38 avg=2.88\n",
      "[313 | 74.62] loss=1.40 avg=2.86\n",
      "[314 | 74.67] loss=2.65 avg=2.86\n",
      "[315 | 74.72] loss=2.29 avg=2.85\n",
      "[316 | 74.77] loss=2.36 avg=2.85\n",
      "[317 | 74.82] loss=3.33 avg=2.85\n",
      "[318 | 74.87] loss=3.99 avg=2.87\n",
      "[319 | 74.92] loss=2.58 avg=2.86\n",
      "[320 | 74.98] loss=3.29 avg=2.87\n",
      "[321 | 75.03] loss=2.49 avg=2.86\n",
      "[322 | 75.09] loss=0.52 avg=2.84\n",
      "[323 | 75.14] loss=3.21 avg=2.84\n",
      "[324 | 75.20] loss=1.71 avg=2.83\n",
      "[325 | 75.25] loss=2.67 avg=2.83\n",
      "[326 | 75.31] loss=1.06 avg=2.81\n",
      "[327 | 75.36] loss=4.21 avg=2.82\n",
      "[328 | 75.41] loss=4.19 avg=2.84\n",
      "[329 | 75.46] loss=5.30 avg=2.86\n",
      "[330 | 75.51] loss=1.55 avg=2.85\n",
      "[331 | 75.57] loss=4.16 avg=2.86\n",
      "[332 | 75.62] loss=2.10 avg=2.86\n",
      "[333 | 75.67] loss=1.71 avg=2.84\n",
      "[334 | 75.72] loss=2.84 avg=2.84\n",
      "[335 | 75.77] loss=1.60 avg=2.83\n",
      "[336 | 75.82] loss=1.48 avg=2.82\n",
      "[337 | 75.88] loss=1.47 avg=2.80\n",
      "[338 | 75.93] loss=1.63 avg=2.79\n",
      "[339 | 75.98] loss=1.08 avg=2.77\n",
      "[340 | 76.03] loss=4.29 avg=2.79\n",
      "[341 | 76.08] loss=3.76 avg=2.80\n",
      "[342 | 76.14] loss=3.97 avg=2.81\n",
      "[343 | 76.19] loss=3.43 avg=2.82\n",
      "[344 | 76.24] loss=3.07 avg=2.82\n",
      "[345 | 76.29] loss=1.92 avg=2.81\n",
      "[346 | 76.34] loss=4.00 avg=2.82\n",
      "[347 | 76.40] loss=2.64 avg=2.82\n",
      "[348 | 76.45] loss=3.32 avg=2.83\n",
      "[349 | 76.50] loss=2.77 avg=2.83\n",
      "[350 | 76.55] loss=3.17 avg=2.83\n",
      "[351 | 76.61] loss=3.27 avg=2.83\n",
      "[352 | 76.66] loss=3.58 avg=2.84\n",
      "[353 | 76.71] loss=3.95 avg=2.85\n",
      "[354 | 76.76] loss=1.45 avg=2.84\n",
      "[355 | 76.82] loss=2.61 avg=2.84\n",
      "[356 | 76.87] loss=0.90 avg=2.82\n",
      "[357 | 76.92] loss=2.55 avg=2.81\n",
      "[358 | 76.98] loss=1.13 avg=2.80\n",
      "[359 | 77.03] loss=3.68 avg=2.81\n",
      "[360 | 77.08] loss=3.15 avg=2.81\n",
      "[361 | 77.13] loss=3.88 avg=2.82\n",
      "[362 | 77.18] loss=3.84 avg=2.83\n",
      "[363 | 77.23] loss=2.36 avg=2.83\n",
      "[364 | 77.28] loss=3.75 avg=2.84\n",
      "[365 | 77.34] loss=2.45 avg=2.83\n",
      "[366 | 77.39] loss=2.44 avg=2.83\n",
      "[367 | 77.44] loss=2.59 avg=2.83\n",
      "[368 | 77.50] loss=1.83 avg=2.82\n",
      "[369 | 77.55] loss=2.70 avg=2.81\n",
      "[370 | 77.60] loss=2.37 avg=2.81\n",
      "[371 | 77.66] loss=3.58 avg=2.82\n",
      "[372 | 77.71] loss=2.44 avg=2.81\n",
      "[373 | 77.76] loss=2.10 avg=2.81\n",
      "[374 | 77.82] loss=4.82 avg=2.83\n",
      "[375 | 77.87] loss=2.31 avg=2.82\n",
      "[376 | 77.92] loss=2.98 avg=2.82\n",
      "[377 | 77.97] loss=2.01 avg=2.81\n",
      "[378 | 78.03] loss=1.28 avg=2.80\n",
      "[379 | 78.08] loss=2.10 avg=2.79\n",
      "[380 | 78.13] loss=2.43 avg=2.79\n",
      "[381 | 78.18] loss=1.30 avg=2.77\n",
      "[382 | 78.24] loss=1.93 avg=2.76\n",
      "[383 | 78.29] loss=2.17 avg=2.76\n",
      "[384 | 78.34] loss=4.02 avg=2.77\n",
      "[385 | 78.40] loss=1.68 avg=2.76\n",
      "[386 | 78.45] loss=3.12 avg=2.76\n",
      "[387 | 78.51] loss=1.81 avg=2.75\n",
      "[388 | 78.56] loss=3.14 avg=2.76\n",
      "[389 | 78.61] loss=3.58 avg=2.77\n",
      "[390 | 78.66] loss=4.44 avg=2.78\n",
      "[391 | 78.71] loss=2.61 avg=2.78\n",
      "[392 | 78.76] loss=4.49 avg=2.80\n",
      "[393 | 78.81] loss=3.32 avg=2.80\n",
      "[394 | 78.86] loss=4.14 avg=2.82\n",
      "[395 | 78.92] loss=3.54 avg=2.83\n",
      "[396 | 78.97] loss=1.70 avg=2.81\n",
      "[397 | 79.02] loss=1.48 avg=2.80\n",
      "[398 | 79.07] loss=2.30 avg=2.80\n",
      "[399 | 79.12] loss=1.31 avg=2.78\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " sizeof.get(0f)));\n",
      "                                                   \n",
      "                                              \n",
      "                                       \n",
      "                                      \n",
      "                                 \n",
      "                              \n",
      "                            \n",
      "                          \n",
      "                       \n",
      "                \n",
      "              \n",
      "           \n",
      "            \n",
      "        \n",
      "        \n",
      "       \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "     Toast,\t;\n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "   d = Toast(d, Toast.TOUCH, Toast.PENDING);\n",
      "   \n",
      "  \n",
      "   for (int i = 0; i < i; ++i) {\n",
      "   \n",
      "if (d == Toast) {\n",
      "      d.printStackTrace(\"Pending: %d\", e);\n",
      "    }\n",
      "   \n",
      "    for (int j = 0; j < j; ++i) {\n",
      "       for (int j = 0; j < j; ++j) {\n",
      "        return j;\n",
      "       }\n",
      "      \n",
      "    \n",
      "    int d;\n",
      "  \n",
      "  \n",
      "   d = Toast.TOUCH;\n",
      "  \n",
      "  \n",
      "   float i;\n",
      "  \n",
      "  \n",
      "   int j;\n",
      "  \n",
      "    java.lang.String pindex;\n",
      "   d.printStackTrace(java.lang.String.valueOf(n));\n",
      "  \n",
      "}\n",
      "    void dns(double n, double b1, double b2, double b3, double b4, int[] fd, int[] e3, int[] ff);\n",
      "   int j;\n",
      "  \n",
      "   java.lang.String v,\t;\n",
      "  \n",
      "   int i;\n",
      "  \n",
      "   java.lang.String v;\n",
      "  \n",
      "   java.lang.String v;\n",
      "   \n",
      "   int i;\n",
      "  \n",
      "   java.util.Dimiters\t();\n",
      "  \n",
      "   java.util.Integer v1f;\n",
      "  \n",
      "   boolean isSparse;\n",
      "  \n",
      "   java.util.Boolean isNotSymlink();\n",
      "  \n",
      "   java.util.Constants[] sparse;\n",
      "  \n",
      "   java.util.String[] sparse = new java.util.Constants[]{\n",
      "  \"-java.util.String\" };\n",
      "   java.util.Vector m;\n",
      "  \n",
      "  java.util.Vector v1f;\n",
      "  \n",
      "  java.util.Vector _0;\n",
      "  \n",
      "  java.lang.String l1, l2;\n",
      "  \n",
      "  java.util.String t1, t2;\n",
      "  \n",
      "  java.util.String l3;\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400 | 95.26] validation loss = 2.74\n",
      "[400 | 95.32] loss=4.11 avg=2.79\n",
      "[401 | 95.37] loss=3.75 avg=2.80\n",
      "[402 | 95.42] loss=1.86 avg=2.79\n",
      "[403 | 95.47] loss=2.39 avg=2.79\n",
      "[404 | 95.53] loss=3.58 avg=2.80\n",
      "[405 | 95.58] loss=1.28 avg=2.78\n",
      "[406 | 95.63] loss=3.24 avg=2.79\n",
      "[407 | 95.68] loss=5.02 avg=2.81\n",
      "[408 | 95.73] loss=2.18 avg=2.80\n",
      "[409 | 95.78] loss=1.55 avg=2.79\n",
      "[410 | 95.83] loss=1.98 avg=2.78\n",
      "[411 | 95.88] loss=3.59 avg=2.79\n",
      "[412 | 95.94] loss=3.63 avg=2.80\n",
      "[413 | 95.99] loss=3.43 avg=2.81\n",
      "[414 | 96.04] loss=1.56 avg=2.79\n",
      "[415 | 96.09] loss=1.41 avg=2.78\n",
      "[416 | 96.14] loss=2.73 avg=2.78\n",
      "[417 | 96.19] loss=1.89 avg=2.77\n",
      "[418 | 96.24] loss=2.14 avg=2.76\n",
      "[419 | 96.30] loss=3.46 avg=2.77\n",
      "[420 | 96.35] loss=3.03 avg=2.77\n",
      "[421 | 96.40] loss=1.91 avg=2.76\n",
      "[422 | 96.45] loss=2.31 avg=2.76\n",
      "[423 | 96.50] loss=4.73 avg=2.78\n",
      "[424 | 96.56] loss=2.97 avg=2.78\n",
      "[425 | 96.61] loss=2.66 avg=2.78\n",
      "[426 | 96.66] loss=2.60 avg=2.78\n",
      "[427 | 96.71] loss=4.10 avg=2.79\n",
      "[428 | 96.77] loss=3.54 avg=2.80\n",
      "[429 | 96.82] loss=2.13 avg=2.79\n",
      "[430 | 96.88] loss=1.78 avg=2.78\n",
      "[431 | 96.93] loss=4.41 avg=2.80\n",
      "[432 | 96.98] loss=2.96 avg=2.80\n",
      "[433 | 97.03] loss=3.90 avg=2.81\n",
      "[434 | 97.08] loss=4.01 avg=2.82\n",
      "[435 | 97.13] loss=3.41 avg=2.83\n",
      "[436 | 97.18] loss=1.98 avg=2.82\n",
      "[437 | 97.23] loss=1.57 avg=2.81\n",
      "[438 | 97.28] loss=1.77 avg=2.80\n",
      "[439 | 97.33] loss=1.93 avg=2.79\n",
      "[440 | 97.39] loss=2.23 avg=2.78\n",
      "[441 | 97.44] loss=1.36 avg=2.77\n",
      "[442 | 97.49] loss=2.54 avg=2.77\n",
      "[443 | 97.55] loss=4.35 avg=2.78\n",
      "[444 | 97.60] loss=3.01 avg=2.78\n",
      "[445 | 97.65] loss=1.32 avg=2.77\n",
      "[446 | 97.70] loss=3.90 avg=2.78\n",
      "[447 | 97.76] loss=3.16 avg=2.78\n",
      "[448 | 97.81] loss=2.85 avg=2.79\n",
      "[449 | 97.86] loss=4.90 avg=2.81\n",
      "[450 | 97.92] loss=1.71 avg=2.80\n",
      "[451 | 97.97] loss=2.51 avg=2.79\n",
      "[452 | 98.03] loss=0.81 avg=2.77\n",
      "[453 | 98.08] loss=4.09 avg=2.79\n",
      "[454 | 98.12] loss=2.21 avg=2.78\n",
      "[455 | 98.17] loss=3.09 avg=2.78\n",
      "[456 | 98.23] loss=3.39 avg=2.79\n",
      "[457 | 98.28] loss=1.83 avg=2.78\n",
      "[458 | 98.33] loss=1.88 avg=2.77\n",
      "[459 | 98.38] loss=3.31 avg=2.78\n",
      "[460 | 98.43] loss=2.94 avg=2.78\n",
      "[461 | 98.48] loss=2.96 avg=2.78\n",
      "[462 | 98.54] loss=2.55 avg=2.78\n",
      "[463 | 98.59] loss=1.66 avg=2.77\n",
      "[464 | 98.64] loss=3.98 avg=2.78\n",
      "[465 | 98.70] loss=2.28 avg=2.77\n",
      "[466 | 98.75] loss=3.66 avg=2.78\n",
      "[467 | 98.81] loss=1.75 avg=2.77\n",
      "[468 | 98.86] loss=1.37 avg=2.76\n",
      "[469 | 98.91] loss=1.03 avg=2.74\n",
      "[470 | 98.96] loss=3.49 avg=2.75\n",
      "[471 | 99.01] loss=1.88 avg=2.74\n",
      "[472 | 99.07] loss=3.66 avg=2.75\n",
      "[473 | 99.12] loss=4.50 avg=2.77\n",
      "[474 | 99.18] loss=1.76 avg=2.76\n",
      "[475 | 99.23] loss=3.82 avg=2.77\n",
      "[476 | 99.28] loss=1.88 avg=2.76\n",
      "[477 | 99.33] loss=2.37 avg=2.75\n",
      "[478 | 99.38] loss=2.22 avg=2.75\n",
      "[479 | 99.43] loss=3.08 avg=2.75\n",
      "[480 | 99.48] loss=1.88 avg=2.74\n",
      "[481 | 99.53] loss=2.78 avg=2.74\n",
      "[482 | 99.59] loss=2.53 avg=2.74\n",
      "[483 | 99.64] loss=3.09 avg=2.74\n",
      "[484 | 99.69] loss=3.74 avg=2.75\n",
      "[485 | 99.74] loss=2.91 avg=2.76\n",
      "[486 | 99.79] loss=2.32 avg=2.75\n",
      "[487 | 99.85] loss=2.13 avg=2.75\n",
      "[488 | 99.90] loss=2.50 avg=2.74\n",
      "[489 | 99.95] loss=1.87 avg=2.73\n",
      "[490 | 100.00] loss=2.83 avg=2.74\n",
      "[491 | 100.05] loss=2.08 avg=2.73\n",
      "[492 | 100.10] loss=3.71 avg=2.74\n",
      "[493 | 100.16] loss=2.13 avg=2.73\n",
      "[494 | 100.21] loss=2.11 avg=2.73\n",
      "[495 | 100.26] loss=4.02 avg=2.74\n",
      "[496 | 100.32] loss=3.15 avg=2.74\n",
      "[497 | 100.37] loss=3.36 avg=2.75\n",
      "[498 | 100.42] loss=2.07 avg=2.74\n",
      "[499 | 100.47] loss=3.02 avg=2.75\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 43.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "ptr.calls.moved = (!moved));\n",
      "     }\n",
      "\n",
      "      moved += (((moved += 1)) - 1));\n",
      "     moved += mmoved;\n",
      "    }\n",
      "\n",
      "    mn = getMn();\n",
      "    int iMn = (iMn >= 6)?(new\t(mn));\n",
      "    mn = mn + 1;\n",
      "    while (mn) {\n",
      "        new juven = getMn();\n",
      "        mn = (mn + 0 ? 1 : 0);\n",
      "        int iMn = (mn + 1 ? 1 : 2);\n",
      "        if (mn * (mn) > ((iMn- 2))) {\n",
      "          iMn = (iMn- 1 ? 1 : 0);\n",
      "          iMn = (iMn - 2 ? 1 : 0);\n",
      "         mn = (mn + 1 ? 1 : 0);\n",
      "         iMn = (mn + 1 ? 0 : 1);\n",
      "        iMn =  (mn + 1 ? 0 : 0);\n",
      "        int iMn = (iMn - (mn + 2)) *(mn/2);\n",
      "        int iMn_2 = mn != mn_2;\n",
      "       if (mn < (mn_2)))) {\n",
      "         mn = 0;\n",
      "        for (iMn = 0; iMn < (iMn_2));\n",
      "        mn = mn_2;\n",
      "       if ((mn_2 - 1) >= iMn_2)\n",
      "       mn = (mn - (jmp_2));\n",
      "       mnd = iMn_2;\n",
      "       mnd = mn_2;\n",
      "       if ((mn_2 - 1) <= (mnd_2))\n",
      "       mnd = 0;\n",
      "       mn = ((mnd - 1) - 1);\n",
      "        int iMn_2 = (iMn - (mnd_2))) *(mn_2 + (jmp_2));\n",
      "    }\n",
      "    try {\n",
      "       if (!(mn_2 - 1) == (mnd_2))\n",
      "       mnd = 0;\n",
      "       if (mnd < (mnd_2) || mnd_3)))\n",
      "       mnd = 0;\n",
      "       if ((mnd_3 - 1) & (mnd_2 - 1)) < (mnd_2))\n",
      "      mnd  = 0;\n",
      "       if ((mnd_3 - 1) == mnd_2)) {\n",
      "        if (mnd_3 * (mnd_2) + 1) == (mnd_2))\n",
      "       mnd = 0;\n",
      "        mnd  = 0;\n",
      "        mnd ;\n",
      "        if ((mnd - (jmp_1)) < (mnd_1)))\n",
      "        mnd  = 0;\n",
      "        mnd  = 0;\n",
      "         if ((mnd - (jmp_7)) < (mnd_1)))\n",
      "       } catch (java.lang.SystemException e) {\n",
      "        e.printStackTrace();\n",
      "       if ((e.read()) == true) {\n",
      "       return e.getElementsByTagName(mN * (e.getElementById(jmp_7))) + mN);\n",
      "      }\n",
      "     jmp_7 += (((jmp_2 - 1))) /\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500 | 116.43] validation loss = 2.67\n",
      "[500 | 116.49] loss=4.87 avg=2.77\n",
      "[501 | 116.54] loss=2.39 avg=2.76\n",
      "[502 | 116.60] loss=2.86 avg=2.76\n",
      "[503 | 116.65] loss=2.89 avg=2.77\n",
      "[504 | 116.71] loss=4.05 avg=2.78\n",
      "[505 | 116.76] loss=3.29 avg=2.78\n",
      "[506 | 116.81] loss=1.62 avg=2.77\n",
      "[507 | 116.86] loss=1.81 avg=2.76\n",
      "[508 | 116.91] loss=1.13 avg=2.75\n",
      "[509 | 116.97] loss=2.99 avg=2.75\n",
      "[510 | 117.01] loss=1.72 avg=2.74\n",
      "[511 | 117.06] loss=2.16 avg=2.73\n",
      "[512 | 117.12] loss=3.45 avg=2.74\n",
      "[513 | 117.17] loss=0.95 avg=2.72\n",
      "[514 | 117.23] loss=1.62 avg=2.71\n",
      "[515 | 117.28] loss=4.13 avg=2.72\n",
      "[516 | 117.33] loss=3.35 avg=2.73\n",
      "[517 | 117.38] loss=2.90 avg=2.73\n",
      "[518 | 117.43] loss=2.99 avg=2.73\n",
      "[519 | 117.48] loss=3.89 avg=2.75\n",
      "[520 | 117.53] loss=2.40 avg=2.74\n",
      "[521 | 117.59] loss=4.74 avg=2.76\n",
      "[522 | 117.64] loss=1.79 avg=2.75\n",
      "[523 | 117.69] loss=4.18 avg=2.77\n",
      "[524 | 117.75] loss=1.14 avg=2.75\n",
      "[525 | 117.80] loss=1.77 avg=2.74\n",
      "[526 | 117.85] loss=2.87 avg=2.74\n",
      "[527 | 117.90] loss=2.33 avg=2.74\n",
      "[528 | 117.95] loss=2.57 avg=2.74\n",
      "[529 | 118.00] loss=1.39 avg=2.72\n",
      "[530 | 118.05] loss=3.34 avg=2.73\n",
      "[531 | 118.11] loss=1.92 avg=2.72\n",
      "[532 | 118.16] loss=3.60 avg=2.73\n",
      "[533 | 118.21] loss=3.77 avg=2.74\n",
      "[534 | 118.26] loss=3.31 avg=2.75\n",
      "[535 | 118.32] loss=1.97 avg=2.74\n",
      "[536 | 118.37] loss=2.71 avg=2.74\n",
      "[537 | 118.42] loss=4.45 avg=2.76\n",
      "[538 | 118.47] loss=2.90 avg=2.76\n",
      "[539 | 118.52] loss=3.36 avg=2.76\n",
      "[540 | 118.58] loss=2.20 avg=2.76\n",
      "[541 | 118.63] loss=1.73 avg=2.75\n",
      "[542 | 118.68] loss=2.40 avg=2.74\n",
      "[543 | 118.73] loss=3.66 avg=2.75\n",
      "[544 | 118.78] loss=2.96 avg=2.75\n",
      "[545 | 118.83] loss=3.85 avg=2.77\n",
      "[546 | 118.88] loss=4.27 avg=2.78\n",
      "[547 | 118.93] loss=1.74 avg=2.77\n",
      "[548 | 118.98] loss=2.22 avg=2.76\n",
      "[549 | 119.03] loss=1.52 avg=2.75\n",
      "[550 | 119.08] loss=2.74 avg=2.75\n",
      "[551 | 119.13] loss=0.87 avg=2.73\n",
      "[552 | 119.18] loss=2.87 avg=2.73\n",
      "[553 | 119.24] loss=3.32 avg=2.74\n",
      "[554 | 119.29] loss=3.60 avg=2.75\n",
      "[555 | 119.34] loss=2.18 avg=2.74\n",
      "[556 | 119.40] loss=3.88 avg=2.75\n",
      "[557 | 119.45] loss=4.47 avg=2.77\n",
      "[558 | 119.50] loss=2.22 avg=2.77\n",
      "[559 | 119.56] loss=4.06 avg=2.78\n",
      "[560 | 119.61] loss=2.42 avg=2.78\n",
      "[561 | 119.66] loss=2.76 avg=2.78\n",
      "[562 | 119.71] loss=2.35 avg=2.77\n",
      "[563 | 119.76] loss=4.58 avg=2.79\n",
      "[564 | 119.81] loss=1.82 avg=2.78\n",
      "[565 | 119.87] loss=1.90 avg=2.77\n",
      "[566 | 119.92] loss=2.02 avg=2.76\n",
      "[567 | 119.98] loss=2.42 avg=2.76\n",
      "[568 | 120.03] loss=1.25 avg=2.74\n",
      "[569 | 120.08] loss=1.88 avg=2.74\n",
      "[570 | 120.13] loss=2.91 avg=2.74\n",
      "[571 | 120.19] loss=2.05 avg=2.73\n",
      "[572 | 120.24] loss=1.91 avg=2.72\n",
      "[573 | 120.29] loss=2.34 avg=2.72\n",
      "[574 | 120.35] loss=4.20 avg=2.73\n",
      "[575 | 120.40] loss=2.58 avg=2.73\n",
      "[576 | 120.45] loss=2.81 avg=2.73\n",
      "[577 | 120.50] loss=2.31 avg=2.73\n",
      "[578 | 120.55] loss=3.51 avg=2.74\n",
      "[579 | 120.60] loss=1.78 avg=2.73\n",
      "[580 | 120.65] loss=1.68 avg=2.72\n",
      "[581 | 120.70] loss=3.68 avg=2.73\n",
      "[582 | 120.75] loss=2.32 avg=2.72\n",
      "[583 | 120.81] loss=1.64 avg=2.71\n",
      "[584 | 120.87] loss=1.52 avg=2.70\n",
      "[585 | 120.92] loss=2.70 avg=2.70\n",
      "[586 | 120.98] loss=1.87 avg=2.69\n",
      "[587 | 121.03] loss=1.49 avg=2.68\n",
      "[588 | 121.08] loss=1.17 avg=2.66\n",
      "[589 | 121.14] loss=1.34 avg=2.65\n",
      "[590 | 121.19] loss=3.50 avg=2.66\n",
      "[591 | 121.24] loss=3.25 avg=2.67\n",
      "[592 | 121.29] loss=3.66 avg=2.68\n",
      "[593 | 121.34] loss=3.20 avg=2.68\n",
      "[594 | 121.39] loss=1.95 avg=2.67\n",
      "[595 | 121.44] loss=3.94 avg=2.69\n",
      "[596 | 121.49] loss=2.47 avg=2.68\n",
      "[597 | 121.54] loss=3.09 avg=2.69\n",
      "[598 | 121.60] loss=2.04 avg=2.68\n",
      "[599 | 121.65] loss=1.86 avg=2.67\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "idid:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tIRED\t\tIRED\tIRED\t\tIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIREDIR\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tAliasAliasAliasBase\t\t\t\t\t\t\t\t\t\t\tAliasAliasBase\t\t\tAliasBaseBaseAliasAliasAliasBaseAliasAliasAliasAliasBaseAliasAliasAliasAliasBaseAliasAliasAliasAliasBaseAliasVariantAliasAliasAliasAliasVariantAliasAliasAliasAliasAliAdapterAliasAliasAliasAliasAliasParameterAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasNameAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasNameAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAliasAlias\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600 | 137.67] validation loss = 2.62\n",
      "[600 | 137.72] loss=3.41 avg=2.68\n",
      "[601 | 137.78] loss=2.37 avg=2.68\n",
      "[602 | 137.83] loss=1.14 avg=2.66\n",
      "[603 | 137.88] loss=1.97 avg=2.65\n",
      "[604 | 137.94] loss=3.18 avg=2.66\n",
      "[605 | 138.00] loss=2.60 avg=2.66\n",
      "[606 | 138.05] loss=2.58 avg=2.66\n",
      "[607 | 138.10] loss=4.84 avg=2.68\n",
      "[608 | 138.15] loss=2.38 avg=2.68\n",
      "[609 | 138.20] loss=2.07 avg=2.67\n",
      "[610 | 138.26] loss=1.93 avg=2.66\n",
      "[611 | 138.31] loss=3.13 avg=2.67\n",
      "[612 | 138.36] loss=2.35 avg=2.67\n",
      "[613 | 138.41] loss=2.76 avg=2.67\n",
      "[614 | 138.47] loss=1.52 avg=2.65\n",
      "[615 | 138.52] loss=1.69 avg=2.65\n",
      "[616 | 138.57] loss=1.81 avg=2.64\n",
      "[617 | 138.63] loss=2.96 avg=2.64\n",
      "[618 | 138.67] loss=3.53 avg=2.65\n",
      "[619 | 138.73] loss=3.09 avg=2.65\n",
      "[620 | 138.78] loss=1.95 avg=2.65\n",
      "[621 | 138.84] loss=2.92 avg=2.65\n",
      "[622 | 138.89] loss=2.03 avg=2.64\n",
      "[623 | 138.95] loss=3.11 avg=2.65\n",
      "[624 | 139.00] loss=2.82 avg=2.65\n",
      "[625 | 139.05] loss=2.69 avg=2.65\n",
      "[626 | 139.10] loss=2.60 avg=2.65\n",
      "[627 | 139.16] loss=3.26 avg=2.66\n",
      "[628 | 139.21] loss=2.96 avg=2.66\n",
      "[629 | 139.26] loss=3.03 avg=2.66\n",
      "[630 | 139.30] loss=3.98 avg=2.68\n",
      "[631 | 139.35] loss=2.37 avg=2.67\n",
      "[632 | 139.40] loss=3.32 avg=2.68\n",
      "[633 | 139.46] loss=3.83 avg=2.69\n",
      "[634 | 139.51] loss=1.76 avg=2.68\n",
      "[635 | 139.56] loss=1.81 avg=2.67\n",
      "[636 | 139.61] loss=3.72 avg=2.68\n",
      "[637 | 139.66] loss=3.63 avg=2.69\n",
      "[638 | 139.71] loss=3.30 avg=2.70\n",
      "[639 | 139.76] loss=3.84 avg=2.71\n",
      "[640 | 139.81] loss=0.80 avg=2.69\n",
      "[641 | 139.87] loss=2.91 avg=2.69\n",
      "[642 | 139.92] loss=2.17 avg=2.69\n",
      "[643 | 139.98] loss=2.65 avg=2.69\n",
      "[644 | 140.03] loss=2.69 avg=2.69\n",
      "[645 | 140.08] loss=1.68 avg=2.68\n",
      "[646 | 140.14] loss=2.35 avg=2.67\n",
      "[647 | 140.19] loss=3.67 avg=2.68\n",
      "[648 | 140.25] loss=1.72 avg=2.67\n",
      "[649 | 140.30] loss=1.07 avg=2.66\n",
      "[650 | 140.35] loss=2.30 avg=2.65\n",
      "[651 | 140.41] loss=3.17 avg=2.66\n",
      "[652 | 140.46] loss=2.46 avg=2.66\n",
      "[653 | 140.51] loss=2.21 avg=2.65\n",
      "[654 | 140.57] loss=2.31 avg=2.65\n",
      "[655 | 140.62] loss=2.67 avg=2.65\n",
      "[656 | 140.67] loss=1.14 avg=2.63\n",
      "[657 | 140.72] loss=3.07 avg=2.64\n",
      "[658 | 140.77] loss=3.94 avg=2.65\n",
      "[659 | 140.82] loss=2.22 avg=2.65\n",
      "[660 | 140.88] loss=3.81 avg=2.66\n",
      "[661 | 140.93] loss=3.72 avg=2.67\n",
      "[662 | 140.98] loss=2.68 avg=2.67\n",
      "[663 | 141.03] loss=3.68 avg=2.68\n",
      "[664 | 141.08] loss=1.52 avg=2.67\n",
      "[665 | 141.13] loss=2.45 avg=2.67\n",
      "[666 | 141.18] loss=3.13 avg=2.67\n",
      "[667 | 141.24] loss=2.79 avg=2.67\n",
      "[668 | 141.29] loss=1.12 avg=2.66\n",
      "[669 | 141.34] loss=3.36 avg=2.66\n",
      "[670 | 141.39] loss=2.24 avg=2.66\n",
      "[671 | 141.44] loss=2.65 avg=2.66\n",
      "[672 | 141.49] loss=3.95 avg=2.67\n",
      "[673 | 141.54] loss=1.29 avg=2.66\n",
      "[674 | 141.59] loss=2.61 avg=2.66\n",
      "[675 | 141.64] loss=1.59 avg=2.65\n",
      "[676 | 141.70] loss=1.50 avg=2.64\n",
      "[677 | 141.75] loss=2.37 avg=2.63\n",
      "[678 | 141.81] loss=2.08 avg=2.63\n",
      "[679 | 141.86] loss=2.52 avg=2.63\n",
      "[680 | 141.91] loss=3.24 avg=2.63\n",
      "[681 | 141.96] loss=3.74 avg=2.64\n",
      "[682 | 142.01] loss=2.94 avg=2.65\n",
      "[683 | 142.06] loss=1.73 avg=2.64\n",
      "[684 | 142.12] loss=1.75 avg=2.63\n",
      "[685 | 142.17] loss=2.68 avg=2.63\n",
      "[686 | 142.22] loss=2.45 avg=2.63\n",
      "[687 | 142.27] loss=4.05 avg=2.64\n",
      "[688 | 142.33] loss=3.39 avg=2.65\n",
      "[689 | 142.38] loss=1.16 avg=2.63\n",
      "[690 | 142.43] loss=0.73 avg=2.62\n",
      "[691 | 142.49] loss=2.23 avg=2.61\n",
      "[692 | 142.54] loss=3.24 avg=2.62\n",
      "[693 | 142.60] loss=1.55 avg=2.61\n",
      "[694 | 142.65] loss=1.85 avg=2.60\n",
      "[695 | 142.70] loss=2.27 avg=2.60\n",
      "[696 | 142.75] loss=3.68 avg=2.61\n",
      "[697 | 142.80] loss=3.50 avg=2.62\n",
      "[698 | 142.86] loss=2.75 avg=2.62\n",
      "[699 | 142.91] loss=3.23 avg=2.62\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "        }\n",
      "           }\n",
      "             break\n",
      "            \n",
      "            if ((p->p_base) != p->base_base)\n",
      "                \n",
      "                return\n",
      "            \n",
      "           break\n",
      "         \n",
      "        \n",
      "       \n",
      "       break\n",
      "        break\n",
      "       return \n",
      "      \n",
      "     \n",
      "     break \n",
      "     break1 \n",
      "     \n",
      "     break1  \n",
      "    \n",
      "     break1\n",
      "     return \n",
      "     break2\n",
      "    \n",
      "     break3\n",
      "    break1  \n",
      "   \n",
      "    break5 \n",
      "   \n",
      "   break6  \n",
      "   \n",
      "    break7  \n",
      "    break9 \n",
      "    break1\n",
      "   \n",
      "   break9 \n",
      "    break0 \n",
      "    break2 \n",
      "  \n",
      "    break9 \n",
      "  \n",
      "   break9 \n",
      "   break  \n",
      "   break10 \n",
      "  \n",
      "   break11 \n",
      "  \n",
      "   break17 \n",
      "   break1 \n",
      "   break2  \n",
      "   break9    break2 \n",
      "  \n",
      "   break27 \n",
      "  \n",
      "   break1  \n",
      "   break11    break2    break3 \n",
      "  \n",
      "   break3 \n",
      "   break4  \n",
      "    break2    break7  \n",
      "   break9   (break1)   break3  break8    break9 \n",
      "   break13 \n",
      "  \n",
      "   break30 \n",
      "  \n",
      "   break3 \n",
      "   break7)--\n",
      "     break6    break1 \n",
      "   break7   break8    break9 \n",
      "   break7-  \n",
      "  \n",
      "   break6 \n",
      "   break7 \n",
      "    break8 \n",
      "   break6 \n",
      "    break8 \n",
      "  \n",
      "   break1 \n",
      "   break1 \n",
      "    break2 \n",
      "  \n",
      "   break9 \n",
      "  \n",
      "   break6 \n",
      "   break9  \n",
      "   break11    break12     break3 \n",
      "    break7    break8     break9     break10   \n",
      "   else\n",
      "        break4    break3 \n",
      "        break7      break4  \n",
      "         break9     \n",
      "         break10       break10  \n",
      "         break20      break7 \n",
      "         break0      break1   \n",
      "           break0 \n",
      "            break0 \n",
      "             break0 \n",
      "              break0 \n",
      "              break1  \n",
      "              break1 \n",
      "             break1 \n",
      "             break1 \n",
      "             break2 \n",
      "               break2 \n",
      "                break9   \n",
      "                 break9 \n",
      "     \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[700 | 158.90] validation loss = 2.57\n",
      "[700 | 158.96] loss=2.40 avg=2.62\n",
      "[701 | 159.01] loss=2.65 avg=2.62\n",
      "[702 | 159.06] loss=1.75 avg=2.61\n",
      "[703 | 159.12] loss=2.17 avg=2.61\n",
      "[704 | 159.17] loss=3.23 avg=2.61\n",
      "[705 | 159.23] loss=2.42 avg=2.61\n",
      "[706 | 159.28] loss=2.85 avg=2.62\n",
      "[707 | 159.34] loss=3.45 avg=2.62\n",
      "[708 | 159.39] loss=3.35 avg=2.63\n",
      "[709 | 159.45] loss=2.43 avg=2.63\n",
      "[710 | 159.50] loss=1.96 avg=2.62\n",
      "[711 | 159.55] loss=2.59 avg=2.62\n",
      "[712 | 159.61] loss=2.70 avg=2.62\n",
      "[713 | 159.66] loss=3.21 avg=2.63\n",
      "[714 | 159.72] loss=2.57 avg=2.63\n",
      "[715 | 159.77] loss=3.06 avg=2.63\n",
      "[716 | 159.82] loss=2.41 avg=2.63\n",
      "[717 | 159.87] loss=2.72 avg=2.63\n",
      "[718 | 159.93] loss=2.82 avg=2.63\n",
      "[719 | 159.98] loss=4.26 avg=2.65\n",
      "[720 | 160.03] loss=0.63 avg=2.63\n",
      "[721 | 160.08] loss=1.92 avg=2.62\n",
      "[722 | 160.14] loss=2.14 avg=2.62\n",
      "[723 | 160.19] loss=3.96 avg=2.63\n",
      "[724 | 160.24] loss=3.19 avg=2.64\n",
      "[725 | 160.29] loss=3.46 avg=2.64\n",
      "[726 | 160.35] loss=1.49 avg=2.63\n",
      "[727 | 160.40] loss=3.70 avg=2.64\n",
      "[728 | 160.46] loss=0.74 avg=2.62\n",
      "[729 | 160.51] loss=2.40 avg=2.62\n",
      "[730 | 160.56] loss=2.41 avg=2.62\n",
      "[731 | 160.61] loss=1.49 avg=2.61\n",
      "[732 | 160.66] loss=2.93 avg=2.61\n",
      "[733 | 160.72] loss=1.77 avg=2.60\n",
      "[734 | 160.77] loss=2.89 avg=2.61\n",
      "[735 | 160.82] loss=2.03 avg=2.60\n",
      "[736 | 160.88] loss=2.75 avg=2.60\n",
      "[737 | 160.93] loss=2.03 avg=2.60\n",
      "[738 | 160.99] loss=3.95 avg=2.61\n",
      "[739 | 161.04] loss=0.93 avg=2.59\n",
      "[740 | 161.10] loss=1.27 avg=2.58\n",
      "[741 | 161.15] loss=1.60 avg=2.57\n",
      "[742 | 161.20] loss=1.39 avg=2.56\n",
      "[743 | 161.25] loss=2.66 avg=2.56\n",
      "[744 | 161.30] loss=2.18 avg=2.56\n",
      "[745 | 161.35] loss=1.44 avg=2.54\n",
      "[746 | 161.40] loss=2.83 avg=2.55\n",
      "[747 | 161.46] loss=3.58 avg=2.56\n",
      "[748 | 161.51] loss=0.73 avg=2.54\n",
      "[749 | 161.56] loss=3.18 avg=2.55\n",
      "[750 | 161.62] loss=3.02 avg=2.55\n",
      "[751 | 161.67] loss=3.06 avg=2.56\n",
      "[752 | 161.72] loss=1.91 avg=2.55\n",
      "[753 | 161.78] loss=3.89 avg=2.56\n",
      "[754 | 161.83] loss=3.10 avg=2.57\n",
      "[755 | 161.88] loss=1.82 avg=2.56\n",
      "[756 | 161.93] loss=1.86 avg=2.55\n",
      "[757 | 161.98] loss=3.22 avg=2.56\n",
      "[758 | 162.03] loss=1.87 avg=2.55\n",
      "[759 | 162.08] loss=3.03 avg=2.56\n",
      "[760 | 162.13] loss=1.54 avg=2.55\n",
      "[761 | 162.19] loss=1.26 avg=2.53\n",
      "[762 | 162.24] loss=4.50 avg=2.55\n",
      "[763 | 162.29] loss=1.67 avg=2.55\n",
      "[764 | 162.34] loss=2.26 avg=2.54\n",
      "[765 | 162.39] loss=1.53 avg=2.53\n",
      "[766 | 162.45] loss=2.53 avg=2.53\n",
      "[767 | 162.50] loss=4.16 avg=2.55\n",
      "[768 | 162.55] loss=3.07 avg=2.55\n",
      "[769 | 162.60] loss=3.87 avg=2.57\n",
      "[770 | 162.65] loss=1.34 avg=2.56\n",
      "[771 | 162.70] loss=2.04 avg=2.55\n",
      "[772 | 162.75] loss=2.72 avg=2.55\n",
      "[773 | 162.80] loss=0.95 avg=2.54\n",
      "[774 | 162.85] loss=1.90 avg=2.53\n",
      "[775 | 162.90] loss=2.55 avg=2.53\n",
      "[776 | 162.95] loss=2.29 avg=2.53\n",
      "[777 | 163.01] loss=3.46 avg=2.54\n",
      "[778 | 163.06] loss=2.14 avg=2.53\n",
      "[779 | 163.12] loss=2.21 avg=2.53\n",
      "[780 | 163.17] loss=2.55 avg=2.53\n",
      "[781 | 163.22] loss=1.07 avg=2.51\n",
      "[782 | 163.28] loss=3.34 avg=2.52\n",
      "[783 | 163.33] loss=1.93 avg=2.52\n",
      "[784 | 163.38] loss=3.46 avg=2.53\n",
      "[785 | 163.44] loss=2.09 avg=2.52\n",
      "[786 | 163.50] loss=1.12 avg=2.51\n",
      "[787 | 163.55] loss=1.67 avg=2.50\n",
      "[788 | 163.60] loss=1.17 avg=2.49\n",
      "[789 | 163.65] loss=3.45 avg=2.50\n",
      "[790 | 163.70] loss=2.27 avg=2.49\n",
      "[791 | 163.75] loss=4.09 avg=2.51\n",
      "[792 | 163.80] loss=3.48 avg=2.52\n",
      "[793 | 163.86] loss=2.57 avg=2.52\n",
      "[794 | 163.91] loss=1.00 avg=2.50\n",
      "[795 | 163.96] loss=3.17 avg=2.51\n",
      "[796 | 164.01] loss=0.84 avg=2.49\n",
      "[797 | 164.06] loss=2.04 avg=2.49\n",
      "[798 | 164.11] loss=1.42 avg=2.48\n",
      "[799 | 164.16] loss=3.27 avg=2.49\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:00<00:00, 48.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "FIG.POWER_CONFIG_RESOURCE_SAME = (d.deviceInfo.get((input.getParameter(\"d2m_pw\"))) + D2M_MAX_D2M_COMMAND_WCHAR_SIZE) +\n",
      "                        {\n",
      "                          }\n",
      "                       if ((input.getParameter(\"d2m_pw\") <= D2M_MAX_D2M_COMMAND_WCHAR_SIZE) || ((input.getParameter(\"d2m_pw\") <= D2M_MAX_D2M_COMMAND_WCHAR_SIZE)) || ((input.getParameter(\"d2m_pw\") <= D2M_MAX_D2M_COMMAND_WCHAR_SIZE)))\n",
      "                       if ((input.getParameter(\"d2m_pw\") <= END_PW_RANGE) || ((input.getParameter(\"d2m_pw\") == END_PW_RANGE)) || ((input.getParameter(\"a_pwm\")) == END_A_PWM)) {\n",
      "                       continue;\n",
      "                 }\n",
      "                 \n",
      "             \n",
      "            public void onStart() {\n",
      "                  if (!d3m_pw) {\n",
      "                    d3m_pw = D2M_MAX_D3M_COMMAND_GAME;\n",
      "                  d3m_pw = D2M_MAX_D3M_COMMAND_GAME;\n",
      "                  d3m_pw = D2M_MAX_D3M_COMMAND_GONE;\n",
      "                  \n",
      "               }\n",
      "             \n",
      "           \n",
      "         \n",
      "         \n",
      "        \n",
      "    }\n",
      "     \n",
      "    }\n",
      "   \n",
      "   \n",
      "    org.xmldn.iproute.client.interface.NanocraftServiceInterface clientTpClient = ((net.iproute.client.Interface) + clientTpClient) * 0x3C;\n",
      "     net.iproute.client.interface.NanocraftServerInterface clientServerTpServer = ((net.iproute.client.Interface) + clientTpServer) * (0x0C);\n",
      "    net.iproute.client.interface.NanocraftServerAdapterAdapter adapter = ((net.iproute.client.Interface) + adapter) * 0x3C;\n",
      "   \n",
      "   net.iproute.client.interface.NanocraftServerAdapter adapterDereferenceClientAdapter = ((net.iproute.client.Interface) + adapterDereferenceClient) * (0x3C);\n",
      "    \n",
      "    iproute.client.client.interface.NanocraftServerDereferenceAdapter adapterDereferenceServerHostAdapter = ((network.iproute.client.Client) + adapterDereferenceServerHost) * (0x3C);\n",
      "   \n",
      "    clientTest.getClientTest().load();\n",
      "      clientTest.getClientTest().load();\n",
      "   }\n",
      "  \n",
      "   org.xmldn.iproute.client.IsoClientRoutes interface_to_interface_to_interface_to_interface_to_interface_to_interface_to_\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[800 | 180.09] validation loss = 2.53\n",
      "[800 | 180.15] loss=3.07 avg=2.49\n",
      "[801 | 180.20] loss=2.15 avg=2.49\n",
      "[802 | 180.25] loss=2.87 avg=2.49\n",
      "[803 | 180.30] loss=1.32 avg=2.48\n",
      "[804 | 180.36] loss=2.74 avg=2.48\n",
      "[805 | 180.41] loss=2.32 avg=2.48\n",
      "[806 | 180.46] loss=2.82 avg=2.49\n",
      "[807 | 180.52] loss=2.53 avg=2.49\n",
      "[808 | 180.58] loss=0.82 avg=2.47\n",
      "[809 | 180.63] loss=2.45 avg=2.47\n",
      "[810 | 180.68] loss=3.67 avg=2.48\n",
      "[811 | 180.73] loss=2.53 avg=2.48\n",
      "[812 | 180.79] loss=2.50 avg=2.48\n",
      "[813 | 180.84] loss=2.46 avg=2.48\n",
      "[814 | 180.89] loss=1.81 avg=2.48\n",
      "[815 | 180.95] loss=1.42 avg=2.46\n",
      "[816 | 181.00] loss=3.12 avg=2.47\n",
      "[817 | 181.05] loss=1.15 avg=2.46\n",
      "[818 | 181.10] loss=0.91 avg=2.44\n",
      "[819 | 181.15] loss=1.14 avg=2.43\n",
      "[820 | 181.20] loss=2.52 avg=2.43\n",
      "[821 | 181.26] loss=1.63 avg=2.42\n",
      "[822 | 181.31] loss=1.12 avg=2.41\n",
      "[823 | 181.36] loss=1.85 avg=2.40\n",
      "[824 | 181.41] loss=1.91 avg=2.40\n",
      "[825 | 181.47] loss=2.50 avg=2.40\n",
      "[826 | 181.52] loss=2.16 avg=2.40\n",
      "[827 | 181.57] loss=1.72 avg=2.39\n",
      "[828 | 181.63] loss=2.09 avg=2.39\n",
      "[829 | 181.68] loss=2.03 avg=2.38\n",
      "[830 | 181.73] loss=1.92 avg=2.38\n",
      "[831 | 181.78] loss=1.14 avg=2.37\n",
      "[832 | 181.84] loss=3.01 avg=2.37\n",
      "[833 | 181.89] loss=2.42 avg=2.37\n",
      "[834 | 181.94] loss=2.53 avg=2.38\n",
      "[835 | 182.00] loss=1.06 avg=2.36\n",
      "[836 | 182.05] loss=3.82 avg=2.38\n",
      "[837 | 182.11] loss=3.38 avg=2.39\n",
      "[838 | 182.16] loss=2.60 avg=2.39\n",
      "[839 | 182.21] loss=2.91 avg=2.39\n",
      "[840 | 182.26] loss=2.48 avg=2.40\n",
      "[841 | 182.32] loss=1.01 avg=2.38\n",
      "[842 | 182.37] loss=1.76 avg=2.38\n",
      "[843 | 182.42] loss=1.60 avg=2.37\n",
      "[844 | 182.48] loss=2.73 avg=2.37\n",
      "[845 | 182.53] loss=1.84 avg=2.37\n",
      "[846 | 182.58] loss=2.51 avg=2.37\n",
      "[847 | 182.63] loss=1.74 avg=2.36\n",
      "[848 | 182.69] loss=2.50 avg=2.36\n",
      "[849 | 182.74] loss=3.71 avg=2.38\n",
      "[850 | 182.79] loss=1.89 avg=2.37\n",
      "[851 | 182.85] loss=2.68 avg=2.37\n",
      "[852 | 182.90] loss=2.51 avg=2.38\n",
      "[853 | 182.95] loss=4.15 avg=2.39\n",
      "[854 | 183.00] loss=3.21 avg=2.40\n",
      "[855 | 183.05] loss=2.22 avg=2.40\n",
      "[856 | 183.10] loss=1.97 avg=2.40\n",
      "[857 | 183.15] loss=3.37 avg=2.41\n",
      "[858 | 183.20] loss=2.95 avg=2.41\n",
      "[859 | 183.25] loss=2.22 avg=2.41\n",
      "[860 | 183.30] loss=1.87 avg=2.40\n",
      "[861 | 183.36] loss=0.99 avg=2.39\n",
      "[862 | 183.41] loss=2.23 avg=2.39\n",
      "[863 | 183.46] loss=1.97 avg=2.38\n",
      "[864 | 183.51] loss=1.84 avg=2.38\n",
      "[865 | 183.56] loss=2.04 avg=2.37\n",
      "[866 | 183.62] loss=3.12 avg=2.38\n",
      "[867 | 183.67] loss=2.41 avg=2.38\n",
      "[868 | 183.72] loss=2.30 avg=2.38\n",
      "[869 | 183.77] loss=3.21 avg=2.39\n",
      "[870 | 183.82] loss=0.99 avg=2.38\n",
      "[871 | 183.87] loss=3.37 avg=2.39\n",
      "[872 | 183.92] loss=3.20 avg=2.39\n",
      "[873 | 183.98] loss=4.19 avg=2.41\n",
      "[874 | 184.03] loss=2.70 avg=2.41\n",
      "[875 | 184.08] loss=2.84 avg=2.42\n",
      "[876 | 184.13] loss=2.88 avg=2.42\n",
      "[877 | 184.19] loss=1.63 avg=2.42\n",
      "[878 | 184.24] loss=1.66 avg=2.41\n",
      "[879 | 184.30] loss=3.37 avg=2.42\n",
      "[880 | 184.35] loss=2.50 avg=2.42\n",
      "[881 | 184.40] loss=1.55 avg=2.41\n",
      "[882 | 184.45] loss=2.27 avg=2.41\n",
      "[883 | 184.50] loss=3.11 avg=2.42\n",
      "[884 | 184.56] loss=3.62 avg=2.43\n",
      "[885 | 184.61] loss=1.90 avg=2.42\n",
      "[886 | 184.66] loss=1.27 avg=2.41\n",
      "[887 | 184.72] loss=2.90 avg=2.42\n",
      "[888 | 184.77] loss=1.04 avg=2.40\n",
      "[889 | 184.82] loss=4.45 avg=2.42\n",
      "[890 | 184.87] loss=4.00 avg=2.44\n",
      "[891 | 184.92] loss=1.45 avg=2.43\n",
      "[892 | 184.97] loss=1.65 avg=2.42\n",
      "[893 | 185.03] loss=2.31 avg=2.42\n",
      "[894 | 185.08] loss=1.21 avg=2.41\n",
      "[895 | 185.14] loss=1.69 avg=2.40\n",
      "[896 | 185.19] loss=3.14 avg=2.41\n",
      "[897 | 185.24] loss=3.41 avg=2.42\n",
      "[898 | 185.30] loss=2.52 avg=2.42\n",
      "[899 | 185.35] loss=3.24 avg=2.43\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 48.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "}\n",
      "     public void setTime(int a) throws java.util.TimerException\n",
      "{\n",
      "           jotjr(org.dldldp.jtjjw.utils.tldh.cdr_get_tldinfo());\n",
      "    }\n",
      "    java.lang.String getTime = java.util.Timer;\n",
      "    java.util.TimerTimer.SET_TTL_TIME(a);\n",
      "    java.util.TimerTimer.SET_TTL_TIME(dldl);\n",
      "    java.io.File file;\n",
      "    start_tcp();\n",
      "    java.io.File p;\n",
      "    cpch(sdb_get_timestamp(), \"java0_time_with_a_new_time_with_a_new_time_with_a_new_time_with_a_new_timeout\");\n",
      "    cpch(sdb_get_timestamp(), \"java1_time_with_a_new_time_with_a_new_time_with_a_new_time_with_a_new_timeout_with_new_set_time_with_time_with_new_time_with_new_time_with_new_timeout_with_new_time_with_new_timeout_with_new_timeout_with_new_timeout_with_new_time_from_org_dldp.get_time()) throws java.util.TimeoutException\n",
      "   java.util.TimerTimer.set_timestamp(getTime.to_time()));\n",
      "   java.util.TimerTimer.set_timestamp(getTime.last_set().to_time()));\n",
      "   java.util.TimerTimer.set_timestamp(getTime.time_set().to_time()));\n",
      "   java.io.File temp = jotjr(org.dldl.time.time.to_temp());\n",
      "   java.util.TimerTimer.set_timestamp(temp);\n",
      "\n",
      "   cdr_set_time();\n",
      "   p = java.io.File p;\n",
      "   cpch(((java.io.File) (start_tcp())));\n",
      "   cpch(sdb_get_timestamp(), \"%s\", getTime));\n",
      "   cpch(sdb_get_timestamp(), \"java1_time_with_a_new_time_with_a_new_time_without_a_new_timeout_with_new_time_with_a_new_timeout_with_new_timeout_with_new_time_with_new_timeout_with_new_timeout_with_new_time_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_time_with_new_time_with_new_timeout_with_new_timeout_with_new_timeout_with_new_time_time_with_new_timeout_with_new_time_with_new_timeout_with_new_time_with_new_time_with_new_time_with_new_time_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new_timeout_with_new\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[900 | 201.35] validation loss = 2.50\n",
      "[900 | 201.40] loss=4.12 avg=2.44\n",
      "[901 | 201.46] loss=4.59 avg=2.47\n",
      "[902 | 201.51] loss=1.48 avg=2.46\n",
      "[903 | 201.57] loss=1.80 avg=2.45\n",
      "[904 | 201.62] loss=2.45 avg=2.45\n",
      "[905 | 201.67] loss=3.02 avg=2.45\n",
      "[906 | 201.73] loss=3.70 avg=2.47\n",
      "[907 | 201.79] loss=1.72 avg=2.46\n",
      "[908 | 201.84] loss=1.62 avg=2.45\n",
      "[909 | 201.90] loss=3.02 avg=2.46\n",
      "[910 | 201.96] loss=1.56 avg=2.45\n",
      "[911 | 202.01] loss=1.16 avg=2.44\n",
      "[912 | 202.06] loss=2.93 avg=2.44\n",
      "[913 | 202.12] loss=1.39 avg=2.43\n",
      "[914 | 202.17] loss=2.16 avg=2.43\n",
      "[915 | 202.22] loss=2.44 avg=2.43\n",
      "[916 | 202.27] loss=2.54 avg=2.43\n",
      "[917 | 202.33] loss=1.43 avg=2.42\n",
      "[918 | 202.38] loss=0.33 avg=2.40\n",
      "[919 | 202.44] loss=1.09 avg=2.38\n",
      "[920 | 202.49] loss=2.23 avg=2.38\n",
      "[921 | 202.55] loss=1.57 avg=2.37\n",
      "[922 | 202.60] loss=0.97 avg=2.36\n",
      "[923 | 202.65] loss=2.75 avg=2.36\n",
      "[924 | 202.70] loss=1.42 avg=2.35\n",
      "[925 | 202.76] loss=1.48 avg=2.35\n",
      "[926 | 202.81] loss=2.59 avg=2.35\n",
      "[927 | 202.86] loss=3.52 avg=2.36\n",
      "[928 | 202.92] loss=2.97 avg=2.37\n",
      "[929 | 202.97] loss=2.86 avg=2.37\n",
      "[930 | 203.02] loss=1.89 avg=2.37\n",
      "[931 | 203.07] loss=3.93 avg=2.38\n",
      "[932 | 203.13] loss=2.37 avg=2.38\n",
      "[933 | 203.18] loss=2.67 avg=2.38\n",
      "[934 | 203.23] loss=2.69 avg=2.39\n",
      "[935 | 203.29] loss=2.17 avg=2.39\n",
      "[936 | 203.34] loss=1.45 avg=2.38\n",
      "[937 | 203.39] loss=2.52 avg=2.38\n",
      "[938 | 203.45] loss=2.96 avg=2.38\n",
      "[939 | 203.50] loss=4.02 avg=2.40\n",
      "[940 | 203.54] loss=2.68 avg=2.40\n",
      "[941 | 203.60] loss=1.38 avg=2.39\n",
      "[942 | 203.65] loss=2.91 avg=2.40\n",
      "[943 | 203.70] loss=1.71 avg=2.39\n",
      "[944 | 203.75] loss=1.99 avg=2.39\n",
      "[945 | 203.81] loss=3.60 avg=2.40\n",
      "[946 | 203.86] loss=1.32 avg=2.39\n",
      "[947 | 203.91] loss=2.78 avg=2.39\n",
      "[948 | 203.97] loss=1.40 avg=2.38\n",
      "[949 | 204.02] loss=1.57 avg=2.37\n",
      "[950 | 204.07] loss=3.32 avg=2.38\n",
      "[951 | 204.12] loss=2.21 avg=2.38\n",
      "[952 | 204.18] loss=1.72 avg=2.37\n",
      "[953 | 204.23] loss=3.59 avg=2.39\n",
      "[954 | 204.28] loss=4.00 avg=2.40\n",
      "[955 | 204.33] loss=2.25 avg=2.40\n",
      "[956 | 204.38] loss=0.93 avg=2.39\n",
      "[957 | 204.43] loss=3.03 avg=2.39\n",
      "[958 | 204.49] loss=4.05 avg=2.41\n",
      "[959 | 204.54] loss=0.91 avg=2.39\n",
      "[960 | 204.60] loss=3.10 avg=2.40\n",
      "[961 | 204.65] loss=1.36 avg=2.39\n",
      "[962 | 204.71] loss=2.78 avg=2.40\n",
      "[963 | 204.76] loss=2.73 avg=2.40\n",
      "[964 | 204.81] loss=1.48 avg=2.39\n",
      "[965 | 204.87] loss=2.52 avg=2.39\n",
      "[966 | 204.92] loss=1.54 avg=2.38\n",
      "[967 | 204.97] loss=1.47 avg=2.37\n",
      "[968 | 205.02] loss=1.74 avg=2.37\n",
      "[969 | 205.08] loss=2.63 avg=2.37\n",
      "[970 | 205.13] loss=4.04 avg=2.39\n",
      "[971 | 205.18] loss=1.46 avg=2.38\n",
      "[972 | 205.24] loss=2.83 avg=2.38\n",
      "[973 | 205.29] loss=1.63 avg=2.37\n",
      "[974 | 205.34] loss=3.16 avg=2.38\n",
      "[975 | 205.40] loss=1.58 avg=2.37\n",
      "[976 | 205.46] loss=1.44 avg=2.36\n",
      "[977 | 205.51] loss=2.74 avg=2.37\n",
      "[978 | 205.56] loss=2.00 avg=2.36\n",
      "[979 | 205.62] loss=1.87 avg=2.36\n",
      "[980 | 205.67] loss=3.44 avg=2.37\n",
      "[981 | 205.73] loss=2.63 avg=2.37\n",
      "[982 | 205.78] loss=3.73 avg=2.39\n",
      "[983 | 205.83] loss=1.79 avg=2.38\n",
      "[984 | 205.88] loss=3.72 avg=2.39\n",
      "[985 | 205.94] loss=3.35 avg=2.40\n",
      "[986 | 205.99] loss=3.97 avg=2.42\n",
      "[987 | 206.04] loss=1.88 avg=2.41\n",
      "[988 | 206.09] loss=2.48 avg=2.41\n",
      "[989 | 206.15] loss=3.09 avg=2.42\n",
      "[990 | 206.20] loss=3.69 avg=2.43\n",
      "[991 | 206.25] loss=2.57 avg=2.44\n",
      "[992 | 206.31] loss=1.58 avg=2.43\n",
      "[993 | 206.36] loss=3.84 avg=2.44\n",
      "[994 | 206.41] loss=3.60 avg=2.45\n",
      "[995 | 206.46] loss=1.23 avg=2.44\n",
      "[996 | 206.51] loss=3.27 avg=2.45\n",
      "[997 | 206.56] loss=2.57 avg=2.45\n",
      "[998 | 206.62] loss=1.85 avg=2.44\n",
      "[999 | 206.67] loss=1.33 avg=2.43\n",
      "Saving checkpoint/m1_vulnerability/model-1000\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " (\"\t:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t };\n",
      "\\\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUTION\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[]\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[]].\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((( (((((()))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))\n",
      "                     \n",
      "     \n",
      "   \n",
      "                  \n",
      "                \n",
      "                \n",
      "                \n",
      "                   \n",
      "                     \n",
      "                     \n",
      "                       \n",
      "        ))))\n",
      "       }\n",
      "\n",
      "    \n",
      "              }\n",
      "   }\n",
      "     \n",
      "\n",
      "  \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 48.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000 | 230.19] validation loss = 2.47\n",
      "[1000 | 230.25] loss=3.95 avg=2.45\n",
      "[1001 | 230.30] loss=2.54 avg=2.45\n",
      "[1002 | 230.36] loss=1.74 avg=2.44\n",
      "[1003 | 230.41] loss=1.83 avg=2.44\n",
      "[1004 | 230.46] loss=1.87 avg=2.43\n",
      "[1005 | 230.51] loss=2.69 avg=2.43\n",
      "[1006 | 230.57] loss=2.87 avg=2.44\n",
      "[1007 | 230.62] loss=1.78 avg=2.43\n",
      "[1008 | 230.67] loss=1.76 avg=2.42\n",
      "[1009 | 230.73] loss=2.69 avg=2.43\n",
      "[1010 | 230.78] loss=2.77 avg=2.43\n",
      "[1011 | 230.83] loss=2.24 avg=2.43\n",
      "[1012 | 230.89] loss=2.72 avg=2.43\n",
      "[1013 | 230.94] loss=2.52 avg=2.43\n",
      "[1014 | 230.99] loss=3.00 avg=2.44\n",
      "[1015 | 231.04] loss=3.02 avg=2.44\n",
      "[1016 | 231.10] loss=2.78 avg=2.45\n",
      "[1017 | 231.15] loss=1.67 avg=2.44\n",
      "[1018 | 231.20] loss=1.68 avg=2.43\n",
      "[1019 | 231.26] loss=1.28 avg=2.42\n",
      "[1020 | 231.31] loss=2.10 avg=2.42\n",
      "[1021 | 231.36] loss=2.85 avg=2.42\n",
      "[1022 | 231.41] loss=3.31 avg=2.43\n",
      "[1023 | 231.46] loss=3.41 avg=2.44\n",
      "[1024 | 231.51] loss=2.09 avg=2.44\n",
      "[1025 | 231.56] loss=1.77 avg=2.43\n",
      "[1026 | 231.61] loss=0.94 avg=2.41\n",
      "[1027 | 231.66] loss=4.47 avg=2.43\n",
      "[1028 | 231.72] loss=2.14 avg=2.43\n",
      "[1029 | 231.77] loss=1.13 avg=2.42\n",
      "[1030 | 231.82] loss=2.52 avg=2.42\n",
      "[1031 | 231.87] loss=3.23 avg=2.43\n",
      "[1032 | 231.92] loss=1.53 avg=2.42\n",
      "[1033 | 231.98] loss=2.21 avg=2.42\n",
      "[1034 | 232.03] loss=2.69 avg=2.42\n",
      "[1035 | 232.08] loss=2.82 avg=2.42\n",
      "[1036 | 232.13] loss=1.24 avg=2.41\n",
      "[1037 | 232.18] loss=1.41 avg=2.40\n",
      "[1038 | 232.24] loss=3.36 avg=2.41\n",
      "[1039 | 232.29] loss=1.99 avg=2.41\n",
      "[1040 | 232.35] loss=1.28 avg=2.40\n",
      "[1041 | 232.40] loss=0.98 avg=2.38\n",
      "[1042 | 232.45] loss=1.60 avg=2.37\n",
      "[1043 | 232.51] loss=3.60 avg=2.39\n",
      "[1044 | 232.56] loss=1.32 avg=2.38\n",
      "[1045 | 232.61] loss=2.18 avg=2.37\n",
      "[1046 | 232.66] loss=2.92 avg=2.38\n",
      "[1047 | 232.71] loss=1.36 avg=2.37\n",
      "[1048 | 232.76] loss=1.23 avg=2.36\n",
      "[1049 | 232.81] loss=1.71 avg=2.35\n",
      "[1050 | 232.86] loss=2.27 avg=2.35\n",
      "[1051 | 232.91] loss=3.09 avg=2.36\n",
      "[1052 | 232.97] loss=1.19 avg=2.35\n",
      "[1053 | 233.02] loss=2.19 avg=2.34\n",
      "[1054 | 233.07] loss=1.42 avg=2.34\n",
      "[1055 | 233.12] loss=2.34 avg=2.34\n",
      "[1056 | 233.17] loss=2.00 avg=2.33\n",
      "[1057 | 233.23] loss=2.50 avg=2.33\n",
      "[1058 | 233.28] loss=2.78 avg=2.34\n",
      "[1059 | 233.33] loss=1.56 avg=2.33\n",
      "[1060 | 233.38] loss=1.13 avg=2.32\n",
      "[1061 | 233.43] loss=3.22 avg=2.33\n",
      "[1062 | 233.48] loss=3.22 avg=2.34\n",
      "[1063 | 233.53] loss=2.25 avg=2.34\n",
      "[1064 | 233.58] loss=2.57 avg=2.34\n",
      "[1065 | 233.63] loss=3.47 avg=2.35\n",
      "[1066 | 233.68] loss=2.21 avg=2.35\n",
      "[1067 | 233.73] loss=2.70 avg=2.35\n",
      "[1068 | 233.78] loss=1.56 avg=2.34\n",
      "[1069 | 233.84] loss=2.15 avg=2.34\n",
      "[1070 | 233.89] loss=3.30 avg=2.35\n",
      "[1071 | 233.94] loss=1.88 avg=2.35\n",
      "[1072 | 233.99] loss=1.76 avg=2.34\n",
      "[1073 | 234.05] loss=2.02 avg=2.34\n",
      "[1074 | 234.10] loss=1.44 avg=2.33\n",
      "[1075 | 234.15] loss=2.84 avg=2.33\n",
      "[1076 | 234.21] loss=1.62 avg=2.33\n",
      "[1077 | 234.26] loss=1.57 avg=2.32\n",
      "[1078 | 234.31] loss=3.03 avg=2.33\n",
      "[1079 | 234.36] loss=2.11 avg=2.32\n",
      "[1080 | 234.42] loss=0.84 avg=2.31\n",
      "[1081 | 234.47] loss=1.82 avg=2.30\n",
      "[1082 | 234.52] loss=2.75 avg=2.31\n",
      "[1083 | 234.57] loss=1.93 avg=2.30\n",
      "[1084 | 234.63] loss=1.81 avg=2.30\n",
      "[1085 | 234.68] loss=3.81 avg=2.31\n",
      "[1086 | 234.73] loss=2.23 avg=2.31\n",
      "[1087 | 234.78] loss=0.93 avg=2.30\n",
      "[1088 | 234.84] loss=1.27 avg=2.29\n",
      "[1089 | 234.89] loss=2.51 avg=2.29\n",
      "[1090 | 234.94] loss=1.74 avg=2.29\n",
      "[1091 | 234.99] loss=2.09 avg=2.28\n",
      "[1092 | 235.04] loss=2.79 avg=2.29\n",
      "[1093 | 235.10] loss=3.68 avg=2.30\n",
      "[1094 | 235.15] loss=2.09 avg=2.30\n",
      "[1095 | 235.20] loss=2.15 avg=2.30\n",
      "[1096 | 235.26] loss=2.11 avg=2.30\n",
      "[1097 | 235.31] loss=2.93 avg=2.30\n",
      "[1098 | 235.36] loss=1.89 avg=2.30\n",
      "[1099 | 235.41] loss=2.43 avg=2.30\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 49.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " if the\n",
      "                      \n",
      "                        \n",
      "                     \n",
      "                      \n",
      "                   \n",
      "                 \n",
      "             }\n",
      "           \n",
      "          \n",
      "          return com.getcom.jquery.eventmanager.EventManagerEventEvent.PARAM();\n",
      "        \n",
      "         com.getcommo.logging.Logger.logger(TAG, \"org.jquery.eventmanager.eventmanager.eventmanagerEvent_event_result : %s\", com.getcommo.logging.Logger.logger_name));\n",
      "      \n",
      "      org.bom.coupon.Coupon.loadEvent((com.getcommo.eventmanager.eventmanager.eventmanager.eventmanagerEvent_event_result, com.getcommo.logging.Logger.logger_name));\n",
      "     } private void onEvent(EventEvent event) {\n",
      "      com.getcommo.logging.Logger.logger_info(logger.log.logger(TAG, \"org.getcommo.logging.eventmanager.eventmanagerEvent_event_result : %s\", event).logger_name);\n",
      "     \n",
      "  }\n",
      "  \n",
      "   java.lang.String getMessage(String name, int msg) {\n",
      "      for (java.lang.String j = com.getcommo.console.ConsoleApplication().start(org.getcommo.eventmanager.eventmanager.eventmanagerEvent_event_result, name).equalsIgnoreCase());\n",
      "     \n",
      "     j = org.bom.coupon.Coupon.getEvent(\"org.getcommo.logging.eventmanager.eventmanagerEvent_event_result\", event);\n",
      "     \n",
      "     java.lang.String message_name = j.getMessageToBePosed(name, msg);\n",
      "    \n",
      "    if (org.getcommo.util.EventManager event) {\n",
      "       org.getcommo.event.EventManagerEvent result = com.getcommo.event.EventManager.createEvent(com.getcommo.event.EventListener, event->info_name, \"org.getcommo.logging.eventmanager.eventmanagerEvent_event_result \"));\n",
      "        org.getcommo.event.EventListener result = org.getcommo.event.EventListener.createEvent(com.getcommo.event.EventListener.pre_event, event->info_name);\n",
      "     \n",
      "      \n",
      "       result.set(org.getcommo.logging.eventmanager.event.event_event_result, \"org.getcommo.logging.eventmanager.eventmanagerEvent_event_result_message_message\", message));\n",
      "     \n",
      "     else {\n",
      "        org.getcommo.event.EventListener result = com.getcommo.event.EventListener.createEvent(org.getcommo.event.EventListener.get_event, event->info_name, result);\n",
      "     \n",
      "      if (org = orgs.getcommo.event.event.event.event.result) {\n",
      "      \n",
      "       org.getcommo.event.EventListener result = orgs.getcommo.event.EventListener.get_event, event.info_id);\n",
      "     \n",
      "     \n",
      "     \n",
      "      if (org = orgs.getcommo.event.event.event.result) {\n",
      "       \n",
      "  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1100 | 251.33] validation loss = 2.45\n",
      "[1100 | 251.39] loss=2.95 avg=2.31\n",
      "[1101 | 251.44] loss=2.56 avg=2.31\n",
      "[1102 | 251.50] loss=2.58 avg=2.31\n",
      "[1103 | 251.55] loss=2.90 avg=2.32\n",
      "[1104 | 251.60] loss=2.46 avg=2.32\n",
      "[1105 | 251.66] loss=3.11 avg=2.33\n",
      "[1106 | 251.71] loss=3.03 avg=2.34\n",
      "[1107 | 251.76] loss=3.24 avg=2.34\n",
      "[1108 | 251.82] loss=2.10 avg=2.34\n",
      "[1109 | 251.87] loss=2.78 avg=2.35\n",
      "[1110 | 251.93] loss=1.28 avg=2.34\n",
      "[1111 | 251.98] loss=1.66 avg=2.33\n",
      "[1112 | 252.04] loss=3.18 avg=2.34\n",
      "[1113 | 252.09] loss=2.71 avg=2.34\n",
      "[1114 | 252.14] loss=3.52 avg=2.35\n",
      "[1115 | 252.19] loss=0.99 avg=2.34\n",
      "[1116 | 252.24] loss=3.07 avg=2.35\n",
      "[1117 | 252.30] loss=2.67 avg=2.35\n",
      "[1118 | 252.35] loss=2.58 avg=2.35\n",
      "[1119 | 252.40] loss=1.73 avg=2.35\n",
      "[1120 | 252.45] loss=2.89 avg=2.35\n",
      "[1121 | 252.51] loss=1.62 avg=2.34\n",
      "[1122 | 252.56] loss=2.60 avg=2.35\n",
      "[1123 | 252.62] loss=2.99 avg=2.35\n",
      "[1124 | 252.67] loss=1.87 avg=2.35\n",
      "[1125 | 252.72] loss=2.42 avg=2.35\n",
      "[1126 | 252.77] loss=1.19 avg=2.34\n",
      "[1127 | 252.82] loss=2.56 avg=2.34\n",
      "[1128 | 252.88] loss=3.10 avg=2.35\n",
      "[1129 | 252.93] loss=2.31 avg=2.35\n",
      "[1130 | 252.98] loss=1.64 avg=2.34\n",
      "[1131 | 253.04] loss=1.64 avg=2.33\n",
      "[1132 | 253.09] loss=2.77 avg=2.34\n",
      "[1133 | 253.14] loss=3.02 avg=2.34\n",
      "[1134 | 253.19] loss=2.11 avg=2.34\n",
      "[1135 | 253.25] loss=1.49 avg=2.33\n",
      "[1136 | 253.30] loss=1.57 avg=2.33\n",
      "[1137 | 253.36] loss=2.94 avg=2.33\n",
      "[1138 | 253.41] loss=1.37 avg=2.32\n",
      "[1139 | 253.46] loss=3.12 avg=2.33\n",
      "[1140 | 253.52] loss=1.22 avg=2.32\n",
      "[1141 | 253.57] loss=1.62 avg=2.31\n",
      "[1142 | 253.62] loss=1.57 avg=2.30\n",
      "[1143 | 253.68] loss=2.33 avg=2.30\n",
      "[1144 | 253.73] loss=3.06 avg=2.31\n",
      "[1145 | 253.78] loss=2.02 avg=2.31\n",
      "[1146 | 253.84] loss=3.17 avg=2.32\n",
      "[1147 | 253.89] loss=3.43 avg=2.33\n",
      "[1148 | 253.94] loss=0.83 avg=2.31\n",
      "[1149 | 254.00] loss=3.16 avg=2.32\n",
      "[1150 | 254.05] loss=2.70 avg=2.33\n",
      "[1151 | 254.10] loss=2.53 avg=2.33\n",
      "[1152 | 254.15] loss=2.58 avg=2.33\n",
      "[1153 | 254.21] loss=2.05 avg=2.33\n",
      "[1154 | 254.26] loss=1.56 avg=2.32\n",
      "[1155 | 254.31] loss=3.32 avg=2.33\n",
      "[1156 | 254.37] loss=2.08 avg=2.33\n",
      "[1157 | 254.42] loss=2.16 avg=2.33\n",
      "[1158 | 254.47] loss=3.45 avg=2.34\n",
      "[1159 | 254.53] loss=1.69 avg=2.33\n",
      "[1160 | 254.58] loss=2.39 avg=2.33\n",
      "[1161 | 254.63] loss=2.61 avg=2.33\n",
      "[1162 | 254.69] loss=2.14 avg=2.33\n",
      "[1163 | 254.74] loss=2.55 avg=2.33\n",
      "[1164 | 254.79] loss=1.52 avg=2.33\n",
      "[1165 | 254.84] loss=2.28 avg=2.33\n",
      "[1166 | 254.90] loss=1.98 avg=2.32\n",
      "[1167 | 254.95] loss=3.03 avg=2.33\n",
      "[1168 | 255.00] loss=1.69 avg=2.32\n",
      "[1169 | 255.06] loss=1.49 avg=2.31\n",
      "[1170 | 255.11] loss=2.66 avg=2.32\n",
      "[1171 | 255.16] loss=1.81 avg=2.31\n",
      "[1172 | 255.21] loss=1.73 avg=2.31\n",
      "[1173 | 255.27] loss=2.48 avg=2.31\n",
      "[1174 | 255.32] loss=2.97 avg=2.32\n",
      "[1175 | 255.37] loss=1.41 avg=2.31\n",
      "[1176 | 255.43] loss=2.84 avg=2.31\n",
      "[1177 | 255.48] loss=2.03 avg=2.31\n",
      "[1178 | 255.54] loss=1.46 avg=2.30\n",
      "[1179 | 255.59] loss=2.35 avg=2.30\n",
      "[1180 | 255.64] loss=2.96 avg=2.31\n",
      "[1181 | 255.69] loss=3.06 avg=2.31\n",
      "[1182 | 255.74] loss=2.93 avg=2.32\n",
      "[1183 | 255.79] loss=2.91 avg=2.33\n",
      "[1184 | 255.85] loss=2.20 avg=2.33\n",
      "[1185 | 255.90] loss=2.40 avg=2.33\n",
      "[1186 | 255.95] loss=3.44 avg=2.34\n",
      "[1187 | 256.00] loss=2.69 avg=2.34\n",
      "[1188 | 256.05] loss=1.17 avg=2.33\n",
      "[1189 | 256.10] loss=1.49 avg=2.32\n",
      "[1190 | 256.16] loss=2.56 avg=2.32\n",
      "[1191 | 256.21] loss=1.33 avg=2.31\n",
      "[1192 | 256.26] loss=0.88 avg=2.30\n",
      "[1193 | 256.31] loss=1.32 avg=2.29\n",
      "[1194 | 256.37] loss=2.70 avg=2.29\n",
      "[1195 | 256.42] loss=0.87 avg=2.28\n",
      "[1196 | 256.47] loss=1.88 avg=2.28\n",
      "[1197 | 256.53] loss=1.96 avg=2.27\n",
      "[1198 | 256.58] loss=0.66 avg=2.26\n",
      "[1199 | 256.63] loss=3.65 avg=2.27\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "\t\t\t\t\t();\n",
      "\n",
      "                           =\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t };\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t };\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t };\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t\t\t;\t\t\t\t\t\t\t;\t\t\t\t\t;\t\t\t\t\t;\t\t\t;\t\t;\t\t';\t\t\t\t\t\t\t\t;\t\t\t\t;\t\t;\t\t;\t\t;\t\t;\t';\t\t\t\t\t\t\t\t\t\t;\t\t;';\t\t\t\t\t\t;\t';\t\t';\t';\t';\t';\t';';\t';';\t';';')\t';';\t','';\t','';';)[]';];\t\t\t\t';\t';\t','';\t;';,'';';)';]);\t\t\t';';\t';';','';','';','';','';',';','';','';','';');\t\t';';';','';','',';','','';',';','';','','';','';','';','';','';','';','';','';',';','';','';','';','',';','');';',';','']';\t\t\t\t\t';';','';','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','',';';';';';';';';';';',',',',',',',','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','','\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1200 | 272.60] validation loss = 2.43\n",
      "[1200 | 272.65] loss=2.49 avg=2.27\n",
      "[1201 | 272.71] loss=1.06 avg=2.26\n",
      "[1202 | 272.76] loss=1.12 avg=2.25\n",
      "[1203 | 272.81] loss=2.27 avg=2.25\n",
      "[1204 | 272.87] loss=2.53 avg=2.25\n",
      "[1205 | 272.92] loss=2.71 avg=2.26\n",
      "[1206 | 272.97] loss=1.28 avg=2.25\n",
      "[1207 | 273.02] loss=2.51 avg=2.25\n",
      "[1208 | 273.07] loss=1.27 avg=2.24\n",
      "[1209 | 273.13] loss=3.19 avg=2.25\n",
      "[1210 | 273.18] loss=3.65 avg=2.26\n",
      "[1211 | 273.23] loss=2.86 avg=2.27\n",
      "[1212 | 273.28] loss=1.35 avg=2.26\n",
      "[1213 | 273.34] loss=3.54 avg=2.27\n",
      "[1214 | 273.39] loss=1.57 avg=2.27\n",
      "[1215 | 273.45] loss=2.83 avg=2.27\n",
      "[1216 | 273.50] loss=2.22 avg=2.27\n",
      "[1217 | 273.55] loss=3.30 avg=2.28\n",
      "[1218 | 273.61] loss=2.31 avg=2.28\n",
      "[1219 | 273.66] loss=1.15 avg=2.27\n",
      "[1220 | 273.71] loss=2.08 avg=2.27\n",
      "[1221 | 273.77] loss=1.72 avg=2.26\n",
      "[1222 | 273.82] loss=1.85 avg=2.26\n",
      "[1223 | 273.87] loss=1.96 avg=2.26\n",
      "[1224 | 273.92] loss=2.44 avg=2.26\n",
      "[1225 | 273.97] loss=2.80 avg=2.26\n",
      "[1226 | 274.02] loss=2.95 avg=2.27\n",
      "[1227 | 274.08] loss=1.72 avg=2.26\n",
      "[1228 | 274.13] loss=2.40 avg=2.27\n",
      "[1229 | 274.18] loss=1.43 avg=2.26\n",
      "[1230 | 274.23] loss=2.53 avg=2.26\n",
      "[1231 | 274.28] loss=1.55 avg=2.25\n",
      "[1232 | 274.34] loss=1.69 avg=2.25\n",
      "[1233 | 274.39] loss=1.86 avg=2.24\n",
      "[1234 | 274.45] loss=2.33 avg=2.24\n",
      "[1235 | 274.50] loss=2.98 avg=2.25\n",
      "[1236 | 274.55] loss=3.06 avg=2.26\n",
      "[1237 | 274.61] loss=2.54 avg=2.26\n",
      "[1238 | 274.66] loss=3.73 avg=2.28\n",
      "[1239 | 274.71] loss=2.61 avg=2.28\n",
      "[1240 | 274.76] loss=2.72 avg=2.28\n",
      "[1241 | 274.81] loss=1.66 avg=2.28\n",
      "[1242 | 274.86] loss=2.79 avg=2.28\n",
      "[1243 | 274.92] loss=2.03 avg=2.28\n",
      "[1244 | 274.98] loss=2.82 avg=2.29\n",
      "[1245 | 275.03] loss=3.15 avg=2.29\n",
      "[1246 | 275.08] loss=2.55 avg=2.30\n",
      "[1247 | 275.14] loss=3.78 avg=2.31\n",
      "[1248 | 275.19] loss=3.25 avg=2.32\n",
      "[1249 | 275.24] loss=1.49 avg=2.31\n",
      "[1250 | 275.30] loss=1.53 avg=2.31\n",
      "[1251 | 275.35] loss=1.51 avg=2.30\n",
      "[1252 | 275.40] loss=1.43 avg=2.29\n",
      "[1253 | 275.46] loss=1.95 avg=2.29\n",
      "[1254 | 275.51] loss=3.23 avg=2.29\n",
      "[1255 | 275.56] loss=2.10 avg=2.29\n",
      "[1256 | 275.62] loss=1.28 avg=2.28\n",
      "[1257 | 275.67] loss=2.09 avg=2.28\n",
      "[1258 | 275.72] loss=3.24 avg=2.29\n",
      "[1259 | 275.77] loss=1.19 avg=2.28\n",
      "[1260 | 275.82] loss=1.48 avg=2.27\n",
      "[1261 | 275.88] loss=2.84 avg=2.28\n",
      "[1262 | 275.93] loss=3.33 avg=2.29\n",
      "[1263 | 275.98] loss=1.69 avg=2.28\n",
      "[1264 | 276.04] loss=2.54 avg=2.28\n",
      "[1265 | 276.09] loss=2.01 avg=2.28\n",
      "[1266 | 276.14] loss=1.87 avg=2.28\n",
      "[1267 | 276.20] loss=2.60 avg=2.28\n",
      "[1268 | 276.25] loss=1.75 avg=2.28\n",
      "[1269 | 276.31] loss=2.38 avg=2.28\n",
      "[1270 | 276.36] loss=1.94 avg=2.27\n",
      "[1271 | 276.41] loss=1.32 avg=2.26\n",
      "[1272 | 276.46] loss=2.11 avg=2.26\n",
      "[1273 | 276.52] loss=1.17 avg=2.25\n",
      "[1274 | 276.58] loss=2.30 avg=2.25\n",
      "[1275 | 276.63] loss=2.95 avg=2.26\n",
      "[1276 | 276.68] loss=1.02 avg=2.25\n",
      "[1277 | 276.73] loss=2.58 avg=2.25\n",
      "[1278 | 276.79] loss=1.49 avg=2.24\n",
      "[1279 | 276.84] loss=3.70 avg=2.26\n",
      "[1280 | 276.89] loss=3.47 avg=2.27\n",
      "[1281 | 276.95] loss=3.78 avg=2.28\n",
      "[1282 | 277.00] loss=2.36 avg=2.28\n",
      "[1283 | 277.05] loss=2.22 avg=2.28\n",
      "[1284 | 277.11] loss=2.07 avg=2.28\n",
      "[1285 | 277.17] loss=1.69 avg=2.28\n",
      "[1286 | 277.23] loss=2.93 avg=2.28\n",
      "[1287 | 277.28] loss=2.27 avg=2.28\n",
      "[1288 | 277.33] loss=1.60 avg=2.28\n",
      "[1289 | 277.39] loss=3.60 avg=2.29\n",
      "[1290 | 277.44] loss=2.94 avg=2.30\n",
      "[1291 | 277.50] loss=2.45 avg=2.30\n",
      "[1292 | 277.55] loss=2.18 avg=2.30\n",
      "[1293 | 277.60] loss=1.38 avg=2.29\n",
      "[1294 | 277.65] loss=1.92 avg=2.28\n",
      "[1295 | 277.70] loss=2.78 avg=2.29\n",
      "[1296 | 277.76] loss=1.94 avg=2.28\n",
      "[1297 | 277.81] loss=1.96 avg=2.28\n",
      "[1298 | 277.87] loss=2.86 avg=2.29\n",
      "[1299 | 277.92] loss=2.63 avg=2.29\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " { (new org.crc.changer.clang.cxcl.java.util.ClassBuilder<java.lang.String>(), java.lang.String));\n",
      "             }));\n",
      "         }\n",
      "         \n",
      "         assert((this.name == java.io.FileExpr.toString().indexOf(this.jfile)) == 0);\n",
      "        assert(this.name == java.io.FileExpr.valueOf(this.jfile)) == 0);\n",
      "       \n",
      "       if (this.java.lang.string.isEmptyOrEmpty(this.name)) {\n",
      "             this.java.lang.string.isEmptyToFirstEmpty(this.name);\n",
      "        \n",
      "    }\n",
      "       assert(this.jfile == null);\n",
      "      \n",
      "      assert(this.jfile instanceof org.crc.cxcl.cxcl.java.util.ClassUtils.name);\n",
      "     \n",
      "     inList(this.name);\n",
      "     }\n",
      "   case 7 :\n",
      "     inList(this.name);\n",
      "    \n",
      "     inList(this.jfile);\n",
      "   \n",
      "    break;\n",
      "   \n",
      "   case 8 :\n",
      "    inList(this.name);\n",
      "   \n",
      "   inList(_(this));\n",
      "  \n",
      "  \n",
      "   if (_(this instanceof org.crc.cxcl.cxcl.java.util.ClassUtils.type)) {\n",
      "    \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "    assert(this.jar == null);\n",
      "   \n",
      "   }\n",
      "  \n",
      "\n",
      "  \n",
      "   \n",
      "   break;\n",
      "  \n",
      " \n",
      "\n",
      " \n",
      "  \n",
      "   case 9 :\n",
      "   \n",
      "   inList(this.name);\n",
      "  \n",
      "  \n",
      "   break;\n",
      "  \n",
      "  \n",
      "  case 10 :\n",
      "    inList(typeof this.name);\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "   break;\n",
      "  \n",
      "  case 11 :\n",
      "   inList(this.name);\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   if (typeof this.newName) {\n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "    this.newName = new org.crc.cxcl.cxcl.fio.NameFactory.newName();\n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "    if (typeof this.newName instanceof org.crc.cxcl.cxcl.tobio.TypeBuilder.<java.lang.String>()) {\n",
      "    \n",
      "   \n",
      "   \n",
      "    this.newName = null;\n",
      "   \n",
      "  \n",
      "  \n",
      "   else\n",
      "   \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   new org.crc.cxcl.cxcli.Computername {\n",
      "     name : this.name;\n",
      "     string : new org.crc.cxcl.cxcli.DefaultComputername(this.name);\n",
      "    }\n",
      "   \n",
      "  \n",
      "\n",
      "   \n",
      "   new com.crc.cxcli.computername2neworg.crc.cxcli.Computername2neworg.crc.cxcli.Computername(new org.crc.cxcli.computername2neworg.crc.cxcli.computername(0));\n",
      "   \n",
      "  \n",
      "  \n",
      "  \n",
      "   new org.crc.cxcli.computername2neworg.crc.cxcli.computername2neworg.crc.cxcli.Computername(this.new\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1300 | 293.86] validation loss = 2.42\n",
      "[1300 | 293.92] loss=3.18 avg=2.30\n",
      "[1301 | 293.97] loss=3.42 avg=2.31\n",
      "[1302 | 294.02] loss=1.58 avg=2.30\n",
      "[1303 | 294.08] loss=1.77 avg=2.30\n",
      "[1304 | 294.13] loss=1.01 avg=2.28\n",
      "[1305 | 294.18] loss=2.64 avg=2.29\n",
      "[1306 | 294.23] loss=3.62 avg=2.30\n",
      "[1307 | 294.29] loss=3.97 avg=2.32\n",
      "[1308 | 294.34] loss=2.40 avg=2.32\n",
      "[1309 | 294.39] loss=1.86 avg=2.31\n",
      "[1310 | 294.44] loss=1.62 avg=2.31\n",
      "[1311 | 294.50] loss=2.86 avg=2.31\n",
      "[1312 | 294.55] loss=1.58 avg=2.31\n",
      "[1313 | 294.61] loss=1.79 avg=2.30\n",
      "[1314 | 294.66] loss=1.60 avg=2.29\n",
      "[1315 | 294.71] loss=2.75 avg=2.30\n",
      "[1316 | 294.77] loss=1.96 avg=2.29\n",
      "[1317 | 294.82] loss=1.55 avg=2.29\n",
      "[1318 | 294.87] loss=0.97 avg=2.27\n",
      "[1319 | 294.92] loss=1.87 avg=2.27\n",
      "[1320 | 294.98] loss=2.57 avg=2.27\n",
      "[1321 | 295.03] loss=3.82 avg=2.29\n",
      "[1322 | 295.09] loss=1.74 avg=2.28\n",
      "[1323 | 295.14] loss=2.37 avg=2.28\n",
      "[1324 | 295.19] loss=3.01 avg=2.29\n",
      "[1325 | 295.24] loss=2.48 avg=2.29\n",
      "[1326 | 295.30] loss=2.61 avg=2.30\n",
      "[1327 | 295.35] loss=1.64 avg=2.29\n",
      "[1328 | 295.40] loss=1.76 avg=2.28\n",
      "[1329 | 295.46] loss=1.70 avg=2.28\n",
      "[1330 | 295.51] loss=2.95 avg=2.29\n",
      "[1331 | 295.56] loss=1.31 avg=2.28\n",
      "[1332 | 295.61] loss=2.08 avg=2.27\n",
      "[1333 | 295.67] loss=3.07 avg=2.28\n",
      "[1334 | 295.72] loss=2.48 avg=2.28\n",
      "[1335 | 295.78] loss=2.80 avg=2.29\n",
      "[1336 | 295.83] loss=1.88 avg=2.28\n",
      "[1337 | 295.88] loss=1.63 avg=2.28\n",
      "[1338 | 295.93] loss=1.42 avg=2.27\n",
      "[1339 | 295.98] loss=2.38 avg=2.27\n",
      "[1340 | 296.03] loss=1.17 avg=2.26\n",
      "[1341 | 296.09] loss=2.02 avg=2.26\n",
      "[1342 | 296.14] loss=3.23 avg=2.27\n",
      "[1343 | 296.19] loss=1.29 avg=2.26\n",
      "[1344 | 296.24] loss=3.07 avg=2.27\n",
      "[1345 | 296.30] loss=3.30 avg=2.28\n",
      "[1346 | 296.35] loss=1.64 avg=2.27\n",
      "[1347 | 296.41] loss=1.53 avg=2.26\n",
      "[1348 | 296.46] loss=1.16 avg=2.25\n",
      "[1349 | 296.51] loss=1.91 avg=2.25\n",
      "[1350 | 296.56] loss=2.03 avg=2.25\n",
      "[1351 | 296.61] loss=0.71 avg=2.23\n",
      "[1352 | 296.67] loss=3.67 avg=2.24\n",
      "[1353 | 296.72] loss=2.33 avg=2.25\n",
      "[1354 | 296.77] loss=2.45 avg=2.25\n",
      "[1355 | 296.82] loss=2.82 avg=2.25\n",
      "[1356 | 296.87] loss=3.61 avg=2.27\n",
      "[1357 | 296.93] loss=2.53 avg=2.27\n",
      "[1358 | 296.98] loss=1.36 avg=2.26\n",
      "[1359 | 297.04] loss=3.42 avg=2.27\n",
      "[1360 | 297.09] loss=2.31 avg=2.27\n",
      "[1361 | 297.14] loss=2.68 avg=2.28\n",
      "[1362 | 297.20] loss=2.35 avg=2.28\n",
      "[1363 | 297.25] loss=1.77 avg=2.27\n",
      "[1364 | 297.30] loss=2.66 avg=2.28\n",
      "[1365 | 297.36] loss=4.66 avg=2.30\n",
      "[1366 | 297.41] loss=1.65 avg=2.29\n",
      "[1367 | 297.46] loss=2.23 avg=2.29\n",
      "[1368 | 297.52] loss=1.31 avg=2.28\n",
      "[1369 | 297.57] loss=3.52 avg=2.29\n",
      "[1370 | 297.62] loss=3.11 avg=2.30\n",
      "[1371 | 297.67] loss=2.73 avg=2.31\n",
      "[1372 | 297.74] loss=2.46 avg=2.31\n",
      "[1373 | 297.79] loss=2.97 avg=2.32\n",
      "[1374 | 297.84] loss=2.20 avg=2.31\n",
      "[1375 | 297.89] loss=1.66 avg=2.31\n",
      "[1376 | 297.95] loss=3.55 avg=2.32\n",
      "[1377 | 298.00] loss=3.39 avg=2.33\n",
      "[1378 | 298.05] loss=1.71 avg=2.32\n",
      "[1379 | 298.11] loss=1.95 avg=2.32\n",
      "[1380 | 298.17] loss=3.36 avg=2.33\n",
      "[1381 | 298.22] loss=2.35 avg=2.33\n",
      "[1382 | 298.27] loss=3.75 avg=2.35\n",
      "[1383 | 298.33] loss=3.97 avg=2.36\n",
      "[1384 | 298.38] loss=1.49 avg=2.35\n",
      "[1385 | 298.43] loss=2.11 avg=2.35\n",
      "[1386 | 298.49] loss=1.78 avg=2.34\n",
      "[1387 | 298.54] loss=2.51 avg=2.35\n",
      "[1388 | 298.60] loss=2.75 avg=2.35\n",
      "[1389 | 298.65] loss=1.40 avg=2.34\n",
      "[1390 | 298.70] loss=2.26 avg=2.34\n",
      "[1391 | 298.75] loss=2.33 avg=2.34\n",
      "[1392 | 298.81] loss=3.60 avg=2.35\n",
      "[1393 | 298.86] loss=1.39 avg=2.34\n",
      "[1394 | 298.92] loss=1.74 avg=2.34\n",
      "[1395 | 298.97] loss=1.66 avg=2.33\n",
      "[1396 | 299.02] loss=2.25 avg=2.33\n",
      "[1397 | 299.07] loss=3.22 avg=2.34\n",
      "[1398 | 299.12] loss=1.44 avg=2.33\n",
      "[1399 | 299.18] loss=1.97 avg=2.33\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 43.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      ");\n",
      "                                                         \n",
      "                                                     else\n",
      "                                                    \n",
      "                                                 \n",
      "                                               \n",
      "                                               \n",
      "                                    \n",
      "                               \n",
      "                           \n",
      "                              }\n",
      "    }\n",
      "                       }\n",
      "                          \n",
      "                         \n",
      "                        }\n",
      "   }\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "                            \n",
      "                            \n",
      "   \n",
      "\n",
      "   /*\n",
      "\n",
      "    */\n",
      "\n",
      "                        \n",
      "                           \n",
      "                           \n",
      "                       \n",
      "                      \n",
      "                         \n",
      "                        \n",
      "                        \n",
      "                             \n",
      "                                \n",
      "                          \n",
      "                          \n",
      "\n",
      "   \n",
      "   \n",
      "                 \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1400 | 315.19] validation loss = 2.40\n",
      "[1400 | 315.25] loss=3.21 avg=2.33\n",
      "[1401 | 315.30] loss=2.86 avg=2.34\n",
      "[1402 | 315.36] loss=1.43 avg=2.33\n",
      "[1403 | 315.41] loss=1.47 avg=2.32\n",
      "[1404 | 315.46] loss=3.16 avg=2.33\n",
      "[1405 | 315.51] loss=3.55 avg=2.34\n",
      "[1406 | 315.56] loss=1.67 avg=2.34\n",
      "[1407 | 315.61] loss=2.93 avg=2.34\n",
      "[1408 | 315.67] loss=2.28 avg=2.34\n",
      "[1409 | 315.72] loss=1.86 avg=2.34\n",
      "[1410 | 315.78] loss=2.94 avg=2.34\n",
      "[1411 | 315.83] loss=1.50 avg=2.33\n",
      "[1412 | 315.88] loss=3.60 avg=2.35\n",
      "[1413 | 315.93] loss=1.04 avg=2.33\n",
      "[1414 | 315.99] loss=1.05 avg=2.32\n",
      "[1415 | 316.04] loss=2.28 avg=2.32\n",
      "[1416 | 316.09] loss=1.81 avg=2.32\n",
      "[1417 | 316.14] loss=2.70 avg=2.32\n",
      "[1418 | 316.20] loss=2.67 avg=2.32\n",
      "[1419 | 316.25] loss=2.10 avg=2.32\n",
      "[1420 | 316.31] loss=2.68 avg=2.32\n",
      "[1421 | 316.36] loss=3.00 avg=2.33\n",
      "[1422 | 316.41] loss=4.24 avg=2.35\n",
      "[1423 | 316.46] loss=2.81 avg=2.35\n",
      "[1424 | 316.52] loss=2.88 avg=2.36\n",
      "[1425 | 316.57] loss=4.01 avg=2.38\n",
      "[1426 | 316.63] loss=1.92 avg=2.37\n",
      "[1427 | 316.68] loss=1.75 avg=2.37\n",
      "[1428 | 316.73] loss=2.09 avg=2.36\n",
      "[1429 | 316.79] loss=2.83 avg=2.37\n",
      "[1430 | 316.84] loss=2.76 avg=2.37\n",
      "[1431 | 316.89] loss=2.73 avg=2.38\n",
      "[1432 | 316.94] loss=1.34 avg=2.36\n",
      "[1433 | 317.00] loss=1.48 avg=2.36\n",
      "[1434 | 317.06] loss=2.28 avg=2.36\n",
      "[1435 | 317.11] loss=2.50 avg=2.36\n",
      "[1436 | 317.17] loss=1.77 avg=2.35\n",
      "[1437 | 317.22] loss=3.00 avg=2.36\n",
      "[1438 | 317.28] loss=2.25 avg=2.36\n",
      "[1439 | 317.33] loss=1.56 avg=2.35\n",
      "[1440 | 317.38] loss=2.75 avg=2.35\n",
      "[1441 | 317.43] loss=2.43 avg=2.35\n",
      "[1442 | 317.48] loss=0.99 avg=2.34\n",
      "[1443 | 317.53] loss=1.63 avg=2.33\n",
      "[1444 | 317.58] loss=2.67 avg=2.34\n",
      "[1445 | 317.64] loss=2.54 avg=2.34\n",
      "[1446 | 317.69] loss=2.59 avg=2.34\n",
      "[1447 | 317.74] loss=2.51 avg=2.34\n",
      "[1448 | 317.80] loss=2.44 avg=2.34\n",
      "[1449 | 317.85] loss=2.60 avg=2.35\n",
      "[1450 | 317.90] loss=3.46 avg=2.36\n",
      "[1451 | 317.95] loss=1.79 avg=2.35\n",
      "[1452 | 318.01] loss=1.66 avg=2.34\n",
      "[1453 | 318.06] loss=1.50 avg=2.34\n",
      "[1454 | 318.11] loss=2.22 avg=2.33\n",
      "[1455 | 318.17] loss=3.02 avg=2.34\n",
      "[1456 | 318.22] loss=2.93 avg=2.35\n",
      "[1457 | 318.28] loss=1.33 avg=2.34\n",
      "[1458 | 318.33] loss=3.05 avg=2.34\n",
      "[1459 | 318.38] loss=1.71 avg=2.34\n",
      "[1460 | 318.43] loss=4.05 avg=2.35\n",
      "[1461 | 318.49] loss=2.27 avg=2.35\n",
      "[1462 | 318.54] loss=1.27 avg=2.34\n",
      "[1463 | 318.60] loss=1.98 avg=2.34\n",
      "[1464 | 318.65] loss=2.62 avg=2.34\n",
      "[1465 | 318.70] loss=3.14 avg=2.35\n",
      "[1466 | 318.75] loss=1.35 avg=2.34\n",
      "[1467 | 318.80] loss=3.22 avg=2.35\n",
      "[1468 | 318.85] loss=1.77 avg=2.34\n",
      "[1469 | 318.91] loss=1.96 avg=2.34\n",
      "[1470 | 318.96] loss=2.51 avg=2.34\n",
      "[1471 | 319.01] loss=2.71 avg=2.34\n",
      "[1472 | 319.06] loss=2.88 avg=2.35\n",
      "[1473 | 319.12] loss=1.74 avg=2.34\n",
      "[1474 | 319.17] loss=1.77 avg=2.34\n",
      "[1475 | 319.23] loss=2.67 avg=2.34\n",
      "[1476 | 319.28] loss=3.48 avg=2.35\n",
      "[1477 | 319.33] loss=1.85 avg=2.35\n",
      "[1478 | 319.39] loss=3.48 avg=2.36\n",
      "[1479 | 319.44] loss=2.28 avg=2.36\n",
      "[1480 | 319.49] loss=2.47 avg=2.36\n",
      "[1481 | 319.55] loss=2.76 avg=2.36\n",
      "[1482 | 319.60] loss=1.65 avg=2.36\n",
      "[1483 | 319.65] loss=2.37 avg=2.36\n",
      "[1484 | 319.70] loss=1.99 avg=2.35\n",
      "[1485 | 319.76] loss=1.76 avg=2.35\n",
      "[1486 | 319.81] loss=3.04 avg=2.35\n",
      "[1487 | 319.87] loss=3.25 avg=2.36\n",
      "[1488 | 319.92] loss=2.96 avg=2.37\n",
      "[1489 | 319.98] loss=1.72 avg=2.36\n",
      "[1490 | 320.03] loss=3.37 avg=2.37\n",
      "[1491 | 320.09] loss=2.91 avg=2.38\n",
      "[1492 | 320.14] loss=2.31 avg=2.38\n",
      "[1493 | 320.19] loss=2.91 avg=2.38\n",
      "[1494 | 320.24] loss=2.33 avg=2.38\n",
      "[1495 | 320.30] loss=3.49 avg=2.39\n",
      "[1496 | 320.35] loss=1.37 avg=2.38\n",
      "[1497 | 320.41] loss=0.81 avg=2.37\n",
      "[1498 | 320.46] loss=3.28 avg=2.38\n",
      "[1499 | 320.51] loss=2.65 avg=2.38\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "state[0].getItemName().name());\n",
      "    int value = value + 1;\n",
      "   if (item->getItemName() ==\n",
      "*value) {\n",
      "     if (item->getItemID() == 0) {\n",
      "       result = result * (getItemID() - 1;\n",
      "        result.setText().add(\"(0 - 0) \");\n",
      "        result.add(value);\n",
      "     }\n",
      "    if (item->getItemID() == 0) {\n",
      "        result = result * (getItemID() - 1;\n",
      "        result.setText().add(\"(0 - 0) \");\n",
      "    }\n",
      "    if (((item->getItemID() - 1) == 0) && (item->getItemID() - 1) == 0)) {\n",
      "       result = (item*getItemID() - 1);\n",
      "     }\n",
      "    if (item->getItemID() == 0) {\n",
      "       result = (getItemID() - 1);\n",
      "       result.setText().add(\"(0 - 0) \");\n",
      "    }\n",
      "    if (item->getItemID() == 0) {\n",
      "      result = (item*getItemID() - 1);\n",
      "      ?>\n",
      "\n",
      "             \n",
      "           result = (getItemID() - 1);\n",
      "\n",
      "   }\n",
      "    if (item->getItemID() == 0) {\n",
      "       result = (getItemID()) + 1;\n",
      "    }\n",
      "    if (item->getItemID() == 0) {\n",
      "       result = (getItemID()) - 1;\n",
      "   }\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "       \n",
      "    return -1;\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "     \n",
      "    \n",
      "      \n",
      "\n",
      "    \n",
      "     \n",
      "       \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      " \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500 | 336.31] validation loss = 2.38\n",
      "[1500 | 336.37] loss=2.28 avg=2.38\n",
      "[1501 | 336.42] loss=2.64 avg=2.38\n",
      "[1502 | 336.48] loss=1.09 avg=2.37\n",
      "[1503 | 336.53] loss=2.40 avg=2.37\n",
      "[1504 | 336.58] loss=2.97 avg=2.37\n",
      "[1505 | 336.64] loss=1.74 avg=2.37\n",
      "[1506 | 336.69] loss=3.04 avg=2.37\n",
      "[1507 | 336.74] loss=2.40 avg=2.37\n",
      "[1508 | 336.79] loss=2.98 avg=2.38\n",
      "[1509 | 336.85] loss=2.42 avg=2.38\n",
      "[1510 | 336.90] loss=2.91 avg=2.39\n",
      "[1511 | 336.95] loss=2.81 avg=2.39\n",
      "[1512 | 337.01] loss=1.49 avg=2.38\n",
      "[1513 | 337.06] loss=1.45 avg=2.37\n",
      "[1514 | 337.11] loss=2.24 avg=2.37\n",
      "[1515 | 337.16] loss=1.94 avg=2.37\n",
      "[1516 | 337.22] loss=2.19 avg=2.37\n",
      "[1517 | 337.27] loss=3.48 avg=2.38\n",
      "[1518 | 337.32] loss=1.65 avg=2.37\n",
      "[1519 | 337.38] loss=2.31 avg=2.37\n",
      "[1520 | 337.44] loss=1.75 avg=2.36\n",
      "[1521 | 337.49] loss=2.76 avg=2.37\n",
      "[1522 | 337.54] loss=2.43 avg=2.37\n",
      "[1523 | 337.59] loss=1.93 avg=2.36\n",
      "[1524 | 337.65] loss=2.47 avg=2.36\n",
      "[1525 | 337.70] loss=3.04 avg=2.37\n",
      "[1526 | 337.75] loss=3.55 avg=2.38\n",
      "[1527 | 337.81] loss=1.76 avg=2.38\n",
      "[1528 | 337.86] loss=2.55 avg=2.38\n",
      "[1529 | 337.91] loss=1.55 avg=2.37\n",
      "[1530 | 337.97] loss=2.28 avg=2.37\n",
      "[1531 | 338.02] loss=2.47 avg=2.37\n",
      "[1532 | 338.07] loss=2.11 avg=2.37\n",
      "[1533 | 338.12] loss=2.12 avg=2.36\n",
      "[1534 | 338.17] loss=2.34 avg=2.36\n",
      "[1535 | 338.22] loss=1.77 avg=2.36\n",
      "[1536 | 338.28] loss=2.26 avg=2.36\n",
      "[1537 | 338.33] loss=2.78 avg=2.36\n",
      "[1538 | 338.38] loss=1.65 avg=2.35\n",
      "[1539 | 338.43] loss=2.97 avg=2.36\n",
      "[1540 | 338.49] loss=2.46 avg=2.36\n",
      "[1541 | 338.54] loss=2.21 avg=2.36\n",
      "[1542 | 338.59] loss=3.26 avg=2.37\n",
      "[1543 | 338.64] loss=3.14 avg=2.38\n",
      "[1544 | 338.70] loss=2.70 avg=2.38\n",
      "[1545 | 338.75] loss=1.33 avg=2.37\n",
      "[1546 | 338.81] loss=0.75 avg=2.35\n",
      "[1547 | 338.86] loss=2.26 avg=2.35\n",
      "[1548 | 338.91] loss=3.08 avg=2.36\n",
      "[1549 | 338.97] loss=2.07 avg=2.36\n",
      "[1550 | 339.02] loss=4.06 avg=2.37\n",
      "[1551 | 339.08] loss=2.19 avg=2.37\n",
      "[1552 | 339.13] loss=3.07 avg=2.38\n",
      "[1553 | 339.18] loss=3.46 avg=2.39\n",
      "[1554 | 339.24] loss=2.26 avg=2.39\n",
      "[1555 | 339.29] loss=4.05 avg=2.41\n",
      "[1556 | 339.35] loss=0.67 avg=2.39\n",
      "[1557 | 339.40] loss=1.41 avg=2.38\n",
      "[1558 | 339.45] loss=1.14 avg=2.37\n",
      "[1559 | 339.51] loss=2.68 avg=2.37\n",
      "[1560 | 339.56] loss=0.81 avg=2.35\n",
      "[1561 | 339.62] loss=2.50 avg=2.35\n",
      "[1562 | 339.67] loss=1.62 avg=2.35\n",
      "[1563 | 339.72] loss=2.37 avg=2.35\n",
      "[1564 | 339.77] loss=1.19 avg=2.34\n",
      "[1565 | 339.83] loss=2.47 avg=2.34\n",
      "[1566 | 339.88] loss=2.92 avg=2.34\n",
      "[1567 | 339.93] loss=4.04 avg=2.36\n",
      "[1568 | 339.99] loss=0.83 avg=2.34\n",
      "[1569 | 340.04] loss=1.54 avg=2.34\n",
      "[1570 | 340.10] loss=2.02 avg=2.33\n",
      "[1571 | 340.15] loss=2.80 avg=2.34\n",
      "[1572 | 340.21] loss=1.89 avg=2.33\n",
      "[1573 | 340.26] loss=2.21 avg=2.33\n",
      "[1574 | 340.31] loss=3.73 avg=2.35\n",
      "[1575 | 340.36] loss=2.16 avg=2.34\n",
      "[1576 | 340.42] loss=1.97 avg=2.34\n",
      "[1577 | 340.47] loss=1.82 avg=2.34\n",
      "[1578 | 340.52] loss=0.56 avg=2.32\n",
      "[1579 | 340.57] loss=1.79 avg=2.31\n",
      "[1580 | 340.63] loss=3.40 avg=2.32\n",
      "[1581 | 340.69] loss=0.83 avg=2.31\n",
      "[1582 | 340.74] loss=2.58 avg=2.31\n",
      "[1583 | 340.79] loss=1.45 avg=2.30\n",
      "[1584 | 340.85] loss=2.99 avg=2.31\n",
      "[1585 | 340.90] loss=1.27 avg=2.30\n",
      "[1586 | 340.96] loss=2.66 avg=2.30\n",
      "[1587 | 341.02] loss=1.17 avg=2.29\n",
      "[1588 | 341.07] loss=2.61 avg=2.29\n",
      "[1589 | 341.13] loss=1.30 avg=2.28\n",
      "[1590 | 341.18] loss=3.06 avg=2.29\n",
      "[1591 | 341.23] loss=2.70 avg=2.30\n",
      "[1592 | 341.28] loss=1.99 avg=2.29\n",
      "[1593 | 341.33] loss=2.01 avg=2.29\n",
      "[1594 | 341.38] loss=4.19 avg=2.31\n",
      "[1595 | 341.43] loss=2.03 avg=2.31\n",
      "[1596 | 341.49] loss=3.01 avg=2.31\n",
      "[1597 | 341.54] loss=2.69 avg=2.32\n",
      "[1598 | 341.60] loss=1.94 avg=2.31\n",
      "[1599 | 341.65] loss=3.09 avg=2.32\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "              if (input.hasKey(key.value)) {\n",
      "                    if (text == \"\") {\n",
      "                                                                                                            \n",
      "if (input.hasKey(text)) {\n",
      "                                                                                                                                                                                                     end\n",
      "                                                                        \n",
      "                                                                                       \n",
      "                                                                                         \n",
      "                                                                                          \n",
      "                                                                    \n",
      "                                                        *\n",
      "                                                                   \n",
      "                                                                   \n",
      "                                                 \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1600 | 357.60] validation loss = 2.37\n",
      "[1600 | 357.66] loss=2.13 avg=2.32\n",
      "[1601 | 357.71] loss=3.45 avg=2.33\n",
      "[1602 | 357.77] loss=2.34 avg=2.33\n",
      "[1603 | 357.82] loss=1.37 avg=2.32\n",
      "[1604 | 357.88] loss=2.12 avg=2.32\n",
      "[1605 | 357.93] loss=2.62 avg=2.32\n",
      "[1606 | 357.98] loss=3.53 avg=2.33\n",
      "[1607 | 358.03] loss=2.50 avg=2.34\n",
      "[1608 | 358.09] loss=2.24 avg=2.34\n",
      "[1609 | 358.14] loss=2.24 avg=2.33\n",
      "[1610 | 358.19] loss=0.91 avg=2.32\n",
      "[1611 | 358.25] loss=2.24 avg=2.32\n",
      "[1612 | 358.30] loss=2.62 avg=2.32\n",
      "[1613 | 358.35] loss=1.72 avg=2.32\n",
      "[1614 | 358.41] loss=2.07 avg=2.31\n",
      "[1615 | 358.46] loss=3.26 avg=2.32\n",
      "[1616 | 358.52] loss=1.62 avg=2.32\n",
      "[1617 | 358.57] loss=2.37 avg=2.32\n",
      "[1618 | 358.62] loss=3.18 avg=2.33\n",
      "[1619 | 358.67] loss=3.02 avg=2.33\n",
      "[1620 | 358.73] loss=1.74 avg=2.33\n",
      "[1621 | 358.78] loss=1.09 avg=2.31\n",
      "[1622 | 358.83] loss=3.18 avg=2.32\n",
      "[1623 | 358.88] loss=2.23 avg=2.32\n",
      "[1624 | 358.94] loss=3.50 avg=2.33\n",
      "[1625 | 358.99] loss=4.26 avg=2.35\n",
      "[1626 | 359.04] loss=2.56 avg=2.35\n",
      "[1627 | 359.09] loss=1.64 avg=2.35\n",
      "[1628 | 359.15] loss=1.61 avg=2.34\n",
      "[1629 | 359.20] loss=1.37 avg=2.33\n",
      "[1630 | 359.26] loss=1.96 avg=2.33\n",
      "[1631 | 359.31] loss=1.57 avg=2.32\n",
      "[1632 | 359.36] loss=1.91 avg=2.32\n",
      "[1633 | 359.42] loss=2.47 avg=2.32\n",
      "[1634 | 359.47] loss=4.50 avg=2.34\n",
      "[1635 | 359.52] loss=3.93 avg=2.35\n",
      "[1636 | 359.57] loss=1.66 avg=2.35\n",
      "[1637 | 359.63] loss=1.46 avg=2.34\n",
      "[1638 | 359.68] loss=2.69 avg=2.34\n",
      "[1639 | 359.73] loss=1.51 avg=2.33\n",
      "[1640 | 359.79] loss=1.32 avg=2.32\n",
      "[1641 | 359.84] loss=2.74 avg=2.33\n",
      "[1642 | 359.89] loss=2.12 avg=2.33\n",
      "[1643 | 359.95] loss=2.02 avg=2.32\n",
      "[1644 | 360.00] loss=2.70 avg=2.33\n",
      "[1645 | 360.05] loss=2.36 avg=2.33\n",
      "[1646 | 360.11] loss=3.22 avg=2.34\n",
      "[1647 | 360.16] loss=2.37 avg=2.34\n",
      "[1648 | 360.21] loss=3.25 avg=2.35\n",
      "[1649 | 360.27] loss=2.25 avg=2.34\n",
      "[1650 | 360.32] loss=2.75 avg=2.35\n",
      "[1651 | 360.37] loss=2.69 avg=2.35\n",
      "[1652 | 360.43] loss=1.85 avg=2.35\n",
      "[1653 | 360.48] loss=1.59 avg=2.34\n",
      "[1654 | 360.53] loss=2.35 avg=2.34\n",
      "[1655 | 360.59] loss=2.55 avg=2.34\n",
      "[1656 | 360.64] loss=2.76 avg=2.35\n",
      "[1657 | 360.69] loss=2.35 avg=2.35\n",
      "[1658 | 360.74] loss=2.05 avg=2.34\n",
      "[1659 | 360.80] loss=3.13 avg=2.35\n",
      "[1660 | 360.85] loss=2.32 avg=2.35\n",
      "[1661 | 360.91] loss=1.99 avg=2.35\n",
      "[1662 | 360.96] loss=1.44 avg=2.34\n",
      "[1663 | 361.01] loss=2.19 avg=2.34\n",
      "[1664 | 361.06] loss=2.36 avg=2.34\n",
      "[1665 | 361.11] loss=1.85 avg=2.33\n",
      "[1666 | 361.16] loss=1.64 avg=2.32\n",
      "[1667 | 361.21] loss=1.94 avg=2.32\n",
      "[1668 | 361.27] loss=2.83 avg=2.33\n",
      "[1669 | 361.32] loss=2.57 avg=2.33\n",
      "[1670 | 361.37] loss=3.30 avg=2.34\n",
      "[1671 | 361.43] loss=2.92 avg=2.34\n",
      "[1672 | 361.48] loss=1.73 avg=2.34\n",
      "[1673 | 361.53] loss=1.68 avg=2.33\n",
      "[1674 | 361.59] loss=1.72 avg=2.32\n",
      "[1675 | 361.64] loss=1.75 avg=2.32\n",
      "[1676 | 361.70] loss=2.61 avg=2.32\n",
      "[1677 | 361.75] loss=2.56 avg=2.32\n",
      "[1678 | 361.80] loss=1.53 avg=2.32\n",
      "[1679 | 361.85] loss=0.69 avg=2.30\n",
      "[1680 | 361.91] loss=3.09 avg=2.31\n",
      "[1681 | 361.96] loss=2.18 avg=2.31\n",
      "[1682 | 362.01] loss=3.28 avg=2.32\n",
      "[1683 | 362.06] loss=2.23 avg=2.32\n",
      "[1684 | 362.12] loss=3.12 avg=2.32\n",
      "[1685 | 362.17] loss=3.03 avg=2.33\n",
      "[1686 | 362.23] loss=1.36 avg=2.32\n",
      "[1687 | 362.28] loss=2.98 avg=2.33\n",
      "[1688 | 362.34] loss=1.20 avg=2.32\n",
      "[1689 | 362.39] loss=1.52 avg=2.31\n",
      "[1690 | 362.45] loss=1.72 avg=2.30\n",
      "[1691 | 362.50] loss=3.51 avg=2.31\n",
      "[1692 | 362.55] loss=1.85 avg=2.31\n",
      "[1693 | 362.60] loss=2.38 avg=2.31\n",
      "[1694 | 362.66] loss=3.22 avg=2.32\n",
      "[1695 | 362.71] loss=0.72 avg=2.30\n",
      "[1696 | 362.77] loss=3.08 avg=2.31\n",
      "[1697 | 362.82] loss=2.30 avg=2.31\n",
      "[1698 | 362.87] loss=3.57 avg=2.32\n",
      "[1699 | 362.93] loss=2.22 avg=2.32\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "\\null);\n",
      "\\t\\t}\n",
      "     }\n",
      "     @property(javax.swing.widget.FrameLayout *)\n",
      "    @property(javax.swing.widget.Text *)\n",
      "\n",
      "   @param (javax.swing.widget.Text)\n",
      "public void textInput(javax.swing.widget.FrameLayout* frame, javax.swing.widget.FrameLayout* frameOptions) {\n",
      "      @property(javax.swing.widget.FrameLayout)\n",
      "      @return\n",
      "      {\n",
      "                         @param (frame)\n",
      "                            if (frameOptions) {\n",
      "                                      if (frameRequest.size() < (2)) {\n",
      "                                   \n",
      "                                   \n",
      "                                if (!frames.isEmpty())\n",
      "                               \n",
      "                          \n",
      "                      }\n",
      "                       \n",
      "                    \n",
      "                 \n",
      "            \n",
      "      \n",
      "     \n",
      "   \n",
      "   \n",
      "   \n",
      "   }\n",
      "  \n",
      "public void getInput() {\n",
      "      javax.swing.widget.Frame textInputs = new javax.swing.widget.Frame();\n",
      "      if (!frame.equals(textInputs).equals(frame, (java.io.FileInputStream\n",
      "       )) {\n",
      "                         } else {\n",
      "                        }\n",
      "          }\n",
      "    i32 textInputs.setDefault(); }\n",
      "   java.util.List<javax.swing.widget.FrameLayout> (javax.swing.widget.FrameLayout, java.lang.String textInputs, java.util.List<javax.swing.widget.FrameLayout> inputOptions, java.io.Byte[] outputFields) {\n",
      "         \n",
      "                   \n",
      "                       \n",
      "                          \n",
      "                                   \n",
      "                              \n",
      "                         \n",
      "                    \n",
      " }\n",
      "   * @package javax.swing.widget.FrameLayout\n",
      "  android.view._loadButton(textInputs);\n",
      "   android.widget.Layout btnAppend(new android.\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1700 | 378.92] validation loss = 2.36\n",
      "[1700 | 378.97] loss=3.28 avg=2.33\n",
      "[1701 | 379.03] loss=1.75 avg=2.33\n",
      "[1702 | 379.08] loss=2.36 avg=2.33\n",
      "[1703 | 379.14] loss=3.16 avg=2.34\n",
      "[1704 | 379.19] loss=2.53 avg=2.34\n",
      "[1705 | 379.24] loss=1.63 avg=2.33\n",
      "[1706 | 379.29] loss=2.21 avg=2.33\n",
      "[1707 | 379.35] loss=3.13 avg=2.34\n",
      "[1708 | 379.40] loss=2.73 avg=2.34\n",
      "[1709 | 379.46] loss=0.67 avg=2.32\n",
      "[1710 | 379.51] loss=1.34 avg=2.31\n",
      "[1711 | 379.56] loss=2.02 avg=2.31\n",
      "[1712 | 379.61] loss=1.77 avg=2.31\n",
      "[1713 | 379.66] loss=3.14 avg=2.31\n",
      "[1714 | 379.71] loss=1.40 avg=2.31\n",
      "[1715 | 379.76] loss=1.30 avg=2.30\n",
      "[1716 | 379.82] loss=2.47 avg=2.30\n",
      "[1717 | 379.87] loss=2.85 avg=2.30\n",
      "[1718 | 379.93] loss=1.42 avg=2.29\n",
      "[1719 | 379.98] loss=1.05 avg=2.28\n",
      "[1720 | 380.04] loss=3.52 avg=2.29\n",
      "[1721 | 380.09] loss=2.51 avg=2.30\n",
      "[1722 | 380.14] loss=2.01 avg=2.29\n",
      "[1723 | 380.19] loss=3.23 avg=2.30\n",
      "[1724 | 380.25] loss=1.67 avg=2.30\n",
      "[1725 | 380.30] loss=2.19 avg=2.29\n",
      "[1726 | 380.35] loss=1.98 avg=2.29\n",
      "[1727 | 380.41] loss=2.28 avg=2.29\n",
      "[1728 | 380.46] loss=2.98 avg=2.30\n",
      "[1729 | 380.51] loss=2.63 avg=2.30\n",
      "[1730 | 380.56] loss=2.73 avg=2.31\n",
      "[1731 | 380.61] loss=3.17 avg=2.31\n",
      "[1732 | 380.67] loss=3.18 avg=2.32\n",
      "[1733 | 380.72] loss=2.16 avg=2.32\n",
      "[1734 | 380.77] loss=1.76 avg=2.32\n",
      "[1735 | 380.82] loss=2.90 avg=2.32\n",
      "[1736 | 380.88] loss=2.50 avg=2.32\n",
      "[1737 | 380.93] loss=1.80 avg=2.32\n",
      "[1738 | 380.99] loss=3.46 avg=2.33\n",
      "[1739 | 381.04] loss=2.57 avg=2.33\n",
      "[1740 | 381.09] loss=2.27 avg=2.33\n",
      "[1741 | 381.15] loss=1.82 avg=2.33\n",
      "[1742 | 381.20] loss=2.40 avg=2.33\n",
      "[1743 | 381.25] loss=1.75 avg=2.32\n",
      "[1744 | 381.30] loss=2.07 avg=2.32\n",
      "[1745 | 381.36] loss=2.52 avg=2.32\n",
      "[1746 | 381.41] loss=2.65 avg=2.32\n",
      "[1747 | 381.46] loss=2.01 avg=2.32\n",
      "[1748 | 381.52] loss=2.21 avg=2.32\n",
      "[1749 | 381.57] loss=1.51 avg=2.31\n",
      "[1750 | 381.62] loss=2.13 avg=2.31\n",
      "[1751 | 381.67] loss=2.15 avg=2.31\n",
      "[1752 | 381.72] loss=2.07 avg=2.31\n",
      "[1753 | 381.77] loss=3.25 avg=2.32\n",
      "[1754 | 381.83] loss=2.35 avg=2.32\n",
      "[1755 | 381.88] loss=3.20 avg=2.32\n",
      "[1756 | 381.93] loss=3.00 avg=2.33\n",
      "[1757 | 381.98] loss=2.49 avg=2.33\n",
      "[1758 | 382.04] loss=3.45 avg=2.34\n",
      "[1759 | 382.09] loss=1.53 avg=2.34\n",
      "[1760 | 382.14] loss=2.35 avg=2.34\n",
      "[1761 | 382.19] loss=3.46 avg=2.35\n",
      "[1762 | 382.24] loss=2.76 avg=2.35\n",
      "[1763 | 382.30] loss=2.89 avg=2.36\n",
      "[1764 | 382.35] loss=0.56 avg=2.34\n",
      "[1765 | 382.40] loss=1.90 avg=2.33\n",
      "[1766 | 382.46] loss=2.03 avg=2.33\n",
      "[1767 | 382.51] loss=1.63 avg=2.32\n",
      "[1768 | 382.56] loss=3.06 avg=2.33\n",
      "[1769 | 382.61] loss=2.22 avg=2.33\n",
      "[1770 | 382.67] loss=1.38 avg=2.32\n",
      "[1771 | 382.72] loss=1.55 avg=2.31\n",
      "[1772 | 382.76] loss=2.22 avg=2.31\n",
      "[1773 | 382.81] loss=2.37 avg=2.31\n",
      "[1774 | 382.86] loss=1.72 avg=2.31\n",
      "[1775 | 382.91] loss=1.98 avg=2.30\n",
      "[1776 | 382.96] loss=1.53 avg=2.30\n",
      "[1777 | 383.02] loss=3.31 avg=2.31\n",
      "[1778 | 383.07] loss=2.56 avg=2.31\n",
      "[1779 | 383.12] loss=2.04 avg=2.31\n",
      "[1780 | 383.17] loss=1.88 avg=2.30\n",
      "[1781 | 383.22] loss=2.43 avg=2.30\n",
      "[1782 | 383.27] loss=2.83 avg=2.31\n",
      "[1783 | 383.33] loss=2.19 avg=2.31\n",
      "[1784 | 383.38] loss=1.62 avg=2.30\n",
      "[1785 | 383.43] loss=2.46 avg=2.30\n",
      "[1786 | 383.49] loss=1.62 avg=2.30\n",
      "[1787 | 383.54] loss=1.41 avg=2.29\n",
      "[1788 | 383.59] loss=1.51 avg=2.28\n",
      "[1789 | 383.65] loss=1.50 avg=2.27\n",
      "[1790 | 383.70] loss=1.76 avg=2.27\n",
      "[1791 | 383.75] loss=2.88 avg=2.27\n",
      "[1792 | 383.81] loss=2.18 avg=2.27\n",
      "[1793 | 383.86] loss=3.71 avg=2.29\n",
      "[1794 | 383.91] loss=3.34 avg=2.30\n",
      "[1795 | 383.96] loss=2.30 avg=2.30\n",
      "[1796 | 384.02] loss=2.07 avg=2.29\n",
      "[1797 | 384.07] loss=3.02 avg=2.30\n",
      "[1798 | 384.13] loss=2.40 avg=2.30\n",
      "[1799 | 384.18] loss=2.64 avg=2.31\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      ")););\n",
      "              \n",
      "                 \n",
      "                 {\n",
      "                    \n",
      "                 \n",
      "                \n",
      "            \n",
      "          \n",
      "         \n",
      "     name = java.lang.Integer.parse(s, name);\n",
      "    if (name == 2)\n",
      "          if ((isFile()) != null)\n",
      "         else\n",
      "            \n",
      "          \n",
      "       \n",
      "       java.lang.String word = word.getString(s, true);\n",
      "   \n",
      "       if (g != null) {\n",
      "          \n",
      "           \n",
      "             word = g.getString(s, true);\n",
      "   \n",
      "     \n",
      "      java.lang.String word2 = new java.lang.String(s);\n",
      "  \n",
      "     \n",
      "      if (word.length() != 2) {\n",
      "       \n",
      "     \n",
      "       \n",
      "     \n",
      "       \n",
      "        word = word2.getBoolean(s, false);\n",
      "    \n",
      "    \n",
      "     if (g != null)\n",
      "      \n",
      "   \n",
      "     \n",
      "    \n",
      "     \n",
      "     \n",
      "    if (g.hasInt32(a)) {\n",
      "    case 'a' :\n",
      "       \n",
      "      \n",
      "     \n",
      "     case 1 :\n",
      "       \n",
      "     \n",
      "     case 3 :\n",
      "      \n",
      "    \n",
      "    \n",
      "      \n",
      "    \n",
      "     \n",
      "   \n",
      "   \n",
      "    \n",
      "    return word2;\n",
      "  \n",
      "  \n",
      "  \n",
      "   g.putString((word2);\n",
      "  \n",
      "   \n",
      "   \n",
      "   name = word.getString(s, true);\n",
      "   \n",
      "  \n",
      "   while (word != null) {\n",
      "      \n",
      "     \n",
      "     \n",
      "      if (word.length() != 2) {\n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "    \n",
      "       if (word.length() != 2)\n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "       nextWord = next;\n",
      "   \n",
      "      \n",
      "     \n",
      "       if (g == null) {\n",
      "      \n",
      "      \n",
      "      \n",
      "      \n",
      "      }\n",
      "   \n",
      "   } void\n",
      "    java.lang.Long term2, term3, word4, word5) {\n",
      "     \n",
      "     word1 = word2.getString(s, true);\n",
      "   \n",
      "    \n",
      "    \n",
      "     word2 = word3.getString(s, true);\n",
      "   \n",
      "    word1, word2, word3, word4, word5) {\n",
      "     \n",
      "    \n",
      "    \n",
      "      word1, word2, word3, word4, word5) {\n",
      "      \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1800 | 400.18] validation loss = 2.35\n",
      "[1800 | 400.24] loss=1.71 avg=2.30\n",
      "[1801 | 400.30] loss=1.77 avg=2.29\n",
      "[1802 | 400.35] loss=2.22 avg=2.29\n",
      "[1803 | 400.40] loss=2.27 avg=2.29\n",
      "[1804 | 400.45] loss=2.94 avg=2.30\n",
      "[1805 | 400.51] loss=2.01 avg=2.30\n",
      "[1806 | 400.56] loss=1.87 avg=2.29\n",
      "[1807 | 400.61] loss=1.91 avg=2.29\n",
      "[1808 | 400.66] loss=0.40 avg=2.27\n",
      "[1809 | 400.72] loss=1.73 avg=2.26\n",
      "[1810 | 400.77] loss=1.91 avg=2.26\n",
      "[1811 | 400.83] loss=2.06 avg=2.26\n",
      "[1812 | 400.88] loss=1.60 avg=2.25\n",
      "[1813 | 400.93] loss=2.37 avg=2.25\n",
      "[1814 | 400.98] loss=2.54 avg=2.26\n",
      "[1815 | 401.04] loss=1.88 avg=2.25\n",
      "[1816 | 401.09] loss=2.11 avg=2.25\n",
      "[1817 | 401.15] loss=2.22 avg=2.25\n",
      "[1818 | 401.20] loss=0.73 avg=2.24\n",
      "[1819 | 401.25] loss=1.41 avg=2.23\n",
      "[1820 | 401.31] loss=2.72 avg=2.23\n",
      "[1821 | 401.36] loss=1.48 avg=2.22\n",
      "[1822 | 401.42] loss=2.48 avg=2.23\n",
      "[1823 | 401.47] loss=3.70 avg=2.24\n",
      "[1824 | 401.52] loss=1.34 avg=2.23\n",
      "[1825 | 401.58] loss=2.58 avg=2.24\n",
      "[1826 | 401.63] loss=1.28 avg=2.23\n",
      "[1827 | 401.68] loss=2.94 avg=2.23\n",
      "[1828 | 401.74] loss=1.85 avg=2.23\n",
      "[1829 | 401.79] loss=2.57 avg=2.23\n",
      "[1830 | 401.84] loss=1.25 avg=2.22\n",
      "[1831 | 401.90] loss=1.19 avg=2.21\n",
      "[1832 | 401.95] loss=2.94 avg=2.22\n",
      "[1833 | 402.00] loss=1.96 avg=2.22\n",
      "[1834 | 402.06] loss=2.72 avg=2.22\n",
      "[1835 | 402.11] loss=2.32 avg=2.22\n",
      "[1836 | 402.16] loss=2.08 avg=2.22\n",
      "[1837 | 402.21] loss=2.68 avg=2.23\n",
      "[1838 | 402.26] loss=1.61 avg=2.22\n",
      "[1839 | 402.31] loss=1.98 avg=2.22\n",
      "[1840 | 402.37] loss=1.09 avg=2.21\n",
      "[1841 | 402.42] loss=1.64 avg=2.20\n",
      "[1842 | 402.48] loss=1.76 avg=2.20\n",
      "[1843 | 402.53] loss=1.48 avg=2.19\n",
      "[1844 | 402.59] loss=3.24 avg=2.20\n",
      "[1845 | 402.64] loss=1.19 avg=2.19\n",
      "[1846 | 402.70] loss=2.04 avg=2.19\n",
      "[1847 | 402.75] loss=3.13 avg=2.20\n",
      "[1848 | 402.81] loss=1.74 avg=2.19\n",
      "[1849 | 402.86] loss=1.09 avg=2.18\n",
      "[1850 | 402.92] loss=3.22 avg=2.19\n",
      "[1851 | 402.97] loss=2.21 avg=2.19\n",
      "[1852 | 403.03] loss=3.16 avg=2.20\n",
      "[1853 | 403.08] loss=1.91 avg=2.20\n",
      "[1854 | 403.14] loss=3.26 avg=2.21\n",
      "[1855 | 403.19] loss=1.65 avg=2.20\n",
      "[1856 | 403.25] loss=3.63 avg=2.22\n",
      "[1857 | 403.30] loss=2.47 avg=2.22\n",
      "[1858 | 403.35] loss=2.87 avg=2.23\n",
      "[1859 | 403.41] loss=1.60 avg=2.22\n",
      "[1860 | 403.46] loss=1.76 avg=2.22\n",
      "[1861 | 403.51] loss=2.04 avg=2.22\n",
      "[1862 | 403.57] loss=2.98 avg=2.22\n",
      "[1863 | 403.62] loss=3.29 avg=2.23\n",
      "[1864 | 403.67] loss=1.60 avg=2.23\n",
      "[1865 | 403.72] loss=2.65 avg=2.23\n",
      "[1866 | 403.78] loss=3.67 avg=2.25\n",
      "[1867 | 403.83] loss=2.01 avg=2.24\n",
      "[1868 | 403.88] loss=2.06 avg=2.24\n",
      "[1869 | 403.93] loss=2.56 avg=2.24\n",
      "[1870 | 403.99] loss=3.07 avg=2.25\n",
      "[1871 | 404.04] loss=3.15 avg=2.26\n",
      "[1872 | 404.09] loss=2.69 avg=2.27\n",
      "[1873 | 404.15] loss=2.91 avg=2.27\n",
      "[1874 | 404.20] loss=2.71 avg=2.28\n",
      "[1875 | 404.26] loss=2.62 avg=2.28\n",
      "[1876 | 404.31] loss=1.75 avg=2.28\n",
      "[1877 | 404.37] loss=1.74 avg=2.27\n",
      "[1878 | 404.42] loss=2.26 avg=2.27\n",
      "[1879 | 404.47] loss=1.81 avg=2.27\n",
      "[1880 | 404.53] loss=2.73 avg=2.27\n",
      "[1881 | 404.58] loss=1.69 avg=2.26\n",
      "[1882 | 404.64] loss=1.51 avg=2.26\n",
      "[1883 | 404.69] loss=3.31 avg=2.27\n",
      "[1884 | 404.74] loss=2.95 avg=2.27\n",
      "[1885 | 404.80] loss=1.22 avg=2.26\n",
      "[1886 | 404.85] loss=3.45 avg=2.28\n",
      "[1887 | 404.91] loss=2.72 avg=2.28\n",
      "[1888 | 404.96] loss=1.88 avg=2.28\n",
      "[1889 | 405.01] loss=2.82 avg=2.28\n",
      "[1890 | 405.07] loss=2.93 avg=2.29\n",
      "[1891 | 405.12] loss=2.23 avg=2.29\n",
      "[1892 | 405.18] loss=2.95 avg=2.29\n",
      "[1893 | 405.23] loss=1.81 avg=2.29\n",
      "[1894 | 405.28] loss=1.28 avg=2.28\n",
      "[1895 | 405.33] loss=1.84 avg=2.27\n",
      "[1896 | 405.39] loss=1.87 avg=2.27\n",
      "[1897 | 405.44] loss=2.85 avg=2.28\n",
      "[1898 | 405.50] loss=1.19 avg=2.27\n",
      "[1899 | 405.55] loss=3.31 avg=2.28\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 43.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " (GASP_STORE_NAME_PREFIX, -1);\n",
      "                       \n",
      "                       \n",
      "                      \n",
      "                     \n",
      "             }else\n",
      "                   \n",
      "                   \"\t \" for\t, \"\t \" for\t, and \"\t \" for\t is\t, giliop\t iseatures, giliop\t iseatures areeatures.\t iseatures iseatures giliopeatures iseatures giliopeatures giliopeatures areeatures areeatures giliopeatures areeatures giliopeatures giliopeatures giliopeatures giliopeatures iseatures giliopeatures areeatures \n",
      "                 \n",
      "              \n",
      "               \n",
      "             \n",
      "            \n",
      "        \n",
      "\n",
      " \n",
      "       \n",
      "\n",
      "   \n",
      "      \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "      \n",
      "\n",
      "      \n",
      "\n",
      "       \n",
      "\n",
      "                            \n",
      "\n",
      "          \n",
      "\n",
      "          \n",
      "\n",
      "     \n",
      "                           \n",
      "                                                        \n",
      "                                                                \n",
      "                                                  \n",
      "                                                               \n",
      "                                                               \n",
      "                                                         \n",
      "                                                             \n",
      "                                                                            \n",
      "        \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1900 | 421.44] validation loss = 2.34\n",
      "[1900 | 421.50] loss=3.86 avg=2.29\n",
      "[1901 | 421.55] loss=2.02 avg=2.29\n",
      "[1902 | 421.60] loss=1.76 avg=2.28\n",
      "[1903 | 421.66] loss=3.01 avg=2.29\n",
      "[1904 | 421.71] loss=2.32 avg=2.29\n",
      "[1905 | 421.76] loss=2.31 avg=2.29\n",
      "[1906 | 421.82] loss=2.98 avg=2.30\n",
      "[1907 | 421.87] loss=2.80 avg=2.30\n",
      "[1908 | 421.92] loss=0.83 avg=2.29\n",
      "[1909 | 421.98] loss=2.47 avg=2.29\n",
      "[1910 | 422.03] loss=0.66 avg=2.27\n",
      "[1911 | 422.08] loss=2.03 avg=2.27\n",
      "[1912 | 422.14] loss=1.48 avg=2.26\n",
      "[1913 | 422.19] loss=3.64 avg=2.28\n",
      "[1914 | 422.24] loss=2.67 avg=2.28\n",
      "[1915 | 422.29] loss=0.80 avg=2.27\n",
      "[1916 | 422.34] loss=2.10 avg=2.26\n",
      "[1917 | 422.41] loss=3.04 avg=2.27\n",
      "[1918 | 422.46] loss=2.15 avg=2.27\n",
      "[1919 | 422.51] loss=2.40 avg=2.27\n",
      "[1920 | 422.57] loss=2.73 avg=2.28\n",
      "[1921 | 422.62] loss=2.59 avg=2.28\n",
      "[1922 | 422.67] loss=3.51 avg=2.29\n",
      "[1923 | 422.72] loss=2.83 avg=2.30\n",
      "[1924 | 422.78] loss=2.91 avg=2.30\n",
      "[1925 | 422.83] loss=1.45 avg=2.30\n",
      "[1926 | 422.88] loss=1.24 avg=2.28\n",
      "[1927 | 422.94] loss=2.24 avg=2.28\n",
      "[1928 | 422.99] loss=3.16 avg=2.29\n",
      "[1929 | 423.05] loss=1.54 avg=2.29\n",
      "[1930 | 423.10] loss=2.03 avg=2.28\n",
      "[1931 | 423.16] loss=1.99 avg=2.28\n",
      "[1932 | 423.21] loss=2.73 avg=2.28\n",
      "[1933 | 423.26] loss=1.39 avg=2.28\n",
      "[1934 | 423.32] loss=2.04 avg=2.27\n",
      "[1935 | 423.37] loss=2.73 avg=2.28\n",
      "[1936 | 423.43] loss=1.32 avg=2.27\n",
      "[1937 | 423.48] loss=3.41 avg=2.28\n",
      "[1938 | 423.53] loss=1.51 avg=2.27\n",
      "[1939 | 423.58] loss=2.59 avg=2.28\n",
      "[1940 | 423.63] loss=2.01 avg=2.27\n",
      "[1941 | 423.69] loss=2.32 avg=2.27\n",
      "[1942 | 423.74] loss=2.98 avg=2.28\n",
      "[1943 | 423.79] loss=2.80 avg=2.29\n",
      "[1944 | 423.85] loss=2.09 avg=2.28\n",
      "[1945 | 423.90] loss=1.81 avg=2.28\n",
      "[1946 | 423.95] loss=2.40 avg=2.28\n",
      "[1947 | 424.00] loss=2.10 avg=2.28\n",
      "[1948 | 424.06] loss=0.96 avg=2.26\n",
      "[1949 | 424.11] loss=2.04 avg=2.26\n",
      "[1950 | 424.16] loss=2.41 avg=2.26\n",
      "[1951 | 424.21] loss=3.66 avg=2.28\n",
      "[1952 | 424.27] loss=2.39 avg=2.28\n",
      "[1953 | 424.32] loss=2.13 avg=2.28\n",
      "[1954 | 424.37] loss=1.78 avg=2.27\n",
      "[1955 | 424.42] loss=1.17 avg=2.26\n",
      "[1956 | 424.47] loss=1.62 avg=2.25\n",
      "[1957 | 424.53] loss=1.95 avg=2.25\n",
      "[1958 | 424.57] loss=2.40 avg=2.25\n",
      "[1959 | 424.63] loss=3.56 avg=2.27\n",
      "[1960 | 424.68] loss=2.72 avg=2.27\n",
      "[1961 | 424.73] loss=2.64 avg=2.27\n",
      "[1962 | 424.79] loss=2.51 avg=2.28\n",
      "[1963 | 424.84] loss=2.05 avg=2.27\n",
      "[1964 | 424.89] loss=1.26 avg=2.26\n",
      "[1965 | 424.95] loss=3.09 avg=2.27\n",
      "[1966 | 425.00] loss=2.77 avg=2.28\n",
      "[1967 | 425.05] loss=1.85 avg=2.27\n",
      "[1968 | 425.10] loss=1.83 avg=2.27\n",
      "[1969 | 425.15] loss=2.83 avg=2.27\n",
      "[1970 | 425.21] loss=2.47 avg=2.28\n",
      "[1971 | 425.26] loss=3.41 avg=2.29\n",
      "[1972 | 425.31] loss=1.41 avg=2.28\n",
      "[1973 | 425.36] loss=2.79 avg=2.28\n",
      "[1974 | 425.41] loss=2.42 avg=2.29\n",
      "[1975 | 425.47] loss=2.08 avg=2.28\n",
      "[1976 | 425.52] loss=2.43 avg=2.29\n",
      "[1977 | 425.57] loss=2.48 avg=2.29\n",
      "[1978 | 425.62] loss=1.77 avg=2.28\n",
      "[1979 | 425.67] loss=1.45 avg=2.27\n",
      "[1980 | 425.73] loss=2.11 avg=2.27\n",
      "[1981 | 425.78] loss=3.17 avg=2.28\n",
      "[1982 | 425.84] loss=1.20 avg=2.27\n",
      "[1983 | 425.89] loss=2.34 avg=2.27\n",
      "[1984 | 425.94] loss=3.27 avg=2.28\n",
      "[1985 | 426.00] loss=1.58 avg=2.27\n",
      "[1986 | 426.05] loss=3.05 avg=2.28\n",
      "[1987 | 426.11] loss=2.82 avg=2.29\n",
      "[1988 | 426.17] loss=1.36 avg=2.28\n",
      "[1989 | 426.22] loss=2.79 avg=2.28\n",
      "[1990 | 426.27] loss=2.99 avg=2.29\n",
      "[1991 | 426.32] loss=2.44 avg=2.29\n",
      "[1992 | 426.38] loss=1.52 avg=2.28\n",
      "[1993 | 426.43] loss=2.81 avg=2.29\n",
      "[1994 | 426.49] loss=2.25 avg=2.29\n",
      "[1995 | 426.54] loss=2.54 avg=2.29\n",
      "[1996 | 426.59] loss=2.18 avg=2.29\n",
      "[1997 | 426.64] loss=1.86 avg=2.29\n",
      "[1998 | 426.69] loss=1.45 avg=2.28\n",
      "[1999 | 426.75] loss=1.96 avg=2.27\n",
      "Saving checkpoint/m1_vulnerability/model-2000\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      ")\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t��\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=(\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t的\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tOWN\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUTION\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tFORMATION\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tUTION\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tREDACTED\t\t\t\t\t\t],\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=(\t\t\t\t\t\t\t\t\t\t\t\t\t)\t\t\t\t\t=(\t\t的)\t\t\t\t\t\t\t\t\t\t\t])\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t[[0].(((((((((((((((((((((((((((((((((((((((()(((((((((((((((((((((((((((((((((((((((((((((((((((( (((((((()))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))))]))))))))))))))))))))))))))))))))\n",
      "\n",
      "                                                                                          {\n",
      "                                                                                                               ))))); }\n",
      "\n",
      "                                                             \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000 | 448.26] validation loss = 2.33\n",
      "[2000 | 448.32] loss=3.29 avg=2.28\n",
      "[2001 | 448.37] loss=1.64 avg=2.28\n",
      "[2002 | 448.43] loss=0.91 avg=2.26\n",
      "[2003 | 448.48] loss=1.81 avg=2.26\n",
      "[2004 | 448.54] loss=3.25 avg=2.27\n",
      "[2005 | 448.59] loss=2.74 avg=2.27\n",
      "[2006 | 448.65] loss=2.04 avg=2.27\n",
      "[2007 | 448.70] loss=2.28 avg=2.27\n",
      "[2008 | 448.75] loss=2.36 avg=2.27\n",
      "[2009 | 448.81] loss=2.41 avg=2.27\n",
      "[2010 | 448.86] loss=2.70 avg=2.28\n",
      "[2011 | 448.92] loss=2.87 avg=2.28\n",
      "[2012 | 448.97] loss=1.90 avg=2.28\n",
      "[2013 | 449.02] loss=3.35 avg=2.29\n",
      "[2014 | 449.08] loss=2.22 avg=2.29\n",
      "[2015 | 449.13] loss=3.34 avg=2.30\n",
      "[2016 | 449.18] loss=2.55 avg=2.30\n",
      "[2017 | 449.23] loss=2.43 avg=2.30\n",
      "[2018 | 449.29] loss=1.95 avg=2.30\n",
      "[2019 | 449.34] loss=2.28 avg=2.30\n",
      "[2020 | 449.39] loss=1.42 avg=2.29\n",
      "[2021 | 449.44] loss=1.12 avg=2.28\n",
      "[2022 | 449.50] loss=2.12 avg=2.28\n",
      "[2023 | 449.55] loss=0.86 avg=2.26\n",
      "[2024 | 449.60] loss=1.40 avg=2.26\n",
      "[2025 | 449.65] loss=1.87 avg=2.25\n",
      "[2026 | 449.70] loss=1.66 avg=2.25\n",
      "[2027 | 449.76] loss=2.23 avg=2.25\n",
      "[2028 | 449.81] loss=2.28 avg=2.25\n",
      "[2029 | 449.86] loss=2.17 avg=2.25\n",
      "[2030 | 449.92] loss=2.45 avg=2.25\n",
      "[2031 | 449.97] loss=1.41 avg=2.24\n",
      "[2032 | 450.02] loss=1.81 avg=2.23\n",
      "[2033 | 450.07] loss=3.54 avg=2.25\n",
      "[2034 | 450.13] loss=2.37 avg=2.25\n",
      "[2035 | 450.18] loss=2.80 avg=2.25\n",
      "[2036 | 450.23] loss=3.02 avg=2.26\n",
      "[2037 | 450.29] loss=1.30 avg=2.25\n",
      "[2038 | 450.34] loss=2.68 avg=2.26\n",
      "[2039 | 450.39] loss=2.55 avg=2.26\n",
      "[2040 | 450.44] loss=2.97 avg=2.27\n",
      "[2041 | 450.49] loss=1.90 avg=2.26\n",
      "[2042 | 450.55] loss=1.73 avg=2.26\n",
      "[2043 | 450.60] loss=1.95 avg=2.25\n",
      "[2044 | 450.66] loss=2.33 avg=2.26\n",
      "[2045 | 450.71] loss=2.91 avg=2.26\n",
      "[2046 | 450.76] loss=2.75 avg=2.27\n",
      "[2047 | 450.81] loss=2.33 avg=2.27\n",
      "[2048 | 450.87] loss=1.87 avg=2.26\n",
      "[2049 | 450.92] loss=2.13 avg=2.26\n",
      "[2050 | 450.97] loss=1.41 avg=2.25\n",
      "[2051 | 451.03] loss=3.51 avg=2.27\n",
      "[2052 | 451.08] loss=1.24 avg=2.26\n",
      "[2053 | 451.14] loss=3.25 avg=2.27\n",
      "[2054 | 451.19] loss=1.24 avg=2.26\n",
      "[2055 | 451.25] loss=2.80 avg=2.26\n",
      "[2056 | 451.30] loss=4.16 avg=2.28\n",
      "[2057 | 451.36] loss=3.20 avg=2.29\n",
      "[2058 | 451.41] loss=1.97 avg=2.29\n",
      "[2059 | 451.47] loss=2.73 avg=2.29\n",
      "[2060 | 451.52] loss=3.23 avg=2.30\n",
      "[2061 | 451.57] loss=1.56 avg=2.29\n",
      "[2062 | 451.62] loss=3.02 avg=2.30\n",
      "[2063 | 451.68] loss=2.68 avg=2.30\n",
      "[2064 | 451.73] loss=2.70 avg=2.31\n",
      "[2065 | 451.79] loss=2.37 avg=2.31\n",
      "[2066 | 451.85] loss=2.81 avg=2.31\n",
      "[2067 | 451.90] loss=1.66 avg=2.31\n",
      "[2068 | 451.95] loss=2.35 avg=2.31\n",
      "[2069 | 452.01] loss=2.19 avg=2.31\n",
      "[2070 | 452.06] loss=2.99 avg=2.31\n",
      "[2071 | 452.11] loss=1.81 avg=2.31\n",
      "[2072 | 452.16] loss=1.67 avg=2.30\n",
      "[2073 | 452.22] loss=3.30 avg=2.31\n",
      "[2074 | 452.27] loss=2.04 avg=2.31\n",
      "[2075 | 452.32] loss=1.83 avg=2.30\n",
      "[2076 | 452.38] loss=1.45 avg=2.30\n",
      "[2077 | 452.43] loss=1.86 avg=2.29\n",
      "[2078 | 452.48] loss=2.50 avg=2.29\n",
      "[2079 | 452.53] loss=1.77 avg=2.29\n",
      "[2080 | 452.58] loss=1.49 avg=2.28\n",
      "[2081 | 452.63] loss=2.47 avg=2.28\n",
      "[2082 | 452.69] loss=1.91 avg=2.28\n",
      "[2083 | 452.74] loss=0.57 avg=2.26\n",
      "[2084 | 452.79] loss=2.63 avg=2.26\n",
      "[2085 | 452.85] loss=1.65 avg=2.26\n",
      "[2086 | 452.90] loss=1.89 avg=2.25\n",
      "[2087 | 452.95] loss=2.53 avg=2.26\n",
      "[2088 | 453.01] loss=2.89 avg=2.26\n",
      "[2089 | 453.06] loss=2.91 avg=2.27\n",
      "[2090 | 453.12] loss=2.17 avg=2.27\n",
      "[2091 | 453.17] loss=2.51 avg=2.27\n",
      "[2092 | 453.23] loss=3.28 avg=2.28\n",
      "[2093 | 453.28] loss=1.81 avg=2.28\n",
      "[2094 | 453.33] loss=2.21 avg=2.28\n",
      "[2095 | 453.38] loss=3.07 avg=2.28\n",
      "[2096 | 453.44] loss=3.71 avg=2.30\n",
      "[2097 | 453.49] loss=0.84 avg=2.28\n",
      "[2098 | 453.55] loss=2.26 avg=2.28\n",
      "[2099 | 453.60] loss=2.08 avg=2.28\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "     java.lang.System.out.println(this.getMessage());\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "if (((this.getMessage() == '?')) && (!((this.getMessage() == ' '))))) {\n",
      "try {\n",
      "if ((this.getMessage() == ' '))\n",
      "if ((this.getMessage() == ' '))\n",
      "return 0);\n",
      "return 0;\n",
      "this.getMessage();\n",
      "}\n",
      "return 0;\n",
      "}\n",
      "}\n",
      "\n",
      "//\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "if (this.getMessage() == '?')\n",
      "if (((this.getMessage() == ' ')) && (!this.getMessage() == ' '))\n",
      "return 0);\n",
      "if (((this.getMessage() == ' ')) elseif\n",
      "(this.getMessage() == ' '))\n",
      "return 0);\n",
      "if (((this.getMessage() == ' '))\n",
      "(this.getMessage() == ' '))\n",
      "return 0);\n",
      "return 0;\n",
      "}\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "}\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "return 0;\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "if (this.getMessage() == '?')\n",
      "\n",
      "\n",
      "if (((this.getMessage() == '.')) && !((this.getMessage() == ' '))\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      "  ))))\n",
      "\n",
      "if (e (this))))\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "#\t\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "if (!(this.getMessage() == ';))\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   else\n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "if (((this.getMessage() == ';))))   \n",
      "(i (this))) \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    case 0:\n",
      "\n",
      "      \n",
      "     \n",
      "    case 1:\n",
      "\n",
      "     \n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     }\n",
      "\n",
      "    {\n",
      "\n",
      "       \n",
      "   }\n",
      "\n",
      "     {\n",
      "\n",
      "        \n",
      "      }\n",
      "      \n",
      "     \n",
      "      \n",
      "       \n",
      "        \n",
      "\n",
      "       \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "    \n",
      "\n",
      "     }\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "       \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2100 | 469.55] validation loss = 2.32\n",
      "[2100 | 469.61] loss=1.23 avg=2.27\n",
      "[2101 | 469.66] loss=1.97 avg=2.27\n",
      "[2102 | 469.71] loss=1.71 avg=2.26\n",
      "[2103 | 469.76] loss=1.88 avg=2.26\n",
      "[2104 | 469.81] loss=3.09 avg=2.27\n",
      "[2105 | 469.87] loss=2.81 avg=2.27\n",
      "[2106 | 469.92] loss=1.27 avg=2.26\n",
      "[2107 | 469.98] loss=4.87 avg=2.29\n",
      "[2108 | 470.03] loss=1.50 avg=2.28\n",
      "[2109 | 470.08] loss=2.92 avg=2.29\n",
      "[2110 | 470.14] loss=2.37 avg=2.29\n",
      "[2111 | 470.19] loss=1.26 avg=2.28\n",
      "[2112 | 470.24] loss=2.55 avg=2.28\n",
      "[2113 | 470.30] loss=2.57 avg=2.28\n",
      "[2114 | 470.35] loss=2.93 avg=2.29\n",
      "[2115 | 470.40] loss=3.13 avg=2.30\n",
      "[2116 | 470.46] loss=2.11 avg=2.30\n",
      "[2117 | 470.51] loss=2.93 avg=2.30\n",
      "[2118 | 470.56] loss=2.08 avg=2.30\n",
      "[2119 | 470.62] loss=3.09 avg=2.31\n",
      "[2120 | 470.67] loss=2.44 avg=2.31\n",
      "[2121 | 470.72] loss=3.09 avg=2.32\n",
      "[2122 | 470.78] loss=1.30 avg=2.31\n",
      "[2123 | 470.83] loss=2.65 avg=2.31\n",
      "[2124 | 470.88] loss=4.03 avg=2.33\n",
      "[2125 | 470.94] loss=2.44 avg=2.33\n",
      "[2126 | 470.99] loss=2.35 avg=2.33\n",
      "[2127 | 471.05] loss=1.90 avg=2.32\n",
      "[2128 | 471.10] loss=2.47 avg=2.33\n",
      "[2129 | 471.15] loss=3.44 avg=2.34\n",
      "[2130 | 471.20] loss=2.37 avg=2.34\n",
      "[2131 | 471.26] loss=1.07 avg=2.32\n",
      "[2132 | 471.31] loss=2.10 avg=2.32\n",
      "[2133 | 471.36] loss=3.18 avg=2.33\n",
      "[2134 | 471.42] loss=3.00 avg=2.34\n",
      "[2135 | 471.47] loss=2.87 avg=2.34\n",
      "[2136 | 471.52] loss=2.73 avg=2.35\n",
      "[2137 | 471.58] loss=1.67 avg=2.34\n",
      "[2138 | 471.63] loss=2.35 avg=2.34\n",
      "[2139 | 471.68] loss=3.12 avg=2.35\n",
      "[2140 | 471.74] loss=3.10 avg=2.36\n",
      "[2141 | 471.79] loss=2.05 avg=2.35\n",
      "[2142 | 471.84] loss=1.18 avg=2.34\n",
      "[2143 | 471.89] loss=1.86 avg=2.34\n",
      "[2144 | 471.95] loss=2.59 avg=2.34\n",
      "[2145 | 472.00] loss=2.28 avg=2.34\n",
      "[2146 | 472.06] loss=3.43 avg=2.35\n",
      "[2147 | 472.11] loss=1.32 avg=2.34\n",
      "[2148 | 472.16] loss=2.88 avg=2.34\n",
      "[2149 | 472.22] loss=2.29 avg=2.34\n",
      "[2150 | 472.27] loss=1.78 avg=2.34\n",
      "[2151 | 472.32] loss=2.33 avg=2.34\n",
      "[2152 | 472.37] loss=0.91 avg=2.32\n",
      "[2153 | 472.43] loss=3.39 avg=2.33\n",
      "[2154 | 472.48] loss=2.68 avg=2.34\n",
      "[2155 | 472.53] loss=3.13 avg=2.35\n",
      "[2156 | 472.59] loss=2.61 avg=2.35\n",
      "[2157 | 472.64] loss=3.54 avg=2.36\n",
      "[2158 | 472.70] loss=2.07 avg=2.36\n",
      "[2159 | 472.75] loss=2.57 avg=2.36\n",
      "[2160 | 472.80] loss=3.44 avg=2.37\n",
      "[2161 | 472.86] loss=2.74 avg=2.37\n",
      "[2162 | 472.91] loss=2.60 avg=2.38\n",
      "[2163 | 472.97] loss=1.93 avg=2.37\n",
      "[2164 | 473.02] loss=0.87 avg=2.36\n",
      "[2165 | 473.07] loss=2.29 avg=2.36\n",
      "[2166 | 473.13] loss=3.23 avg=2.36\n",
      "[2167 | 473.18] loss=2.73 avg=2.37\n",
      "[2168 | 473.24] loss=1.41 avg=2.36\n",
      "[2169 | 473.29] loss=2.63 avg=2.36\n",
      "[2170 | 473.35] loss=1.98 avg=2.36\n",
      "[2171 | 473.40] loss=1.61 avg=2.35\n",
      "[2172 | 473.46] loss=2.33 avg=2.35\n",
      "[2173 | 473.51] loss=1.12 avg=2.34\n",
      "[2174 | 473.57] loss=1.18 avg=2.33\n",
      "[2175 | 473.62] loss=2.61 avg=2.33\n",
      "[2176 | 473.68] loss=2.68 avg=2.33\n",
      "[2177 | 473.73] loss=2.50 avg=2.33\n",
      "[2178 | 473.79] loss=3.72 avg=2.35\n",
      "[2179 | 473.84] loss=3.00 avg=2.35\n",
      "[2180 | 473.90] loss=1.06 avg=2.34\n",
      "[2181 | 473.95] loss=1.63 avg=2.33\n",
      "[2182 | 474.00] loss=1.53 avg=2.33\n",
      "[2183 | 474.06] loss=1.05 avg=2.31\n",
      "[2184 | 474.11] loss=2.04 avg=2.31\n",
      "[2185 | 474.17] loss=1.42 avg=2.30\n",
      "[2186 | 474.22] loss=2.66 avg=2.31\n",
      "[2187 | 474.27] loss=2.45 avg=2.31\n",
      "[2188 | 474.32] loss=2.80 avg=2.31\n",
      "[2189 | 474.38] loss=1.50 avg=2.30\n",
      "[2190 | 474.43] loss=1.36 avg=2.29\n",
      "[2191 | 474.48] loss=1.91 avg=2.29\n",
      "[2192 | 474.54] loss=1.80 avg=2.29\n",
      "[2193 | 474.59] loss=3.16 avg=2.29\n",
      "[2194 | 474.64] loss=2.86 avg=2.30\n",
      "[2195 | 474.70] loss=2.51 avg=2.30\n",
      "[2196 | 474.75] loss=2.56 avg=2.30\n",
      "[2197 | 474.81] loss=1.56 avg=2.30\n",
      "[2198 | 474.86] loss=2.39 avg=2.30\n",
      "[2199 | 474.91] loss=1.18 avg=2.29\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "oding (getId());\n",
      "         (string[0] = \"X\";\n",
      "        /*   */\n",
      "     } catch (java.lang.Exception e) {\n",
      "    }\n",
      "    java.lang.String *str = java.lang.String.parse(getInt());\n",
      "if (str) {\n",
      "     char *vars = java.lang.String.parse(str, (android.view.StringView *))(android.view.Vars.get());\n",
      "/* */\n",
      "      if (vars)\n",
      "             java.lang.String str = str;\n",
      "            android.widget.TextView textView = new android.widget.TextView(str);\n",
      "     }\n",
      "    android.view.LayoutInflater inflater = new android.widget.LayoutInflater(str);\n",
      "vars->put(vars);\n",
      "if (str)\n",
      "             android.widget.TextView textView.put(vars);\n",
      "if (vars)\n",
      "             {\n",
      "                               (string[0] = \"\";\n",
      "                             string[] data;\n",
      "                                 android.widget.LayoutParams layoutParams = new android.widget.LayoutParams();\n",
      "                   \n",
      "                          if (data.equals(getData(str, 0))) {                        \n",
      "                           \n",
      "                              android.widget.TextView data = new android.widget.TextView(str, data);\n",
      "         }\n",
      "              }\n",
      "   } throw new java.lang.UriException(string[0]) ;\n",
      "android.widget.TextView *textView = new android.widget.TextView(data);\n",
      "if (textView)\n",
      "             android.widget.TextView\n",
      "import android.view.Intent *t;\n",
      "if (t == null)\n",
      "            \n",
      "             \n",
      "              \n",
      "              \n",
      "           \n",
      "             \n",
      "          \n",
      "         \n",
      "         \n",
      "        \n",
      "    }\n",
      "    \n",
      "   \n",
      "     int (currentTextSeek) ? strText = strView.setText(text);\n",
      "if (strText)\n",
      "        int (currentTextClose)\n",
      "         \n",
      "            \n",
      "         \n",
      "       \n",
      "     \n",
      "     \n",
      "    \n",
      "   \n",
      "   \n",
      "    \n",
      "   \n",
      "   \n",
      "    android.content.IntentInfo info;\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "      \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "        \"android.widget.TextView    \") \n",
      "}\n",
      "}\n",
      "\n",
      "}\n",
      "}\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2200 | 490.89] validation loss = 2.31\n",
      "[2200 | 490.95] loss=3.26 avg=2.30\n",
      "[2201 | 491.01] loss=1.45 avg=2.29\n",
      "[2202 | 491.06] loss=1.57 avg=2.28\n",
      "[2203 | 491.12] loss=2.94 avg=2.29\n",
      "[2204 | 491.17] loss=3.62 avg=2.30\n",
      "[2205 | 491.22] loss=3.29 avg=2.31\n",
      "[2206 | 491.27] loss=2.94 avg=2.32\n",
      "[2207 | 491.33] loss=3.12 avg=2.33\n",
      "[2208 | 491.38] loss=1.84 avg=2.32\n",
      "[2209 | 491.44] loss=1.20 avg=2.31\n",
      "[2210 | 491.49] loss=1.81 avg=2.30\n",
      "[2211 | 491.54] loss=1.57 avg=2.30\n",
      "[2212 | 491.60] loss=2.13 avg=2.30\n",
      "[2213 | 491.65] loss=1.87 avg=2.29\n",
      "[2214 | 491.70] loss=1.40 avg=2.28\n",
      "[2215 | 491.75] loss=1.86 avg=2.28\n",
      "[2216 | 491.80] loss=2.02 avg=2.28\n",
      "[2217 | 491.86] loss=1.17 avg=2.26\n",
      "[2218 | 491.92] loss=3.54 avg=2.28\n",
      "[2219 | 491.97] loss=1.57 avg=2.27\n",
      "[2220 | 492.03] loss=1.42 avg=2.26\n",
      "[2221 | 492.08] loss=2.09 avg=2.26\n",
      "[2222 | 492.13] loss=3.26 avg=2.27\n",
      "[2223 | 492.19] loss=2.95 avg=2.28\n",
      "[2224 | 492.24] loss=1.99 avg=2.27\n",
      "[2225 | 492.29] loss=2.46 avg=2.28\n",
      "[2226 | 492.35] loss=1.87 avg=2.27\n",
      "[2227 | 492.40] loss=1.84 avg=2.27\n",
      "[2228 | 492.45] loss=2.43 avg=2.27\n",
      "[2229 | 492.50] loss=3.03 avg=2.28\n",
      "[2230 | 492.56] loss=3.66 avg=2.29\n",
      "[2231 | 492.61] loss=2.89 avg=2.30\n",
      "[2232 | 492.67] loss=1.39 avg=2.29\n",
      "[2233 | 492.72] loss=2.09 avg=2.29\n",
      "[2234 | 492.78] loss=3.60 avg=2.30\n",
      "[2235 | 492.84] loss=1.89 avg=2.29\n",
      "[2236 | 492.89] loss=3.19 avg=2.30\n",
      "[2237 | 492.94] loss=1.33 avg=2.29\n",
      "[2238 | 493.00] loss=1.18 avg=2.28\n",
      "[2239 | 493.05] loss=1.31 avg=2.27\n",
      "[2240 | 493.10] loss=3.03 avg=2.28\n",
      "[2241 | 493.16] loss=1.83 avg=2.28\n",
      "[2242 | 493.21] loss=2.65 avg=2.28\n",
      "[2243 | 493.27] loss=1.21 avg=2.27\n",
      "[2244 | 493.32] loss=2.25 avg=2.27\n",
      "[2245 | 493.38] loss=2.16 avg=2.27\n",
      "[2246 | 493.43] loss=2.88 avg=2.27\n",
      "[2247 | 493.49] loss=1.10 avg=2.26\n",
      "[2248 | 493.54] loss=2.72 avg=2.27\n",
      "[2249 | 493.60] loss=2.40 avg=2.27\n",
      "[2250 | 493.65] loss=1.61 avg=2.26\n",
      "[2251 | 493.71] loss=1.36 avg=2.25\n",
      "[2252 | 493.76] loss=1.22 avg=2.24\n",
      "[2253 | 493.81] loss=1.90 avg=2.24\n",
      "[2254 | 493.87] loss=2.00 avg=2.24\n",
      "[2255 | 493.91] loss=2.60 avg=2.24\n",
      "[2256 | 493.97] loss=1.10 avg=2.23\n",
      "[2257 | 494.03] loss=2.44 avg=2.23\n",
      "[2258 | 494.08] loss=2.64 avg=2.23\n",
      "[2259 | 494.14] loss=2.94 avg=2.24\n",
      "[2260 | 494.19] loss=0.82 avg=2.23\n",
      "[2261 | 494.25] loss=2.25 avg=2.23\n",
      "[2262 | 494.30] loss=2.18 avg=2.23\n",
      "[2263 | 494.35] loss=1.23 avg=2.22\n",
      "[2264 | 494.41] loss=2.30 avg=2.22\n",
      "[2265 | 494.46] loss=1.84 avg=2.21\n",
      "[2266 | 494.52] loss=2.08 avg=2.21\n",
      "[2267 | 494.57] loss=2.11 avg=2.21\n",
      "[2268 | 494.62] loss=1.60 avg=2.21\n",
      "[2269 | 494.68] loss=3.08 avg=2.21\n",
      "[2270 | 494.73] loss=1.75 avg=2.21\n",
      "[2271 | 494.78] loss=1.10 avg=2.20\n",
      "[2272 | 494.84] loss=3.78 avg=2.21\n",
      "[2273 | 494.89] loss=2.77 avg=2.22\n",
      "[2274 | 494.95] loss=1.89 avg=2.22\n",
      "[2275 | 495.00] loss=2.35 avg=2.22\n",
      "[2276 | 495.06] loss=2.19 avg=2.22\n",
      "[2277 | 495.11] loss=1.39 avg=2.21\n",
      "[2278 | 495.16] loss=3.44 avg=2.22\n",
      "[2279 | 495.22] loss=2.57 avg=2.23\n",
      "[2280 | 495.27] loss=3.17 avg=2.24\n",
      "[2281 | 495.33] loss=2.40 avg=2.24\n",
      "[2282 | 495.38] loss=3.05 avg=2.24\n",
      "[2283 | 495.44] loss=2.30 avg=2.25\n",
      "[2284 | 495.49] loss=1.95 avg=2.24\n",
      "[2285 | 495.54] loss=1.51 avg=2.24\n",
      "[2286 | 495.59] loss=2.59 avg=2.24\n",
      "[2287 | 495.65] loss=1.69 avg=2.23\n",
      "[2288 | 495.70] loss=1.73 avg=2.23\n",
      "[2289 | 495.75] loss=2.46 avg=2.23\n",
      "[2290 | 495.80] loss=1.70 avg=2.23\n",
      "[2291 | 495.86] loss=1.59 avg=2.22\n",
      "[2292 | 495.92] loss=2.04 avg=2.22\n",
      "[2293 | 495.97] loss=1.55 avg=2.21\n",
      "[2294 | 496.02] loss=2.62 avg=2.21\n",
      "[2295 | 496.08] loss=2.02 avg=2.21\n",
      "[2296 | 496.13] loss=2.76 avg=2.22\n",
      "[2297 | 496.18] loss=1.61 avg=2.21\n",
      "[2298 | 496.24] loss=1.74 avg=2.21\n",
      "[2299 | 496.29] loss=3.32 avg=2.22\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "|String}\n",
      "\n",
      "                                        \n",
      "\n",
      "                             \n",
      "\n",
      "                          \n",
      "\n",
      "                            \n",
      "\n",
      "                         \n",
      "\n",
      "                       \n",
      "\n",
      "                     \n",
      "\n",
      "                  \n",
      "\n",
      "                    \n",
      "\n",
      "                    \n",
      "                    \n",
      "\n",
      "                  \n",
      "\n",
      "              \n",
      "                 \n",
      "                 \n",
      "                 \n",
      "               \n",
      "            \n",
      "            \n",
      "          \n",
      "           \n",
      "  \n",
      "      \n",
      "      \n",
      "     \n",
      "      \n",
      "      \n",
      "  \n",
      "  \n",
      "   \n",
      "    \n",
      "  \n",
      "   \n",
      "   \n",
      "\n",
      "->\n",
      "\n",
      "\n",
      "if _M_ == 1\n",
      "\n",
      "\n",
      "      \n",
      "   \n",
      "   \n",
      "   \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "\n",
      "->\n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "   android.widget.LayoutParams\n",
      "   \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "     tv2_id\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "  \n",
      "\n",
      " I\t \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "    \n",
      "  \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "  \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "   \n",
      "\n",
      "      \n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "     \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  * *\n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2300 | 512.20] validation loss = 2.30\n",
      "[2300 | 512.25] loss=2.68 avg=2.22\n",
      "[2301 | 512.31] loss=1.93 avg=2.22\n",
      "[2302 | 512.36] loss=1.44 avg=2.21\n",
      "[2303 | 512.41] loss=1.18 avg=2.20\n",
      "[2304 | 512.46] loss=1.68 avg=2.20\n",
      "[2305 | 512.52] loss=3.19 avg=2.21\n",
      "[2306 | 512.57] loss=1.42 avg=2.20\n",
      "[2307 | 512.62] loss=1.60 avg=2.19\n",
      "[2308 | 512.68] loss=3.14 avg=2.20\n",
      "[2309 | 512.73] loss=1.10 avg=2.19\n",
      "[2310 | 512.79] loss=2.32 avg=2.19\n",
      "[2311 | 512.84] loss=2.72 avg=2.20\n",
      "[2312 | 512.90] loss=2.63 avg=2.20\n",
      "[2313 | 512.95] loss=2.59 avg=2.21\n",
      "[2314 | 513.01] loss=1.12 avg=2.20\n",
      "[2315 | 513.06] loss=2.38 avg=2.20\n",
      "[2316 | 513.11] loss=1.21 avg=2.19\n",
      "[2317 | 513.17] loss=1.63 avg=2.18\n",
      "[2318 | 513.22] loss=1.33 avg=2.17\n",
      "[2319 | 513.27] loss=3.39 avg=2.19\n",
      "[2320 | 513.32] loss=2.88 avg=2.19\n",
      "[2321 | 513.37] loss=1.03 avg=2.18\n",
      "[2322 | 513.43] loss=2.47 avg=2.18\n",
      "[2323 | 513.48] loss=1.63 avg=2.18\n",
      "[2324 | 513.54] loss=2.26 avg=2.18\n",
      "[2325 | 513.59] loss=1.22 avg=2.17\n",
      "[2326 | 513.65] loss=2.97 avg=2.18\n",
      "[2327 | 513.70] loss=2.37 avg=2.18\n",
      "[2328 | 513.76] loss=1.69 avg=2.17\n",
      "[2329 | 513.81] loss=1.68 avg=2.17\n",
      "[2330 | 513.86] loss=2.35 avg=2.17\n",
      "[2331 | 513.92] loss=2.14 avg=2.17\n",
      "[2332 | 513.97] loss=2.56 avg=2.17\n",
      "[2333 | 514.02] loss=1.09 avg=2.16\n",
      "[2334 | 514.07] loss=2.95 avg=2.17\n",
      "[2335 | 514.12] loss=2.55 avg=2.18\n",
      "[2336 | 514.18] loss=0.96 avg=2.16\n",
      "[2337 | 514.23] loss=3.24 avg=2.17\n",
      "[2338 | 514.29] loss=2.42 avg=2.18\n",
      "[2339 | 514.34] loss=2.76 avg=2.18\n",
      "[2340 | 514.40] loss=0.70 avg=2.17\n",
      "[2341 | 514.45] loss=2.69 avg=2.17\n",
      "[2342 | 514.50] loss=2.21 avg=2.17\n",
      "[2343 | 514.56] loss=1.74 avg=2.17\n",
      "[2344 | 514.61] loss=2.67 avg=2.17\n",
      "[2345 | 514.67] loss=2.54 avg=2.18\n",
      "[2346 | 514.72] loss=1.86 avg=2.17\n",
      "[2347 | 514.78] loss=1.69 avg=2.17\n",
      "[2348 | 514.83] loss=1.02 avg=2.16\n",
      "[2349 | 514.88] loss=2.09 avg=2.16\n",
      "[2350 | 514.94] loss=2.22 avg=2.16\n",
      "[2351 | 514.99] loss=1.47 avg=2.15\n",
      "[2352 | 515.05] loss=2.20 avg=2.15\n",
      "[2353 | 515.10] loss=2.79 avg=2.16\n",
      "[2354 | 515.15] loss=1.89 avg=2.16\n",
      "[2355 | 515.20] loss=1.19 avg=2.15\n",
      "[2356 | 515.26] loss=2.50 avg=2.15\n",
      "[2357 | 515.32] loss=1.69 avg=2.14\n",
      "[2358 | 515.37] loss=1.95 avg=2.14\n",
      "[2359 | 515.43] loss=1.40 avg=2.14\n",
      "[2360 | 515.48] loss=2.66 avg=2.14\n",
      "[2361 | 515.53] loss=1.32 avg=2.13\n",
      "[2362 | 515.59] loss=1.14 avg=2.12\n",
      "[2363 | 515.64] loss=2.02 avg=2.12\n",
      "[2364 | 515.70] loss=2.26 avg=2.12\n",
      "[2365 | 515.75] loss=2.50 avg=2.13\n",
      "[2366 | 515.80] loss=2.42 avg=2.13\n",
      "[2367 | 515.86] loss=2.47 avg=2.13\n",
      "[2368 | 515.91] loss=2.40 avg=2.14\n",
      "[2369 | 515.97] loss=3.06 avg=2.14\n",
      "[2370 | 516.02] loss=0.58 avg=2.13\n",
      "[2371 | 516.07] loss=2.83 avg=2.14\n",
      "[2372 | 516.13] loss=2.56 avg=2.14\n",
      "[2373 | 516.18] loss=3.41 avg=2.15\n",
      "[2374 | 516.23] loss=3.59 avg=2.17\n",
      "[2375 | 516.29] loss=2.14 avg=2.17\n",
      "[2376 | 516.34] loss=2.41 avg=2.17\n",
      "[2377 | 516.40] loss=1.91 avg=2.17\n",
      "[2378 | 516.46] loss=3.00 avg=2.18\n",
      "[2379 | 516.51] loss=2.60 avg=2.18\n",
      "[2380 | 516.57] loss=2.13 avg=2.18\n",
      "[2381 | 516.62] loss=1.85 avg=2.18\n",
      "[2382 | 516.68] loss=2.07 avg=2.17\n",
      "[2383 | 516.73] loss=2.46 avg=2.18\n",
      "[2384 | 516.78] loss=2.25 avg=2.18\n",
      "[2385 | 516.84] loss=2.62 avg=2.18\n",
      "[2386 | 516.90] loss=2.64 avg=2.19\n",
      "[2387 | 516.95] loss=3.01 avg=2.20\n",
      "[2388 | 517.01] loss=1.47 avg=2.19\n",
      "[2389 | 517.06] loss=2.01 avg=2.19\n",
      "[2390 | 517.11] loss=3.06 avg=2.20\n",
      "[2391 | 517.17] loss=1.22 avg=2.19\n",
      "[2392 | 517.22] loss=2.07 avg=2.18\n",
      "[2393 | 517.28] loss=1.98 avg=2.18\n",
      "[2394 | 517.33] loss=1.97 avg=2.18\n",
      "[2395 | 517.38] loss=1.92 avg=2.18\n",
      "[2396 | 517.44] loss=1.97 avg=2.18\n",
      "[2397 | 517.49] loss=1.90 avg=2.17\n",
      "[2398 | 517.55] loss=2.85 avg=2.18\n",
      "[2399 | 517.61] loss=2.96 avg=2.19\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "new;\n",
      "    }\n",
      "    }\n",
      "   if ((v2[0].isCancelable()) && (!(v2[0].isEmpty())) && (!(v2[0].isAlign())) && (!(v2[0] != (char *)).substring(0, '\\0')) && (v2[0].isAlign_p(0))) && (!(v2[0] != '0')))\n",
      "if (v2[0] != '0' && (v2[0] != '1')))\n",
      "pindex1 = new strlen(v2[1]);\n",
      "if (pindex1 != NULL)\n",
      "if (V2(pindex1, 1 + (v2[0])))\n",
      "else\n",
      "if ((x, y, z) % 40 )\n",
      "{\n",
      "mappend(fprintf(buf, \"%uu%c.%d %d%p\". \"', v2[0]);\n",
      "if (x, y, z)\n",
      "{\n",
      "x += z;\n",
      "   \n",
      "\n",
      "             \n",
      "                   break;\n",
      "   }\n",
      "   }\n",
      "  \n",
      "if ((v2[0] != '0' && (v2[0] != '1')))\n",
      "if ((v2[0] != (char *)).substring(0, '\\0')))\n",
      "if ((v2[0][1] && (!(v2[0] != '0')))\n",
      "for (var n = 0; n < cld.sizeof(v2[0].length); n++)\n",
      "{\n",
      "           \n",
      "                  \n",
      "                       else {\n",
      "                   \n",
      "        }\n",
      "   \n",
      "}\n",
      "/*\n",
      "\n",
      "                       \n",
      "    \n",
      "    if ((v2[0] != '1' && (v2[0] != '1')))\n",
      "   \n",
      "    if ((v2[0] != '1' && (v2[0] == '1')))\n",
      "   \n",
      "  case x in ('u\\0'):\n",
      "   \n",
      "   tm = new strlen(v2[1]);\n",
      "   return v2[0];\n",
      "  \n",
      "    if ((v2[0] != '1' && (v2[0] != '1')))\n",
      "   \n",
      "    tm = new strlen(v2[1]);\n",
      "  \n",
      "   break;\n",
      "  \n",
      " \n",
      " \n",
      "/*\n",
      "\n",
      "                      \n",
      "            case 0 :\n",
      "    if (v2[0].length != 1)\n",
      "   \n",
      "                \n",
      "                 continue;\n",
      "  \n",
      "    case 1 :\n",
      "     tm = new strlen(v2[1]);\n",
      "  \n",
      "    break;\n",
      "   \n",
      "   case 2 :\n",
      "     if ((v2[0] != '1' && (v2[0] == '1')))\n",
      "   \n",
      "               }\n",
      "  \n",
      "        default :\n",
      "    \n",
      "   \n",
      "      if ((v2[0] != '1' && (v2[0] == '1')))\n",
      "   \n",
      "    \n",
      "    tm = new strlen(v2[1]);\n",
      "  \n",
      "\n",
      "   \n",
      "  \n",
      "    if ((v2[0] != '1' && (v2[0] == '1')))\n",
      "   \n",
      "    \n",
      "            \n",
      "\n",
      "   \n",
      "   \n",
      "    if ((v2[0] != '1' && (v2[0] == '1')))\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2400 | 533.52] validation loss = 2.30\n",
      "[2400 | 533.58] loss=1.24 avg=2.18\n",
      "[2401 | 533.64] loss=1.67 avg=2.17\n",
      "[2402 | 533.70] loss=2.27 avg=2.17\n",
      "[2403 | 533.75] loss=1.34 avg=2.17\n",
      "[2404 | 533.81] loss=1.22 avg=2.16\n",
      "[2405 | 533.86] loss=1.20 avg=2.15\n",
      "[2406 | 533.91] loss=3.62 avg=2.16\n",
      "[2407 | 533.97] loss=1.86 avg=2.16\n",
      "[2408 | 534.02] loss=1.08 avg=2.15\n",
      "[2409 | 534.08] loss=2.88 avg=2.15\n",
      "[2410 | 534.13] loss=2.33 avg=2.16\n",
      "[2411 | 534.19] loss=1.24 avg=2.15\n",
      "[2412 | 534.24] loss=0.96 avg=2.14\n",
      "[2413 | 534.29] loss=2.14 avg=2.14\n",
      "[2414 | 534.34] loss=2.31 avg=2.14\n",
      "[2415 | 534.40] loss=1.84 avg=2.13\n",
      "[2416 | 534.45] loss=2.53 avg=2.14\n",
      "[2417 | 534.50] loss=3.87 avg=2.16\n",
      "[2418 | 534.56] loss=2.29 avg=2.16\n",
      "[2419 | 534.61] loss=2.57 avg=2.16\n",
      "[2420 | 534.67] loss=1.63 avg=2.16\n",
      "[2421 | 534.72] loss=2.12 avg=2.16\n",
      "[2422 | 534.77] loss=1.53 avg=2.15\n",
      "[2423 | 534.82] loss=3.33 avg=2.16\n",
      "[2424 | 534.88] loss=1.78 avg=2.16\n",
      "[2425 | 534.94] loss=2.71 avg=2.16\n",
      "[2426 | 534.99] loss=2.64 avg=2.17\n",
      "[2427 | 535.05] loss=1.65 avg=2.16\n",
      "[2428 | 535.10] loss=1.27 avg=2.15\n",
      "[2429 | 535.16] loss=2.28 avg=2.15\n",
      "[2430 | 535.21] loss=2.63 avg=2.16\n",
      "[2431 | 535.26] loss=2.96 avg=2.17\n",
      "[2432 | 535.31] loss=1.91 avg=2.16\n",
      "[2433 | 535.36] loss=2.38 avg=2.17\n",
      "[2434 | 535.42] loss=2.89 avg=2.17\n",
      "[2435 | 535.48] loss=2.01 avg=2.17\n",
      "[2436 | 535.53] loss=1.90 avg=2.17\n",
      "[2437 | 535.59] loss=2.20 avg=2.17\n",
      "[2438 | 535.64] loss=1.79 avg=2.17\n",
      "[2439 | 535.69] loss=0.91 avg=2.15\n",
      "[2440 | 535.75] loss=1.28 avg=2.14\n",
      "[2441 | 535.80] loss=2.27 avg=2.15\n",
      "[2442 | 535.86] loss=1.72 avg=2.14\n",
      "[2443 | 535.91] loss=3.23 avg=2.15\n",
      "[2444 | 535.97] loss=1.23 avg=2.14\n",
      "[2445 | 536.02] loss=2.05 avg=2.14\n",
      "[2446 | 536.07] loss=2.08 avg=2.14\n",
      "[2447 | 536.13] loss=3.00 avg=2.15\n",
      "[2448 | 536.18] loss=2.67 avg=2.16\n",
      "[2449 | 536.23] loss=1.65 avg=2.15\n",
      "[2450 | 536.29] loss=2.32 avg=2.15\n",
      "[2451 | 536.34] loss=1.88 avg=2.15\n",
      "[2452 | 536.40] loss=1.43 avg=2.14\n",
      "[2453 | 536.45] loss=2.32 avg=2.14\n",
      "[2454 | 536.51] loss=1.99 avg=2.14\n",
      "[2455 | 536.56] loss=2.75 avg=2.15\n",
      "[2456 | 536.62] loss=1.58 avg=2.14\n",
      "[2457 | 536.67] loss=2.91 avg=2.15\n",
      "[2458 | 536.73] loss=2.08 avg=2.15\n",
      "[2459 | 536.78] loss=2.46 avg=2.15\n",
      "[2460 | 536.83] loss=3.59 avg=2.17\n",
      "[2461 | 536.88] loss=2.63 avg=2.17\n",
      "[2462 | 536.93] loss=2.68 avg=2.18\n",
      "[2463 | 536.98] loss=1.36 avg=2.17\n",
      "[2464 | 537.04] loss=1.45 avg=2.16\n",
      "[2465 | 537.09] loss=3.58 avg=2.18\n",
      "[2466 | 537.14] loss=2.10 avg=2.18\n",
      "[2467 | 537.20] loss=2.07 avg=2.17\n",
      "[2468 | 537.25] loss=2.57 avg=2.18\n",
      "[2469 | 537.31] loss=1.73 avg=2.17\n",
      "[2470 | 537.36] loss=2.02 avg=2.17\n",
      "[2471 | 537.41] loss=2.06 avg=2.17\n",
      "[2472 | 537.47] loss=4.56 avg=2.20\n",
      "[2473 | 537.52] loss=1.39 avg=2.19\n",
      "[2474 | 537.57] loss=2.07 avg=2.19\n",
      "[2475 | 537.63] loss=1.74 avg=2.18\n",
      "[2476 | 537.68] loss=1.47 avg=2.17\n",
      "[2477 | 537.73] loss=3.72 avg=2.19\n",
      "[2478 | 537.79] loss=2.97 avg=2.20\n",
      "[2479 | 537.84] loss=2.15 avg=2.20\n",
      "[2480 | 537.90] loss=2.19 avg=2.20\n",
      "[2481 | 537.95] loss=1.97 avg=2.19\n",
      "[2482 | 538.01] loss=3.32 avg=2.21\n",
      "[2483 | 538.06] loss=2.13 avg=2.21\n",
      "[2484 | 538.12] loss=1.84 avg=2.20\n",
      "[2485 | 538.17] loss=2.83 avg=2.21\n",
      "[2486 | 538.22] loss=2.80 avg=2.21\n",
      "[2487 | 538.27] loss=1.67 avg=2.21\n",
      "[2488 | 538.33] loss=1.62 avg=2.20\n",
      "[2489 | 538.38] loss=1.22 avg=2.19\n",
      "[2490 | 538.43] loss=2.98 avg=2.20\n",
      "[2491 | 538.48] loss=2.96 avg=2.21\n",
      "[2492 | 538.53] loss=2.29 avg=2.21\n",
      "[2493 | 538.59] loss=2.79 avg=2.21\n",
      "[2494 | 538.64] loss=1.64 avg=2.21\n",
      "[2495 | 538.69] loss=2.02 avg=2.21\n",
      "[2496 | 538.75] loss=1.58 avg=2.20\n",
      "[2497 | 538.80] loss=3.28 avg=2.21\n",
      "[2498 | 538.86] loss=2.70 avg=2.22\n",
      "[2499 | 538.91] loss=3.11 avg=2.23\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "  (  } catch ( javafx.marshal.cursor.WrongChoice.java.util.Class m) {\n",
      "               \n",
      "            \n",
      "           \"      } });\n",
      "  \n",
      "   if (!(t(     \n",
      "       setChoiceListener(            \n",
      "           \n",
      "           \n",
      "          \"     });\n",
      "   \n",
      "/*\n",
      "         select_value \n",
      "        \n",
      "       if (select1 != select) {\n",
      "         \n",
      "           if ( select1.isEmpty()) {\n",
      "           \n",
      "           \n",
      "       }\n",
      "         \n",
      "       select1.append(\"    \");\n",
      " }\n",
      "   /*\n",
      "       break;\n",
      "   \n",
      "     \n",
      "     return\n",
      "      case \"select\" :\n",
      "      /*\n",
      "      select_value:\n",
      "      \n",
      "       pick_value = pick_selection + 1;\n",
      "      \n",
      "     \n",
      "   \n",
      "    break;\n",
      "   \n",
      "   case \"select_case\" :\n",
      "     \n",
      "    \n",
      "   \n",
      "   \n",
      "    getSelect(p, list;\n",
      "     \n",
      "     select_case = pick_select_case + 1;\n",
      "    \n",
      "  \n",
      "  \n",
      "  \n",
      "    break;\n",
      "  \n",
      " case \"selected\" :\n",
      "    \n",
      "  case \"selected_case\" :\n",
      "   \n",
      "   case \"selected_case\"\n",
      "   case \"selected_case_value\" :\n",
      "    \n",
      "   \n",
      "   \n",
      "   getSelect(a, list;\n",
      "   \n",
      "   \n",
      "   \n",
      "    getChoice(b, name;\n",
      "    \n",
      "   \n",
      "   \n",
      "    break;\n",
      "  \n",
      "  \n",
      "   case \"selected_selection\" :\n",
      "   \n",
      "   \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   break;\n",
      "  \n",
      " case \"selected_selected-case\" :\n",
      "   \n",
      "  \n",
      "  case \"selected_selected\" :\n",
      "  \n",
      "   break;\n",
      "   \n",
      " case \"selected_selected_case_value\" :\n",
      "   \n",
      "    case \"selected_selected_case_value__          \n",
      "   \n",
      "   \n",
      "   \n",
      "    getSelect(p, list;\n",
      "   \n",
      "   \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "    case\n",
      "     \n",
      "   \n",
      "   \n",
      "    break;\n",
      "   \n",
      "  case \"selected_selected\" :\n",
      "   \n",
      "  \n",
      "   \n",
      "    select_case = pick_selected_case + 1;\n",
      "    choose_select = select_select_case + 1;\n",
      "     pick_selected_case = select_select_case_value + 1;\n",
      "   \n",
      "  \n",
      "  \n",
      "   break;\n",
      "   \n",
      "  \n",
      "  case selectorName.toLowerCase(\n",
      "   \n",
      "    \n",
      "    select_case = select_if_selected_case;\n",
      "    select_case = select_if_selected_case;\n",
      "   \n",
      "  \n",
      "  \n",
      "    select case selectionName.toLowerCase(\n",
      "     \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "    choose_choice = select_if_selected_case_value;\n",
      "    choose_selection = select_if_selected_case_value\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500 | 554.78] validation loss = 2.29\n",
      "[2500 | 554.84] loss=3.80 avg=2.24\n",
      "[2501 | 554.90] loss=1.76 avg=2.24\n",
      "[2502 | 554.95] loss=2.29 avg=2.24\n",
      "[2503 | 555.01] loss=3.33 avg=2.25\n",
      "[2504 | 555.06] loss=1.74 avg=2.24\n",
      "[2505 | 555.12] loss=2.34 avg=2.24\n",
      "[2506 | 555.17] loss=2.78 avg=2.25\n",
      "[2507 | 555.23] loss=3.34 avg=2.26\n",
      "[2508 | 555.28] loss=2.65 avg=2.26\n",
      "[2509 | 555.33] loss=2.13 avg=2.26\n",
      "[2510 | 555.39] loss=1.71 avg=2.26\n",
      "[2511 | 555.44] loss=3.11 avg=2.27\n",
      "[2512 | 555.50] loss=1.66 avg=2.26\n",
      "[2513 | 555.55] loss=2.01 avg=2.26\n",
      "[2514 | 555.60] loss=2.49 avg=2.26\n",
      "[2515 | 555.66] loss=0.88 avg=2.25\n",
      "[2516 | 555.71] loss=2.20 avg=2.25\n",
      "[2517 | 555.76] loss=1.67 avg=2.24\n",
      "[2518 | 555.82] loss=1.42 avg=2.23\n",
      "[2519 | 555.87] loss=2.19 avg=2.23\n",
      "[2520 | 555.93] loss=2.75 avg=2.24\n",
      "[2521 | 555.99] loss=3.14 avg=2.24\n",
      "[2522 | 556.04] loss=1.83 avg=2.24\n",
      "[2523 | 556.10] loss=2.27 avg=2.24\n",
      "[2524 | 556.15] loss=3.08 avg=2.25\n",
      "[2525 | 556.20] loss=1.35 avg=2.24\n",
      "[2526 | 556.26] loss=1.80 avg=2.24\n",
      "[2527 | 556.31] loss=2.33 avg=2.24\n",
      "[2528 | 556.37] loss=1.98 avg=2.23\n",
      "[2529 | 556.42] loss=1.27 avg=2.22\n",
      "[2530 | 556.47] loss=2.64 avg=2.23\n",
      "[2531 | 556.53] loss=3.10 avg=2.24\n",
      "[2532 | 556.58] loss=1.32 avg=2.23\n",
      "[2533 | 556.63] loss=1.38 avg=2.22\n",
      "[2534 | 556.68] loss=3.37 avg=2.23\n",
      "[2535 | 556.74] loss=0.87 avg=2.22\n",
      "[2536 | 556.79] loss=1.02 avg=2.21\n",
      "[2537 | 556.85] loss=2.95 avg=2.21\n",
      "[2538 | 556.90] loss=2.07 avg=2.21\n",
      "[2539 | 556.96] loss=2.67 avg=2.22\n",
      "[2540 | 557.01] loss=1.57 avg=2.21\n",
      "[2541 | 557.06] loss=3.35 avg=2.22\n",
      "[2542 | 557.12] loss=1.43 avg=2.21\n",
      "[2543 | 557.17] loss=1.28 avg=2.20\n",
      "[2544 | 557.22] loss=2.61 avg=2.21\n",
      "[2545 | 557.28] loss=2.07 avg=2.21\n",
      "[2546 | 557.33] loss=1.87 avg=2.20\n",
      "[2547 | 557.38] loss=3.10 avg=2.21\n",
      "[2548 | 557.44] loss=1.95 avg=2.21\n",
      "[2549 | 557.49] loss=3.12 avg=2.22\n",
      "[2550 | 557.54] loss=2.22 avg=2.22\n",
      "[2551 | 557.60] loss=1.02 avg=2.21\n",
      "[2552 | 557.65] loss=0.81 avg=2.19\n",
      "[2553 | 557.70] loss=1.82 avg=2.19\n",
      "[2554 | 557.76] loss=1.60 avg=2.18\n",
      "[2555 | 557.81] loss=1.66 avg=2.18\n",
      "[2556 | 557.87] loss=0.96 avg=2.17\n",
      "[2557 | 557.92] loss=1.68 avg=2.16\n",
      "[2558 | 557.98] loss=0.51 avg=2.14\n",
      "[2559 | 558.03] loss=1.78 avg=2.14\n",
      "[2560 | 558.09] loss=3.25 avg=2.15\n",
      "[2561 | 558.14] loss=1.45 avg=2.14\n",
      "[2562 | 558.19] loss=2.59 avg=2.15\n",
      "[2563 | 558.25] loss=2.63 avg=2.15\n",
      "[2564 | 558.30] loss=2.17 avg=2.15\n",
      "[2565 | 558.35] loss=1.09 avg=2.14\n",
      "[2566 | 558.41] loss=1.71 avg=2.14\n",
      "[2567 | 558.46] loss=2.70 avg=2.14\n",
      "[2568 | 558.52] loss=3.75 avg=2.16\n",
      "[2569 | 558.57] loss=1.77 avg=2.16\n",
      "[2570 | 558.63] loss=3.05 avg=2.17\n",
      "[2571 | 558.68] loss=2.01 avg=2.16\n",
      "[2572 | 558.75] loss=1.93 avg=2.16\n",
      "[2573 | 558.80] loss=2.73 avg=2.17\n",
      "[2574 | 558.86] loss=2.20 avg=2.17\n",
      "[2575 | 558.91] loss=2.41 avg=2.17\n",
      "[2576 | 558.97] loss=1.68 avg=2.17\n",
      "[2577 | 559.02] loss=3.14 avg=2.18\n",
      "[2578 | 559.07] loss=2.35 avg=2.18\n",
      "[2579 | 559.12] loss=1.60 avg=2.17\n",
      "[2580 | 559.18] loss=2.40 avg=2.17\n",
      "[2581 | 559.23] loss=1.22 avg=2.16\n",
      "[2582 | 559.28] loss=2.72 avg=2.17\n",
      "[2583 | 559.34] loss=2.75 avg=2.18\n",
      "[2584 | 559.39] loss=2.07 avg=2.17\n",
      "[2585 | 559.45] loss=2.53 avg=2.18\n",
      "[2586 | 559.50] loss=2.33 avg=2.18\n",
      "[2587 | 559.55] loss=2.78 avg=2.19\n",
      "[2588 | 559.60] loss=1.28 avg=2.18\n",
      "[2589 | 559.66] loss=1.46 avg=2.17\n",
      "[2590 | 559.71] loss=2.42 avg=2.17\n",
      "[2591 | 559.76] loss=3.47 avg=2.18\n",
      "[2592 | 559.81] loss=2.03 avg=2.18\n",
      "[2593 | 559.87] loss=3.35 avg=2.19\n",
      "[2594 | 559.92] loss=2.59 avg=2.20\n",
      "[2595 | 559.97] loss=2.22 avg=2.20\n",
      "[2596 | 560.02] loss=2.86 avg=2.21\n",
      "[2597 | 560.08] loss=1.14 avg=2.19\n",
      "[2598 | 560.13] loss=2.46 avg=2.20\n",
      "[2599 | 560.18] loss=2.12 avg=2.20\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "].\n",
      "                 }\n",
      "               if (currentLineNumber < (1 + (lastLineNumber)))\n",
      "{\n",
      "               if (currentLineNumber < (nextLineNumber))\n",
      "{\n",
      "                            if (currentLineNumber == nextLineNumber)\n",
      "{                                            \n",
      "                                             break\n",
      "                                     }\n",
      "                                   }\n",
      "                      \n",
      "                   }\n",
      "      }\n",
      "    setTimeout(function (e, lastLineNumber, nextLineNumber, currentLineNumber, nextLineNumber) {\n",
      "          \n",
      "             \n",
      "            \n",
      "      }\n",
      "     if (currentLineNumber == nextLineNumber)\n",
      "{\n",
      "   if (currentLineNumber != currentLineNumber)\n",
      "   lastLineNumber = currentLineNumber;\n",
      "         \n",
      "         \n",
      "         \n",
      "        \"  \" return   \n",
      "   }\n",
      "    if (currentLineNumber != nextLineNumber)\n",
      "  {\n",
      "       \n",
      "        \n",
      "       \n",
      "       \n",
      "     \n",
      "     \n",
      "     in = nextLineNumber;\n",
      "    nextLineNumber = (nextLineNumber - currentLineNumber) + (nextLineNumber - currentLineNumber);\n",
      "   nextLineNumber++;\n",
      "   endLineNumber = (nextLineNumber < (nextLineNumber - 0);\n",
      "   continueLineNumber++;\n",
      "   ++(if (dereferencing(nextLineNumber, in, nextLineNumber), currentLineNumber) <= 0);\n",
      "   if (dereferencing(nextLineNumber, nextLineNumber < (nextLineNumber - startLineNumber)) <= 0)\n",
      "   return\n",
      "   == (startLineNumber++) || ((startLineNumber - 1));\n",
      "   continueLineNumber++;\n",
      "   ++(if (dereferencing(nextLineNumber, (nextLineNumber - 0), currentLineNumber) <= (startLineNumber - startLineNumber)) {\n",
      "                         \n",
      "          \n",
      "         \n",
      "               \n",
      "              } else {\n",
      "            \n",
      "            }\n",
      "           \n",
      "     }\n",
      "    \n",
      "     \n",
      "     }\n",
      "    else\n",
      "       \n",
      "    )\n",
      "  \n",
      "  \n",
      "   *\n",
      "\n",
      "               \n",
      "  \n",
      "      \n",
      "      \n",
      "       \n",
      "        \n",
      "        \n",
      "            \n",
      "            \n",
      "         \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2600 | 576.12] validation loss = 2.29\n",
      "[2600 | 576.18] loss=1.42 avg=2.19\n",
      "[2601 | 576.23] loss=2.49 avg=2.19\n",
      "[2602 | 576.29] loss=2.54 avg=2.20\n",
      "[2603 | 576.35] loss=2.41 avg=2.20\n",
      "[2604 | 576.40] loss=2.09 avg=2.20\n",
      "[2605 | 576.46] loss=3.28 avg=2.21\n",
      "[2606 | 576.51] loss=2.96 avg=2.21\n",
      "[2607 | 576.56] loss=2.75 avg=2.22\n",
      "[2608 | 576.62] loss=2.15 avg=2.22\n",
      "[2609 | 576.67] loss=1.84 avg=2.22\n",
      "[2610 | 576.72] loss=2.34 avg=2.22\n",
      "[2611 | 576.78] loss=2.46 avg=2.22\n",
      "[2612 | 576.83] loss=2.44 avg=2.22\n",
      "[2613 | 576.88] loss=1.48 avg=2.21\n",
      "[2614 | 576.94] loss=1.67 avg=2.21\n",
      "[2615 | 576.99] loss=2.05 avg=2.21\n",
      "[2616 | 577.04] loss=3.33 avg=2.22\n",
      "[2617 | 577.10] loss=2.13 avg=2.22\n",
      "[2618 | 577.15] loss=2.49 avg=2.22\n",
      "[2619 | 577.21] loss=1.84 avg=2.22\n",
      "[2620 | 577.27] loss=1.21 avg=2.21\n",
      "[2621 | 577.32] loss=1.89 avg=2.20\n",
      "[2622 | 577.37] loss=2.42 avg=2.21\n",
      "[2623 | 577.43] loss=2.94 avg=2.21\n",
      "[2624 | 577.49] loss=2.71 avg=2.22\n",
      "[2625 | 577.54] loss=2.40 avg=2.22\n",
      "[2626 | 577.59] loss=2.35 avg=2.22\n",
      "[2627 | 577.65] loss=4.23 avg=2.24\n",
      "[2628 | 577.70] loss=2.02 avg=2.24\n",
      "[2629 | 577.75] loss=3.14 avg=2.25\n",
      "[2630 | 577.80] loss=1.94 avg=2.24\n",
      "[2631 | 577.86] loss=1.33 avg=2.24\n",
      "[2632 | 577.92] loss=2.30 avg=2.24\n",
      "[2633 | 577.97] loss=2.70 avg=2.24\n",
      "[2634 | 578.03] loss=2.32 avg=2.24\n",
      "[2635 | 578.09] loss=1.64 avg=2.24\n",
      "[2636 | 578.14] loss=1.87 avg=2.23\n",
      "[2637 | 578.20] loss=1.91 avg=2.23\n",
      "[2638 | 578.25] loss=1.51 avg=2.22\n",
      "[2639 | 578.31] loss=2.81 avg=2.23\n",
      "[2640 | 578.37] loss=1.34 avg=2.22\n",
      "[2641 | 578.42] loss=1.46 avg=2.21\n",
      "[2642 | 578.47] loss=2.01 avg=2.21\n",
      "[2643 | 578.53] loss=0.88 avg=2.20\n",
      "[2644 | 578.58] loss=3.18 avg=2.21\n",
      "[2645 | 578.64] loss=2.20 avg=2.21\n",
      "[2646 | 578.69] loss=1.84 avg=2.20\n",
      "[2647 | 578.74] loss=3.17 avg=2.21\n",
      "[2648 | 578.80] loss=1.83 avg=2.21\n",
      "[2649 | 578.85] loss=1.33 avg=2.20\n",
      "[2650 | 578.90] loss=1.59 avg=2.19\n",
      "[2651 | 578.96] loss=2.74 avg=2.20\n",
      "[2652 | 579.02] loss=2.43 avg=2.20\n",
      "[2653 | 579.07] loss=2.71 avg=2.21\n",
      "[2654 | 579.12] loss=2.52 avg=2.21\n",
      "[2655 | 579.18] loss=1.41 avg=2.20\n",
      "[2656 | 579.23] loss=4.02 avg=2.22\n",
      "[2657 | 579.28] loss=0.97 avg=2.21\n",
      "[2658 | 579.33] loss=3.31 avg=2.22\n",
      "[2659 | 579.39] loss=1.78 avg=2.21\n",
      "[2660 | 579.44] loss=3.60 avg=2.23\n",
      "[2661 | 579.50] loss=1.87 avg=2.22\n",
      "[2662 | 579.55] loss=1.07 avg=2.21\n",
      "[2663 | 579.60] loss=2.68 avg=2.22\n",
      "[2664 | 579.65] loss=2.28 avg=2.22\n",
      "[2665 | 579.71] loss=2.65 avg=2.22\n",
      "[2666 | 579.76] loss=3.00 avg=2.23\n",
      "[2667 | 579.81] loss=1.25 avg=2.22\n",
      "[2668 | 579.87] loss=1.64 avg=2.21\n",
      "[2669 | 579.92] loss=3.01 avg=2.22\n",
      "[2670 | 579.98] loss=2.67 avg=2.23\n",
      "[2671 | 580.03] loss=3.18 avg=2.24\n",
      "[2672 | 580.08] loss=3.00 avg=2.24\n",
      "[2673 | 580.13] loss=3.17 avg=2.25\n",
      "[2674 | 580.19] loss=3.26 avg=2.26\n",
      "[2675 | 580.24] loss=3.12 avg=2.27\n",
      "[2676 | 580.30] loss=0.88 avg=2.26\n",
      "[2677 | 580.36] loss=1.11 avg=2.25\n",
      "[2678 | 580.41] loss=2.44 avg=2.25\n",
      "[2679 | 580.47] loss=1.72 avg=2.24\n",
      "[2680 | 580.52] loss=2.67 avg=2.25\n",
      "[2681 | 580.57] loss=1.91 avg=2.24\n",
      "[2682 | 580.63] loss=1.76 avg=2.24\n",
      "[2683 | 580.68] loss=2.30 avg=2.24\n",
      "[2684 | 580.74] loss=2.95 avg=2.25\n",
      "[2685 | 580.79] loss=1.48 avg=2.24\n",
      "[2686 | 580.85] loss=3.11 avg=2.25\n",
      "[2687 | 580.90] loss=0.89 avg=2.23\n",
      "[2688 | 580.96] loss=2.44 avg=2.24\n",
      "[2689 | 581.01] loss=1.77 avg=2.23\n",
      "[2690 | 581.06] loss=2.12 avg=2.23\n",
      "[2691 | 581.11] loss=2.13 avg=2.23\n",
      "[2692 | 581.17] loss=1.66 avg=2.22\n",
      "[2693 | 581.22] loss=2.70 avg=2.23\n",
      "[2694 | 581.27] loss=1.75 avg=2.22\n",
      "[2695 | 581.32] loss=1.60 avg=2.22\n",
      "[2696 | 581.38] loss=2.28 avg=2.22\n",
      "[2697 | 581.43] loss=2.73 avg=2.22\n",
      "[2698 | 581.49] loss=2.57 avg=2.23\n",
      "[2699 | 581.55] loss=2.13 avg=2.23\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "end = \"\";\n",
      "          \"\";\n",
      "         a[2], a[1]-a[0] = -s[a] + 0;\n",
      "       \";\n",
      "      \n",
      "       if (a[ - 1 ] < 0 ) {\n",
      "        i[a] += (i[ + i[ + i[ + i[ + i[ + i[ ] ] ] ] ] ] & 0x;\n",
      "     }\n",
      "      if (a[- 1 ] < 0) {\n",
      "       m[a] *= a[a];\n",
      "      if (a[a- 1 ] > 0) {\n",
      "         m[a] *= a[a];\n",
      "    }\n",
      "\n",
      "\n",
      "/*\n",
      "\n",
      "1 * Convert number 1/2 to n\t, first number to ary\n",
      "  */\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "     for (i = 0; i < i. length; i++) {\n",
      "       if(a[i]) {\n",
      "        e[i] = (i[i]);\n",
      "     }\n",
      "    \n",
      "    /*\n",
      "     i[i] += i[i[i]];\n",
      "    if (a[i]) {\n",
      "       m[a] = a[a];\n",
      "\n",
      "     }\n",
      "   \n",
      "   if (s[a] < 0 && s[+2] < 0) {\n",
      "       a[a] = a[a];\n",
      "     } {\n",
      "      if (a[b][i][0]) {\n",
      "       e[a] = g[a];\n",
      "     }\n",
      "   \n",
      "  \n",
      "\n",
      "\n",
      "  else\n",
      "\n",
      "   \n",
      "     for (      b[i]) {\n",
      "        m[a] = a[a * b];\n",
      "    \n",
      "\n",
      "   \n",
      "    a[b] = a[((i[ + i[ + i[ + i[ = i[ - i[ + i[ + i[ ] ] ] ] ] ] ]] + b[j];\n",
      "    \n",
      "\n",
      "    if (if[if[if[if[if[if[if [[if[if[if[if[if[if[if[if[if[if[if ][return *]][return *] *]]])] *]]] *= 1) {\n",
      "        return a;\n",
      "    \n",
      "       for (if[if[if[if[if[if[if[if[if[if[if[return x]]]]] *]][return x]] & 0x;\n",
      "\n",
      "       return a;\n",
      "     }\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "   break [][if[if[if[if[if[if[if\n",
      "\n",
      "      } ]][return \";\")]];\n",
      "\n",
      "   \n",
      "    else\n",
      "\n",
      "      \n",
      "\n",
      "     if(this) {\n",
      "\n",
      "       try\n",
      "                            if(s[this] < 0) {\n",
      "          m[a] = g[a];\n",
      "          if(a[b] < 0) {\n",
      "           m[a] = g[a];\n",
      "          if(a[b][i][0]) {\n",
      "           if(m[a] == m[a]->n\t) {\n",
      "          m[a] = m[a * b / - b];\n",
      "            if(if[if[if[if *].f\t && if[if[if[if[if[if\n",
      "\n",
      "   if (if[if[if[if[if[if][if [if if [if[if [if *]]] ]* =]]] / =\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2700 | 597.56] validation loss = 2.28\n",
      "[2700 | 597.62] loss=1.63 avg=2.22\n",
      "[2701 | 597.68] loss=2.44 avg=2.22\n",
      "[2702 | 597.73] loss=2.25 avg=2.22\n",
      "[2703 | 597.78] loss=2.97 avg=2.23\n",
      "[2704 | 597.84] loss=1.67 avg=2.22\n",
      "[2705 | 597.90] loss=2.42 avg=2.23\n",
      "[2706 | 597.95] loss=3.73 avg=2.24\n",
      "[2707 | 598.01] loss=2.53 avg=2.24\n",
      "[2708 | 598.06] loss=2.24 avg=2.24\n",
      "[2709 | 598.11] loss=3.18 avg=2.25\n",
      "[2710 | 598.17] loss=2.57 avg=2.26\n",
      "[2711 | 598.22] loss=1.09 avg=2.24\n",
      "[2712 | 598.27] loss=2.49 avg=2.25\n",
      "[2713 | 598.32] loss=3.03 avg=2.25\n",
      "[2714 | 598.38] loss=3.99 avg=2.27\n",
      "[2715 | 598.43] loss=1.11 avg=2.26\n",
      "[2716 | 598.49] loss=2.21 avg=2.26\n",
      "[2717 | 598.54] loss=1.05 avg=2.25\n",
      "[2718 | 598.59] loss=2.20 avg=2.25\n",
      "[2719 | 598.65] loss=2.39 avg=2.25\n",
      "[2720 | 598.70] loss=1.96 avg=2.25\n",
      "[2721 | 598.76] loss=2.31 avg=2.25\n",
      "[2722 | 598.81] loss=1.01 avg=2.23\n",
      "[2723 | 598.86] loss=3.66 avg=2.25\n",
      "[2724 | 598.92] loss=1.86 avg=2.24\n",
      "[2725 | 598.97] loss=3.42 avg=2.26\n",
      "[2726 | 599.03] loss=1.75 avg=2.25\n",
      "[2727 | 599.08] loss=2.49 avg=2.25\n",
      "[2728 | 599.13] loss=1.92 avg=2.25\n",
      "[2729 | 599.19] loss=3.58 avg=2.26\n",
      "[2730 | 599.25] loss=0.85 avg=2.25\n",
      "[2731 | 599.30] loss=3.00 avg=2.26\n",
      "[2732 | 599.36] loss=3.76 avg=2.27\n",
      "[2733 | 599.40] loss=2.73 avg=2.28\n",
      "[2734 | 599.46] loss=3.21 avg=2.29\n",
      "[2735 | 599.51] loss=3.88 avg=2.30\n",
      "[2736 | 599.56] loss=2.47 avg=2.30\n",
      "[2737 | 599.62] loss=2.33 avg=2.30\n",
      "[2738 | 599.67] loss=1.44 avg=2.29\n",
      "[2739 | 599.73] loss=1.52 avg=2.29\n",
      "[2740 | 599.78] loss=2.53 avg=2.29\n",
      "[2741 | 599.84] loss=3.38 avg=2.30\n",
      "[2742 | 599.89] loss=3.23 avg=2.31\n",
      "[2743 | 599.95] loss=1.68 avg=2.30\n",
      "[2744 | 600.00] loss=1.35 avg=2.29\n",
      "[2745 | 600.06] loss=2.41 avg=2.30\n",
      "[2746 | 600.11] loss=2.56 avg=2.30\n",
      "[2747 | 600.17] loss=2.88 avg=2.30\n",
      "[2748 | 600.22] loss=1.47 avg=2.30\n",
      "[2749 | 600.28] loss=2.64 avg=2.30\n",
      "[2750 | 600.33] loss=1.41 avg=2.29\n",
      "[2751 | 600.38] loss=1.30 avg=2.28\n",
      "[2752 | 600.44] loss=2.00 avg=2.28\n",
      "[2753 | 600.50] loss=2.59 avg=2.28\n",
      "[2754 | 600.55] loss=2.33 avg=2.28\n",
      "[2755 | 600.60] loss=2.95 avg=2.29\n",
      "[2756 | 600.66] loss=1.35 avg=2.28\n",
      "[2757 | 600.71] loss=1.63 avg=2.27\n",
      "[2758 | 600.76] loss=2.07 avg=2.27\n",
      "[2759 | 600.82] loss=2.60 avg=2.27\n",
      "[2760 | 600.87] loss=3.29 avg=2.28\n",
      "[2761 | 600.92] loss=3.34 avg=2.29\n",
      "[2762 | 600.98] loss=1.74 avg=2.29\n",
      "[2763 | 601.03] loss=2.39 avg=2.29\n",
      "[2764 | 601.08] loss=1.79 avg=2.28\n",
      "[2765 | 601.14] loss=2.65 avg=2.29\n",
      "[2766 | 601.19] loss=1.64 avg=2.28\n",
      "[2767 | 601.25] loss=1.87 avg=2.28\n",
      "[2768 | 601.30] loss=2.04 avg=2.27\n",
      "[2769 | 601.35] loss=1.93 avg=2.27\n",
      "[2770 | 601.41] loss=3.27 avg=2.28\n",
      "[2771 | 601.46] loss=1.40 avg=2.27\n",
      "[2772 | 601.52] loss=2.24 avg=2.27\n",
      "[2773 | 601.57] loss=1.56 avg=2.27\n",
      "[2774 | 601.62] loss=2.01 avg=2.26\n",
      "[2775 | 601.68] loss=1.51 avg=2.26\n",
      "[2776 | 601.73] loss=2.97 avg=2.26\n",
      "[2777 | 601.78] loss=1.71 avg=2.26\n",
      "[2778 | 601.84] loss=2.46 avg=2.26\n",
      "[2779 | 601.89] loss=1.63 avg=2.25\n",
      "[2780 | 601.95] loss=2.51 avg=2.26\n",
      "[2781 | 602.01] loss=2.65 avg=2.26\n",
      "[2782 | 602.06] loss=2.78 avg=2.26\n",
      "[2783 | 602.12] loss=1.53 avg=2.26\n",
      "[2784 | 602.18] loss=0.96 avg=2.24\n",
      "[2785 | 602.23] loss=1.16 avg=2.23\n",
      "[2786 | 602.28] loss=3.85 avg=2.25\n",
      "[2787 | 602.34] loss=2.64 avg=2.25\n",
      "[2788 | 602.39] loss=2.02 avg=2.25\n",
      "[2789 | 602.45] loss=1.45 avg=2.24\n",
      "[2790 | 602.50] loss=2.66 avg=2.25\n",
      "[2791 | 602.56] loss=2.16 avg=2.25\n",
      "[2792 | 602.61] loss=2.24 avg=2.25\n",
      "[2793 | 602.66] loss=2.12 avg=2.24\n",
      "[2794 | 602.72] loss=1.48 avg=2.24\n",
      "[2795 | 602.78] loss=3.14 avg=2.25\n",
      "[2796 | 602.83] loss=1.45 avg=2.24\n",
      "[2797 | 602.89] loss=1.86 avg=2.23\n",
      "[2798 | 602.94] loss=1.64 avg=2.23\n",
      "[2799 | 603.00] loss=2.89 avg=2.23\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "  \n",
      "          if (e == NULL && e.get_type == INT_VAR) {\n",
      "          if ((strcmp(pw, \"\\d\\d\\d\\d\", xn)) == 0 && strlen(chr) == 0)\n",
      "                        || (strcmp(pw, \"\\d\\d\\d\\d\\d\", xn).equals(xn)) && strlen(chr) == null == 0)\n",
      "                    else\n",
      "                      if (e)\n",
      "                          else\n",
      "                         \n",
      "                     \n",
      "                           \n",
      "                         return chr;\n",
      "           } return  \n",
      "    }\n",
      "                      \n",
      "                    \n",
      "                     \n",
      "                  \n",
      "                     })\n",
      "     {                      }\n",
      "                     \n",
      "                   \n",
      "                 }\n",
      "\n",
      "  \n",
      "                 \n",
      "                  }\n",
      "               \n",
      "               \n",
      "          \n",
      "       \n",
      "\n",
      "      \n",
      "    \n",
      "     \n",
      "\n",
      "      \n",
      "    \n",
      "     \n",
      "\n",
      "     \n",
      "     \n",
      "    \n",
      "\n",
      "     \n",
      "     \n",
      "\n",
      "     \n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "      {\n",
      "                \n",
      "\n",
      "\n",
      "               \n",
      "           \n",
      "\n",
      "               \n",
      "             \n",
      "                    \n",
      "\n",
      "       \n",
      "\n",
      "      \n",
      "\n",
      "      \n",
      "\n",
      "      \n",
      "            \n",
      "             \n",
      "             \n",
      "                \n",
      "\n",
      "      \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "  }\n",
      "  \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   )--\n",
      "\n",
      "   \n",
      "\n",
      " )--\n",
      "\n",
      "  \n",
      "  )--\n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2800 | 618.94] validation loss = 2.27\n",
      "[2800 | 619.00] loss=1.48 avg=2.23\n",
      "[2801 | 619.06] loss=1.19 avg=2.22\n",
      "[2802 | 619.12] loss=2.46 avg=2.22\n",
      "[2803 | 619.17] loss=1.74 avg=2.21\n",
      "[2804 | 619.22] loss=1.99 avg=2.21\n",
      "[2805 | 619.28] loss=0.55 avg=2.20\n",
      "[2806 | 619.33] loss=1.70 avg=2.19\n",
      "[2807 | 619.38] loss=3.34 avg=2.20\n",
      "[2808 | 619.44] loss=2.36 avg=2.20\n",
      "[2809 | 619.49] loss=2.95 avg=2.21\n",
      "[2810 | 619.55] loss=1.34 avg=2.20\n",
      "[2811 | 619.60] loss=1.49 avg=2.20\n",
      "[2812 | 619.65] loss=2.21 avg=2.20\n",
      "[2813 | 619.71] loss=2.76 avg=2.20\n",
      "[2814 | 619.76] loss=1.34 avg=2.19\n",
      "[2815 | 619.82] loss=1.58 avg=2.19\n",
      "[2816 | 619.87] loss=1.51 avg=2.18\n",
      "[2817 | 619.93] loss=2.08 avg=2.18\n",
      "[2818 | 619.98] loss=1.60 avg=2.17\n",
      "[2819 | 620.04] loss=1.85 avg=2.17\n",
      "[2820 | 620.09] loss=3.12 avg=2.18\n",
      "[2821 | 620.15] loss=2.30 avg=2.18\n",
      "[2822 | 620.20] loss=2.02 avg=2.18\n",
      "[2823 | 620.25] loss=2.10 avg=2.18\n",
      "[2824 | 620.30] loss=1.88 avg=2.18\n",
      "[2825 | 620.36] loss=1.76 avg=2.17\n",
      "[2826 | 620.41] loss=1.69 avg=2.17\n",
      "[2827 | 620.47] loss=3.17 avg=2.18\n",
      "[2828 | 620.52] loss=2.99 avg=2.18\n",
      "[2829 | 620.57] loss=1.90 avg=2.18\n",
      "[2830 | 620.63] loss=2.72 avg=2.19\n",
      "[2831 | 620.68] loss=2.84 avg=2.19\n",
      "[2832 | 620.74] loss=2.91 avg=2.20\n",
      "[2833 | 620.79] loss=2.97 avg=2.21\n",
      "[2834 | 620.85] loss=2.17 avg=2.21\n",
      "[2835 | 620.90] loss=1.94 avg=2.21\n",
      "[2836 | 620.96] loss=2.43 avg=2.21\n",
      "[2837 | 621.01] loss=2.27 avg=2.21\n",
      "[2838 | 621.07] loss=1.24 avg=2.20\n",
      "[2839 | 621.12] loss=0.98 avg=2.19\n",
      "[2840 | 621.18] loss=0.72 avg=2.17\n",
      "[2841 | 621.23] loss=2.01 avg=2.17\n",
      "[2842 | 621.28] loss=2.08 avg=2.17\n",
      "[2843 | 621.34] loss=3.07 avg=2.18\n",
      "[2844 | 621.39] loss=1.46 avg=2.17\n",
      "[2845 | 621.45] loss=2.50 avg=2.17\n",
      "[2846 | 621.50] loss=1.99 avg=2.17\n",
      "[2847 | 621.55] loss=1.64 avg=2.17\n",
      "[2848 | 621.61] loss=2.29 avg=2.17\n",
      "[2849 | 621.66] loss=2.66 avg=2.17\n",
      "[2850 | 621.72] loss=3.33 avg=2.18\n",
      "[2851 | 621.77] loss=2.18 avg=2.18\n",
      "[2852 | 621.83] loss=3.08 avg=2.19\n",
      "[2853 | 621.88] loss=0.96 avg=2.18\n",
      "[2854 | 621.94] loss=1.47 avg=2.17\n",
      "[2855 | 621.99] loss=3.23 avg=2.18\n",
      "[2856 | 622.04] loss=2.94 avg=2.19\n",
      "[2857 | 622.10] loss=2.65 avg=2.20\n",
      "[2858 | 622.15] loss=2.97 avg=2.20\n",
      "[2859 | 622.20] loss=2.11 avg=2.20\n",
      "[2860 | 622.26] loss=2.95 avg=2.21\n",
      "[2861 | 622.31] loss=1.02 avg=2.20\n",
      "[2862 | 622.37] loss=2.82 avg=2.21\n",
      "[2863 | 622.42] loss=3.63 avg=2.22\n",
      "[2864 | 622.47] loss=3.04 avg=2.23\n",
      "[2865 | 622.53] loss=2.43 avg=2.23\n",
      "[2866 | 622.58] loss=3.00 avg=2.24\n",
      "[2867 | 622.64] loss=0.64 avg=2.22\n",
      "[2868 | 622.69] loss=1.85 avg=2.22\n",
      "[2869 | 622.74] loss=2.42 avg=2.22\n",
      "[2870 | 622.80] loss=1.71 avg=2.22\n",
      "[2871 | 622.85] loss=1.12 avg=2.20\n",
      "[2872 | 622.91] loss=1.15 avg=2.19\n",
      "[2873 | 622.96] loss=2.79 avg=2.20\n",
      "[2874 | 623.01] loss=1.90 avg=2.20\n",
      "[2875 | 623.07] loss=2.06 avg=2.20\n",
      "[2876 | 623.12] loss=2.21 avg=2.20\n",
      "[2877 | 623.18] loss=2.02 avg=2.19\n",
      "[2878 | 623.23] loss=2.60 avg=2.20\n",
      "[2879 | 623.29] loss=1.60 avg=2.19\n",
      "[2880 | 623.34] loss=1.95 avg=2.19\n",
      "[2881 | 623.40] loss=2.04 avg=2.19\n",
      "[2882 | 623.45] loss=2.81 avg=2.19\n",
      "[2883 | 623.51] loss=1.97 avg=2.19\n",
      "[2884 | 623.56] loss=3.26 avg=2.20\n",
      "[2885 | 623.61] loss=2.67 avg=2.21\n",
      "[2886 | 623.66] loss=1.83 avg=2.20\n",
      "[2887 | 623.72] loss=3.06 avg=2.21\n",
      "[2888 | 623.77] loss=1.41 avg=2.20\n",
      "[2889 | 623.82] loss=1.90 avg=2.20\n",
      "[2890 | 623.88] loss=1.92 avg=2.20\n",
      "[2891 | 623.93] loss=1.70 avg=2.19\n",
      "[2892 | 623.99] loss=1.71 avg=2.19\n",
      "[2893 | 624.05] loss=2.26 avg=2.19\n",
      "[2894 | 624.10] loss=2.15 avg=2.19\n",
      "[2895 | 624.15] loss=1.36 avg=2.18\n",
      "[2896 | 624.21] loss=1.52 avg=2.17\n",
      "[2897 | 624.26] loss=2.60 avg=2.18\n",
      "[2898 | 624.31] loss=1.74 avg=2.17\n",
      "[2899 | 624.37] loss=1.36 avg=2.17\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 43.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "     if (type == 'string' && type == 'char' && type == 'double'))) {\n",
      "          if (key != 'string' && key == 'double' && char (type = key[2].length) != 0) {\n",
      "                             switch (!(type == 'string' && type == 'char'))) {\n",
      "                               if (type == 'string' && type == 'double' && char (type = type[1].length) != 0) {\n",
      "                                     return false;\n",
      "                                    } else {\n",
      "                                  else {\n",
      "                                         if (type == 'string' && type == 'double' && char (type = type[2].length) != 0) {\n",
      "                                              return false;\n",
      "                                      return false;\n",
      "                                     break;\n",
      "                            } */\n",
      "                          \n",
      "                         }\n",
      "           }\n",
      "       \n",
      "  }\n",
      "    \n",
      "   {\n",
      "                              }\n",
      "      {\n",
      "                               }\n",
      "                           \n",
      "                          \n",
      "             \n",
      "                    \n",
      "                  \n",
      "                }\n",
      "         \n",
      "        \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "       \n",
      "      \n",
      "      \n",
      "       \n",
      "      \n",
      " }\n",
      "       \n",
      "     \n",
      "     \n",
      "     \n",
      "  }\n",
      "    \n",
      "   \n",
      "   \n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "}\n",
      "   \n",
      "}\n",
      "  \n",
      "  \n",
      "}\n",
      "  \n",
      "  \n",
      "}\n",
      "   \n",
      "  \n",
      "   (\n",
      "\n",
      "  \n",
      "  \n",
      "   \n",
      "  )\n",
      "  \n",
      "  )))\n",
      "\n",
      "   \n",
      "\n",
      " )\n",
      "\n",
      "\n",
      "\n",
      "// \n",
      "\n",
      " \n",
      "\n",
      " #ifdef SPARK4 \n",
      "\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2900 | 640.43] validation loss = 2.27\n",
      "[2900 | 640.50] loss=3.14 avg=2.18\n",
      "[2901 | 640.55] loss=1.68 avg=2.17\n",
      "[2902 | 640.61] loss=1.99 avg=2.17\n",
      "[2903 | 640.66] loss=1.70 avg=2.16\n",
      "[2904 | 640.71] loss=1.60 avg=2.16\n",
      "[2905 | 640.77] loss=3.13 avg=2.17\n",
      "[2906 | 640.82] loss=2.36 avg=2.17\n",
      "[2907 | 640.88] loss=1.70 avg=2.17\n",
      "[2908 | 640.93] loss=1.53 avg=2.16\n",
      "[2909 | 640.99] loss=2.73 avg=2.16\n",
      "[2910 | 641.04] loss=1.57 avg=2.16\n",
      "[2911 | 641.10] loss=3.24 avg=2.17\n",
      "[2912 | 641.16] loss=2.76 avg=2.18\n",
      "[2913 | 641.21] loss=2.85 avg=2.18\n",
      "[2914 | 641.27] loss=1.98 avg=2.18\n",
      "[2915 | 641.32] loss=1.74 avg=2.18\n",
      "[2916 | 641.37] loss=3.32 avg=2.19\n",
      "[2917 | 641.43] loss=2.01 avg=2.19\n",
      "[2918 | 641.48] loss=2.37 avg=2.19\n",
      "[2919 | 641.54] loss=2.62 avg=2.19\n",
      "[2920 | 641.59] loss=2.06 avg=2.19\n",
      "[2921 | 641.64] loss=1.33 avg=2.18\n",
      "[2922 | 641.70] loss=1.93 avg=2.18\n",
      "[2923 | 641.76] loss=3.02 avg=2.19\n",
      "[2924 | 641.81] loss=1.87 avg=2.18\n",
      "[2925 | 641.86] loss=1.71 avg=2.18\n",
      "[2926 | 641.92] loss=2.69 avg=2.18\n",
      "[2927 | 641.97] loss=1.47 avg=2.18\n",
      "[2928 | 642.02] loss=1.40 avg=2.17\n",
      "[2929 | 642.08] loss=1.39 avg=2.16\n",
      "[2930 | 642.13] loss=1.86 avg=2.16\n",
      "[2931 | 642.19] loss=1.45 avg=2.15\n",
      "[2932 | 642.25] loss=1.54 avg=2.15\n",
      "[2933 | 642.30] loss=1.84 avg=2.14\n",
      "[2934 | 642.35] loss=2.33 avg=2.14\n",
      "[2935 | 642.41] loss=2.74 avg=2.15\n",
      "[2936 | 642.46] loss=2.50 avg=2.15\n",
      "[2937 | 642.52] loss=2.18 avg=2.15\n",
      "[2938 | 642.57] loss=1.90 avg=2.15\n",
      "[2939 | 642.63] loss=3.58 avg=2.17\n",
      "[2940 | 642.68] loss=2.74 avg=2.17\n",
      "[2941 | 642.74] loss=2.10 avg=2.17\n",
      "[2942 | 642.79] loss=3.14 avg=2.18\n",
      "[2943 | 642.85] loss=2.40 avg=2.18\n",
      "[2944 | 642.90] loss=1.64 avg=2.18\n",
      "[2945 | 642.95] loss=2.14 avg=2.18\n",
      "[2946 | 643.01] loss=2.62 avg=2.18\n",
      "[2947 | 643.06] loss=1.70 avg=2.18\n",
      "[2948 | 643.11] loss=2.85 avg=2.18\n",
      "[2949 | 643.17] loss=1.97 avg=2.18\n",
      "[2950 | 643.22] loss=2.14 avg=2.18\n",
      "[2951 | 643.27] loss=2.96 avg=2.19\n",
      "[2952 | 643.33] loss=1.73 avg=2.18\n",
      "[2953 | 643.38] loss=1.40 avg=2.18\n",
      "[2954 | 643.43] loss=2.02 avg=2.17\n",
      "[2955 | 643.49] loss=2.37 avg=2.18\n",
      "[2956 | 643.54] loss=2.36 avg=2.18\n",
      "[2957 | 643.60] loss=2.05 avg=2.18\n",
      "[2958 | 643.65] loss=1.93 avg=2.17\n",
      "[2959 | 643.71] loss=2.07 avg=2.17\n",
      "[2960 | 643.76] loss=2.56 avg=2.18\n",
      "[2961 | 643.82] loss=2.40 avg=2.18\n",
      "[2962 | 643.88] loss=2.44 avg=2.18\n",
      "[2963 | 643.93] loss=2.31 avg=2.18\n",
      "[2964 | 643.99] loss=2.19 avg=2.18\n",
      "[2965 | 644.04] loss=1.80 avg=2.18\n",
      "[2966 | 644.09] loss=2.67 avg=2.18\n",
      "[2967 | 644.15] loss=2.11 avg=2.18\n",
      "[2968 | 644.20] loss=2.42 avg=2.19\n",
      "[2969 | 644.26] loss=2.72 avg=2.19\n",
      "[2970 | 644.31] loss=2.62 avg=2.20\n",
      "[2971 | 644.36] loss=2.94 avg=2.20\n",
      "[2972 | 644.42] loss=2.41 avg=2.21\n",
      "[2973 | 644.47] loss=2.05 avg=2.20\n",
      "[2974 | 644.53] loss=1.67 avg=2.20\n",
      "[2975 | 644.58] loss=2.96 avg=2.21\n",
      "[2976 | 644.64] loss=1.72 avg=2.20\n",
      "[2977 | 644.69] loss=1.64 avg=2.20\n",
      "[2978 | 644.75] loss=2.72 avg=2.20\n",
      "[2979 | 644.80] loss=2.15 avg=2.20\n",
      "[2980 | 644.86] loss=3.89 avg=2.22\n",
      "[2981 | 644.91] loss=1.56 avg=2.21\n",
      "[2982 | 644.96] loss=1.70 avg=2.21\n",
      "[2983 | 645.01] loss=0.89 avg=2.19\n",
      "[2984 | 645.07] loss=1.95 avg=2.19\n",
      "[2985 | 645.13] loss=1.43 avg=2.18\n",
      "[2986 | 645.18] loss=2.24 avg=2.18\n",
      "[2987 | 645.24] loss=1.86 avg=2.18\n",
      "[2988 | 645.29] loss=1.83 avg=2.18\n",
      "[2989 | 645.35] loss=2.16 avg=2.18\n",
      "[2990 | 645.40] loss=2.23 avg=2.18\n",
      "[2991 | 645.45] loss=2.10 avg=2.18\n",
      "[2992 | 645.51] loss=2.77 avg=2.18\n",
      "[2993 | 645.56] loss=2.55 avg=2.19\n",
      "[2994 | 645.62] loss=1.48 avg=2.18\n",
      "[2995 | 645.67] loss=2.19 avg=2.18\n",
      "[2996 | 645.72] loss=1.74 avg=2.17\n",
      "[2997 | 645.78] loss=1.52 avg=2.17\n",
      "[2998 | 645.83] loss=1.21 avg=2.16\n",
      "[2999 | 645.88] loss=1.21 avg=2.15\n",
      "Saving checkpoint/m1_vulnerability/model-3000\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " given a value to that value to match a condition for which there is no error on the result);\n",
      "\n",
      "\n",
      "            \n",
      "\n",
      "         \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "{\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      " }\n",
      "\n",
      "\n",
      "}\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " }\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "   }\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  }\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  }\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "      \n",
      "  \n",
      "      \n",
      "\n",
      "     \n",
      "      \n",
      "      \n",
      "\n",
      "       \n",
      "       \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "       \n",
      "      \n",
      "\n",
      "     \n",
      "       \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "\n",
      "     \n",
      "     \n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "      \n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000 | 667.42] validation loss = 2.26\n",
      "[3000 | 667.48] loss=2.36 avg=2.15\n",
      "[3001 | 667.53] loss=2.51 avg=2.15\n",
      "[3002 | 667.59] loss=2.26 avg=2.16\n",
      "[3003 | 667.64] loss=1.30 avg=2.15\n",
      "[3004 | 667.70] loss=1.15 avg=2.14\n",
      "[3005 | 667.75] loss=1.37 avg=2.13\n",
      "[3006 | 667.81] loss=1.19 avg=2.12\n",
      "[3007 | 667.86] loss=2.43 avg=2.12\n",
      "[3008 | 667.92] loss=2.65 avg=2.13\n",
      "[3009 | 667.97] loss=1.19 avg=2.12\n",
      "[3010 | 668.03] loss=1.68 avg=2.11\n",
      "[3011 | 668.08] loss=1.97 avg=2.11\n",
      "[3012 | 668.13] loss=1.01 avg=2.10\n",
      "[3013 | 668.19] loss=2.85 avg=2.11\n",
      "[3014 | 668.24] loss=2.61 avg=2.11\n",
      "[3015 | 668.30] loss=2.02 avg=2.11\n",
      "[3016 | 668.35] loss=2.61 avg=2.12\n",
      "[3017 | 668.41] loss=1.33 avg=2.11\n",
      "[3018 | 668.47] loss=2.58 avg=2.12\n",
      "[3019 | 668.52] loss=2.82 avg=2.12\n",
      "[3020 | 668.57] loss=1.70 avg=2.12\n",
      "[3021 | 668.63] loss=2.99 avg=2.13\n",
      "[3022 | 668.68] loss=1.79 avg=2.12\n",
      "[3023 | 668.74] loss=2.42 avg=2.13\n",
      "[3024 | 668.79] loss=2.44 avg=2.13\n",
      "[3025 | 668.85] loss=2.36 avg=2.13\n",
      "[3026 | 668.90] loss=1.90 avg=2.13\n",
      "[3027 | 668.95] loss=2.15 avg=2.13\n",
      "[3028 | 669.01] loss=1.45 avg=2.12\n",
      "[3029 | 669.06] loss=2.42 avg=2.13\n",
      "[3030 | 669.12] loss=1.75 avg=2.12\n",
      "[3031 | 669.17] loss=0.87 avg=2.11\n",
      "[3032 | 669.23] loss=2.99 avg=2.12\n",
      "[3033 | 669.28] loss=1.40 avg=2.11\n",
      "[3034 | 669.34] loss=1.88 avg=2.11\n",
      "[3035 | 669.39] loss=2.09 avg=2.11\n",
      "[3036 | 669.44] loss=1.40 avg=2.10\n",
      "[3037 | 669.49] loss=1.38 avg=2.09\n",
      "[3038 | 669.55] loss=0.75 avg=2.08\n",
      "[3039 | 669.60] loss=1.07 avg=2.07\n",
      "[3040 | 669.65] loss=3.13 avg=2.08\n",
      "[3041 | 669.71] loss=3.36 avg=2.09\n",
      "[3042 | 669.76] loss=1.25 avg=2.09\n",
      "[3043 | 669.82] loss=1.35 avg=2.08\n",
      "[3044 | 669.87] loss=3.02 avg=2.09\n",
      "[3045 | 669.93] loss=3.15 avg=2.10\n",
      "[3046 | 669.98] loss=3.24 avg=2.11\n",
      "[3047 | 670.04] loss=1.85 avg=2.11\n",
      "[3048 | 670.10] loss=2.19 avg=2.11\n",
      "[3049 | 670.15] loss=1.81 avg=2.11\n",
      "[3050 | 670.20] loss=1.71 avg=2.10\n",
      "[3051 | 670.26] loss=1.33 avg=2.09\n",
      "[3052 | 670.31] loss=2.24 avg=2.10\n",
      "[3053 | 670.37] loss=0.84 avg=2.08\n",
      "[3054 | 670.42] loss=2.23 avg=2.08\n",
      "[3055 | 670.48] loss=3.02 avg=2.09\n",
      "[3056 | 670.53] loss=1.08 avg=2.08\n",
      "[3057 | 670.59] loss=2.61 avg=2.09\n",
      "[3058 | 670.64] loss=1.20 avg=2.08\n",
      "[3059 | 670.69] loss=3.95 avg=2.10\n",
      "[3060 | 670.75] loss=2.45 avg=2.10\n",
      "[3061 | 670.81] loss=2.89 avg=2.11\n",
      "[3062 | 670.86] loss=2.59 avg=2.11\n",
      "[3063 | 670.91] loss=1.99 avg=2.11\n",
      "[3064 | 670.97] loss=2.49 avg=2.12\n",
      "[3065 | 671.03] loss=2.33 avg=2.12\n",
      "[3066 | 671.08] loss=1.90 avg=2.12\n",
      "[3067 | 671.14] loss=2.42 avg=2.12\n",
      "[3068 | 671.19] loss=1.60 avg=2.11\n",
      "[3069 | 671.24] loss=1.80 avg=2.11\n",
      "[3070 | 671.30] loss=1.86 avg=2.11\n",
      "[3071 | 671.35] loss=0.91 avg=2.10\n",
      "[3072 | 671.41] loss=3.30 avg=2.11\n",
      "[3073 | 671.46] loss=2.37 avg=2.11\n",
      "[3074 | 671.51] loss=2.53 avg=2.12\n",
      "[3075 | 671.57] loss=1.87 avg=2.11\n",
      "[3076 | 671.63] loss=1.51 avg=2.11\n",
      "[3077 | 671.68] loss=1.55 avg=2.10\n",
      "[3078 | 671.73] loss=2.77 avg=2.11\n",
      "[3079 | 671.79] loss=1.81 avg=2.11\n",
      "[3080 | 671.84] loss=2.70 avg=2.11\n",
      "[3081 | 671.90] loss=1.47 avg=2.11\n",
      "[3082 | 671.95] loss=2.90 avg=2.11\n",
      "[3083 | 672.01] loss=2.23 avg=2.11\n",
      "[3084 | 672.06] loss=2.50 avg=2.12\n",
      "[3085 | 672.12] loss=2.34 avg=2.12\n",
      "[3086 | 672.17] loss=2.20 avg=2.12\n",
      "[3087 | 672.22] loss=2.66 avg=2.13\n",
      "[3088 | 672.28] loss=1.76 avg=2.12\n",
      "[3089 | 672.33] loss=0.85 avg=2.11\n",
      "[3090 | 672.38] loss=1.37 avg=2.10\n",
      "[3091 | 672.43] loss=2.37 avg=2.11\n",
      "[3092 | 672.49] loss=2.28 avg=2.11\n",
      "[3093 | 672.54] loss=1.96 avg=2.11\n",
      "[3094 | 672.59] loss=2.49 avg=2.11\n",
      "[3095 | 672.65] loss=2.77 avg=2.12\n",
      "[3096 | 672.70] loss=2.06 avg=2.12\n",
      "[3097 | 672.75] loss=1.35 avg=2.11\n",
      "[3098 | 672.80] loss=3.08 avg=2.12\n",
      "[3099 | 672.86] loss=2.87 avg=2.13\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " EVENT;\n",
      "}\n",
      "}\n",
      "if (pState->pState_has(PStates))\n",
      "{\n",
      "if (pSTATE->pState_is(pStates))\n",
      "pStates = state;\n",
      "}\n",
      "/*\n",
      "* pStates = setState(&state); */\n",
      "/*\n",
      "* pStates = (setState(&this->pState);\n",
      "/* pStates = setState(&this->state);\n",
      "/*\n",
      "* if (pSTATE && iax->state)\n",
      "if (if (pState->state_is(getState &state)) && iax->state_in( getState | state, pStates, iax->state_last[i].i - (getState | currentState, 0))))\n",
      "{\n",
      "int i;\n",
      "for (i = 0; i < iax->state_size; i++)\n",
      "{\n",
      "if (getState &state, i) iax->state_last[i] = currentState->getData( iax->currentState[i].i - iax->state_current[i].i);\n",
      "}\n",
      "if (!(->getState &state, iax) || (->getState &state, iax ? iax->state_last[i].i : iax->state_state[i].i))\n",
      " iax->state_last[i] = currentState->getData((, iax)))->getData( iax->currentState[i].p)->get->map(getState & iax->model->name, iax)->get->setName(newState & iax->state_name, iax);\n",
      "}\n",
      "/*\n",
      "* pStates = setState(&state); */\n",
      "/* iax = iax->id; iax->state_last[i] = prev->getData( iax->state_last[i].p, iax);\n",
      "}\n",
      "/*\n",
      "* pStates = setState(&state); */\n",
      "/*\n",
      "* if (pState && *(->getState &state) && iax->state_is(pState->pState_next[i]) && iax->id != getState))\n",
      "->getState (getState, iax)); */\n",
      "/*\n",
      "* pStates = (setState(&state); */\n",
      "pStates = iax_get_get_tos;\n",
      "/* iax = iax->id; iax->state_last[i] = prev->getData( iax->state_last[i].p, iax); iax->state_last[i] = !_pState->pState_next[i]);\n",
      " iax->id = getState & iax->id; iax->state[i] = id;\n",
      " iax->state_last[i] = iax->state_n;\n",
      " iax->state_state = iax->state[i];\n",
      " iax->state_state *= iax->state_id;\n",
      " iax->state_state->data = iax->state->data;\n",
      " iax->state = iax->state;\n",
      " iax->state = iax->state;\n",
      " iax->state = iax->state;\n",
      " iax = 0; iax  = iax->state;\n",
      " iax = 0; iax  = iax->state;\n",
      " iax_setState(State, iax)->putState(pStates);\n",
      " iax_setState(State, iax)->putState(_currentState, iax, iax->state[i]);\n",
      " iax_setState(State, iax)->putState(_currentState, iax, iax->state[i].p);\n",
      " iax = iax->state;\n",
      " iax =\"\" =\"; iax =\"; iax  = iax->state;\n",
      " iax ='  =\"; iax ->state.data = iax->state? 0; iax->state[0] = iax->state_id;\n",
      " iax ->state = iax->state; iax  = iax->state; iax  = iax->state; iax  = iax->state; iax  = iax->state;\n",
      " iax ='  =\" =\" =\" =\"\"        \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3100 | 688.71] validation loss = 2.26\n",
      "[3100 | 688.77] loss=1.91 avg=2.12\n",
      "[3101 | 688.82] loss=2.62 avg=2.13\n",
      "[3102 | 688.88] loss=1.56 avg=2.12\n",
      "[3103 | 688.93] loss=1.92 avg=2.12\n",
      "[3104 | 688.99] loss=2.55 avg=2.12\n",
      "[3105 | 689.04] loss=1.69 avg=2.12\n",
      "[3106 | 689.09] loss=1.49 avg=2.11\n",
      "[3107 | 689.15] loss=1.98 avg=2.11\n",
      "[3108 | 689.20] loss=2.43 avg=2.12\n",
      "[3109 | 689.25] loss=2.30 avg=2.12\n",
      "[3110 | 689.31] loss=0.69 avg=2.10\n",
      "[3111 | 689.36] loss=2.56 avg=2.11\n",
      "[3112 | 689.42] loss=2.38 avg=2.11\n",
      "[3113 | 689.47] loss=0.83 avg=2.10\n",
      "[3114 | 689.52] loss=0.97 avg=2.09\n",
      "[3115 | 689.57] loss=3.39 avg=2.10\n",
      "[3116 | 689.63] loss=2.92 avg=2.11\n",
      "[3117 | 689.68] loss=3.26 avg=2.12\n",
      "[3118 | 689.74] loss=2.15 avg=2.12\n",
      "[3119 | 689.79] loss=1.50 avg=2.11\n",
      "[3120 | 689.84] loss=2.93 avg=2.12\n",
      "[3121 | 689.90] loss=1.55 avg=2.12\n",
      "[3122 | 689.95] loss=3.28 avg=2.13\n",
      "[3123 | 690.00] loss=3.05 avg=2.14\n",
      "[3124 | 690.06] loss=2.15 avg=2.14\n",
      "[3125 | 690.11] loss=2.75 avg=2.14\n",
      "[3126 | 690.17] loss=2.80 avg=2.15\n",
      "[3127 | 690.23] loss=3.17 avg=2.16\n",
      "[3128 | 690.28] loss=1.51 avg=2.15\n",
      "[3129 | 690.34] loss=1.68 avg=2.15\n",
      "[3130 | 690.39] loss=1.88 avg=2.15\n",
      "[3131 | 690.45] loss=2.79 avg=2.15\n",
      "[3132 | 690.50] loss=0.93 avg=2.14\n",
      "[3133 | 690.56] loss=2.34 avg=2.14\n",
      "[3134 | 690.61] loss=1.72 avg=2.14\n",
      "[3135 | 690.67] loss=1.70 avg=2.13\n",
      "[3136 | 690.72] loss=1.39 avg=2.13\n",
      "[3137 | 690.77] loss=2.91 avg=2.13\n",
      "[3138 | 690.83] loss=2.81 avg=2.14\n",
      "[3139 | 690.88] loss=2.23 avg=2.14\n",
      "[3140 | 690.94] loss=2.80 avg=2.15\n",
      "[3141 | 690.99] loss=2.93 avg=2.16\n",
      "[3142 | 691.04] loss=2.92 avg=2.16\n",
      "[3143 | 691.10] loss=2.98 avg=2.17\n",
      "[3144 | 691.16] loss=2.48 avg=2.18\n",
      "[3145 | 691.21] loss=2.92 avg=2.18\n",
      "[3146 | 691.26] loss=3.52 avg=2.20\n",
      "[3147 | 691.32] loss=1.80 avg=2.19\n",
      "[3148 | 691.37] loss=2.77 avg=2.20\n",
      "[3149 | 691.42] loss=1.61 avg=2.19\n",
      "[3150 | 691.48] loss=2.44 avg=2.19\n",
      "[3151 | 691.53] loss=1.83 avg=2.19\n",
      "[3152 | 691.59] loss=2.07 avg=2.19\n",
      "[3153 | 691.65] loss=1.86 avg=2.19\n",
      "[3154 | 691.70] loss=2.83 avg=2.19\n",
      "[3155 | 691.76] loss=1.43 avg=2.18\n",
      "[3156 | 691.81] loss=1.98 avg=2.18\n",
      "[3157 | 691.87] loss=1.17 avg=2.17\n",
      "[3158 | 691.93] loss=2.18 avg=2.17\n",
      "[3159 | 691.98] loss=2.86 avg=2.18\n",
      "[3160 | 692.03] loss=1.66 avg=2.17\n",
      "[3161 | 692.09] loss=1.20 avg=2.16\n",
      "[3162 | 692.14] loss=1.60 avg=2.16\n",
      "[3163 | 692.19] loss=2.93 avg=2.17\n",
      "[3164 | 692.25] loss=2.43 avg=2.17\n",
      "[3165 | 692.30] loss=2.53 avg=2.17\n",
      "[3166 | 692.36] loss=2.62 avg=2.18\n",
      "[3167 | 692.41] loss=3.52 avg=2.19\n",
      "[3168 | 692.47] loss=0.83 avg=2.18\n",
      "[3169 | 692.53] loss=1.53 avg=2.17\n",
      "[3170 | 692.58] loss=1.19 avg=2.16\n",
      "[3171 | 692.64] loss=2.33 avg=2.16\n",
      "[3172 | 692.70] loss=1.58 avg=2.16\n",
      "[3173 | 692.75] loss=1.28 avg=2.15\n",
      "[3174 | 692.81] loss=2.86 avg=2.16\n",
      "[3175 | 692.86] loss=1.20 avg=2.15\n",
      "[3176 | 692.92] loss=2.11 avg=2.15\n",
      "[3177 | 692.98] loss=1.84 avg=2.14\n",
      "[3178 | 693.03] loss=2.11 avg=2.14\n",
      "[3179 | 693.09] loss=3.79 avg=2.16\n",
      "[3180 | 693.14] loss=2.70 avg=2.16\n",
      "[3181 | 693.19] loss=2.95 avg=2.17\n",
      "[3182 | 693.25] loss=1.71 avg=2.17\n",
      "[3183 | 693.30] loss=3.23 avg=2.18\n",
      "[3184 | 693.36] loss=2.50 avg=2.18\n",
      "[3185 | 693.41] loss=1.33 avg=2.17\n",
      "[3186 | 693.46] loss=2.69 avg=2.18\n",
      "[3187 | 693.52] loss=1.94 avg=2.18\n",
      "[3188 | 693.57] loss=1.46 avg=2.17\n",
      "[3189 | 693.63] loss=1.98 avg=2.17\n",
      "[3190 | 693.69] loss=2.32 avg=2.17\n",
      "[3191 | 693.74] loss=1.96 avg=2.17\n",
      "[3192 | 693.79] loss=2.60 avg=2.17\n",
      "[3193 | 693.85] loss=4.09 avg=2.19\n",
      "[3194 | 693.90] loss=1.85 avg=2.19\n",
      "[3195 | 693.96] loss=2.59 avg=2.19\n",
      "[3196 | 694.01] loss=2.77 avg=2.20\n",
      "[3197 | 694.06] loss=3.10 avg=2.20\n",
      "[3198 | 694.11] loss=2.62 avg=2.21\n",
      "[3199 | 694.17] loss=0.84 avg=2.19\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)--)----)--)--)--)--)--)--)--)--)--)----)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)----)----)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--------)----)------------------------ ---------------------------- \t\t\t\t\t\t\t\t \t的 \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)--)--)--)------)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--)--------------                                                                                                                                                                                                                                                                                                                                                                                                               }\n",
      "                                                                                                                   \n",
      "                                                                                                            )\n",
      "                                                       }  \n",
      "                                                //\t\t\t\t\t\t\t\t\t\t\t\t\t\t /* /* /* /* */ /* */ /* /* */ */ /* */ /* */ */ /* */ */ /* */ /* */ */ */ /* */ */ /* */ */ /* */ */ }\n",
      "   \n",
      "  public void               \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3200 | 710.12] validation loss = 2.26\n",
      "[3200 | 710.18] loss=0.66 avg=2.18\n",
      "[3201 | 710.24] loss=1.77 avg=2.18\n",
      "[3202 | 710.29] loss=2.41 avg=2.18\n",
      "[3203 | 710.34] loss=2.20 avg=2.18\n",
      "[3204 | 710.39] loss=3.14 avg=2.19\n",
      "[3205 | 710.45] loss=1.46 avg=2.18\n",
      "[3206 | 710.50] loss=2.88 avg=2.19\n",
      "[3207 | 710.55] loss=2.78 avg=2.19\n",
      "[3208 | 710.61] loss=1.38 avg=2.19\n",
      "[3209 | 710.66] loss=1.51 avg=2.18\n",
      "[3210 | 710.72] loss=1.20 avg=2.17\n",
      "[3211 | 710.77] loss=3.44 avg=2.18\n",
      "[3212 | 710.83] loss=2.68 avg=2.19\n",
      "[3213 | 710.88] loss=3.05 avg=2.19\n",
      "[3214 | 710.94] loss=2.00 avg=2.19\n",
      "[3215 | 710.99] loss=1.97 avg=2.19\n",
      "[3216 | 711.05] loss=1.62 avg=2.18\n",
      "[3217 | 711.11] loss=1.66 avg=2.18\n",
      "[3218 | 711.17] loss=2.32 avg=2.18\n",
      "[3219 | 711.22] loss=2.03 avg=2.18\n",
      "[3220 | 711.28] loss=2.64 avg=2.18\n",
      "[3221 | 711.33] loss=1.80 avg=2.18\n",
      "[3222 | 711.39] loss=3.48 avg=2.19\n",
      "[3223 | 711.44] loss=1.91 avg=2.19\n",
      "[3224 | 711.50] loss=2.69 avg=2.20\n",
      "[3225 | 711.55] loss=2.69 avg=2.20\n",
      "[3226 | 711.61] loss=2.62 avg=2.20\n",
      "[3227 | 711.66] loss=2.17 avg=2.20\n",
      "[3228 | 711.71] loss=1.53 avg=2.20\n",
      "[3229 | 711.77] loss=1.83 avg=2.19\n",
      "[3230 | 711.82] loss=2.23 avg=2.19\n",
      "[3231 | 711.88] loss=2.77 avg=2.20\n",
      "[3232 | 711.93] loss=1.66 avg=2.19\n",
      "[3233 | 711.99] loss=0.60 avg=2.18\n",
      "[3234 | 712.04] loss=2.81 avg=2.18\n",
      "[3235 | 712.09] loss=1.22 avg=2.18\n",
      "[3236 | 712.15] loss=1.78 avg=2.17\n",
      "[3237 | 712.20] loss=0.72 avg=2.16\n",
      "[3238 | 712.25] loss=2.60 avg=2.16\n",
      "[3239 | 712.31] loss=2.27 avg=2.16\n",
      "[3240 | 712.36] loss=2.51 avg=2.17\n",
      "[3241 | 712.41] loss=2.83 avg=2.17\n",
      "[3242 | 712.47] loss=1.22 avg=2.16\n",
      "[3243 | 712.52] loss=2.53 avg=2.17\n",
      "[3244 | 712.57] loss=1.82 avg=2.16\n",
      "[3245 | 712.63] loss=1.39 avg=2.16\n",
      "[3246 | 712.68] loss=2.21 avg=2.16\n",
      "[3247 | 712.73] loss=2.53 avg=2.16\n",
      "[3248 | 712.79] loss=1.04 avg=2.15\n",
      "[3249 | 712.84] loss=1.54 avg=2.14\n",
      "[3250 | 712.89] loss=1.40 avg=2.13\n",
      "[3251 | 712.95] loss=2.30 avg=2.14\n",
      "[3252 | 713.00] loss=1.89 avg=2.13\n",
      "[3253 | 713.06] loss=2.59 avg=2.14\n",
      "[3254 | 713.12] loss=1.40 avg=2.13\n",
      "[3255 | 713.18] loss=2.01 avg=2.13\n",
      "[3256 | 713.24] loss=1.78 avg=2.13\n",
      "[3257 | 713.29] loss=3.24 avg=2.14\n",
      "[3258 | 713.35] loss=0.96 avg=2.13\n",
      "[3259 | 713.40] loss=2.03 avg=2.12\n",
      "[3260 | 713.46] loss=2.83 avg=2.13\n",
      "[3261 | 713.52] loss=1.58 avg=2.13\n",
      "[3262 | 713.57] loss=2.12 avg=2.13\n",
      "[3263 | 713.63] loss=2.02 avg=2.13\n",
      "[3264 | 713.68] loss=1.86 avg=2.12\n",
      "[3265 | 713.74] loss=2.15 avg=2.12\n",
      "[3266 | 713.79] loss=2.08 avg=2.12\n",
      "[3267 | 713.85] loss=1.74 avg=2.12\n",
      "[3268 | 713.90] loss=1.01 avg=2.11\n",
      "[3269 | 713.96] loss=2.25 avg=2.11\n",
      "[3270 | 714.01] loss=2.11 avg=2.11\n",
      "[3271 | 714.07] loss=1.01 avg=2.10\n",
      "[3272 | 714.12] loss=1.68 avg=2.09\n",
      "[3273 | 714.18] loss=1.27 avg=2.09\n",
      "[3274 | 714.23] loss=1.33 avg=2.08\n",
      "[3275 | 714.28] loss=0.72 avg=2.06\n",
      "[3276 | 714.34] loss=1.44 avg=2.06\n",
      "[3277 | 714.40] loss=1.45 avg=2.05\n",
      "[3278 | 714.45] loss=2.06 avg=2.05\n",
      "[3279 | 714.50] loss=3.25 avg=2.06\n",
      "[3280 | 714.56] loss=1.12 avg=2.05\n",
      "[3281 | 714.61] loss=2.27 avg=2.06\n",
      "[3282 | 714.66] loss=2.80 avg=2.06\n",
      "[3283 | 714.72] loss=1.59 avg=2.06\n",
      "[3284 | 714.78] loss=2.37 avg=2.06\n",
      "[3285 | 714.83] loss=2.45 avg=2.07\n",
      "[3286 | 714.88] loss=2.50 avg=2.07\n",
      "[3287 | 714.94] loss=2.59 avg=2.08\n",
      "[3288 | 714.99] loss=1.22 avg=2.07\n",
      "[3289 | 715.04] loss=2.81 avg=2.08\n",
      "[3290 | 715.10] loss=1.45 avg=2.07\n",
      "[3291 | 715.15] loss=2.55 avg=2.07\n",
      "[3292 | 715.20] loss=1.08 avg=2.06\n",
      "[3293 | 715.26] loss=1.99 avg=2.06\n",
      "[3294 | 715.31] loss=3.02 avg=2.07\n",
      "[3295 | 715.37] loss=1.80 avg=2.07\n",
      "[3296 | 715.43] loss=1.64 avg=2.07\n",
      "[3297 | 715.48] loss=1.91 avg=2.06\n",
      "[3298 | 715.53] loss=2.86 avg=2.07\n",
      "[3299 | 715.59] loss=2.71 avg=2.08\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " %\n",
      "       \n",
      "    \n",
      "      getString().str());\n",
      "    \n",
      "   \n",
      "    gsType(string[5]);\n",
      "   \n",
      "   \n",
      "    int i = 0;\n",
      "   \n",
      "    for (int j = 0; j < (gsType.length()); j++) {\n",
      "       s = string[i];\n",
      "    \n",
      "      int j, j, c, st, l;\n",
      "     int pj;\n",
      "      s[0];\n",
      "     int b, c, st, l;\n",
      "    \n",
      "    s[1];\n",
      "     b = st;\n",
      "    \n",
      "    if (pj != pj)\n",
      "       s[0] = '#';\n",
      "        p_ = st;\n",
      "    \n",
      "    if (rvalue.empty())\n",
      "      else\n",
      "         p_ = {\n",
      "                int_ = '#';\n",
      "              double_ = '#';\n",
      "       } else\n",
      "          break;\n",
      "    } return t;\n",
      "   ;\n",
      "   b = c;\n",
      "    return s;\n",
      "   }\n",
      "   int       m_;\n",
      "    m_ = d;\n",
      "  \n",
      "    m_ = f;\n",
      "   \n",
      "   setString(m_);\n",
      "  \n",
      "   if (s == \"\\\")\n",
      "      setString(s);\n",
      "  \n",
      "   if (i == 1)     else\n",
      "       m->ifValue =\n",
      "          if (i <= 5)        t = s[i];\n",
      "    m_ = p;\n",
      "    setString(t, m_);\n",
      "  \n",
      "   if (!(strstrn(t, t, i, -1)))\n",
      "      return -1;\n",
      "  \n",
      "   if (n == 0)\n",
      "      if (n)\n",
      "       if (strstrn(str, t, i, -1)) {\n",
      "          d = strstrncnt;\n",
      "   \n",
      "        int_ = '#';\n",
      "          return -1;\n",
      "    \n",
      "      if (h != 3) {\n",
      "          int_ = '#';\n",
      "          f = n;\n",
      "          if (h != 3)         setString(h, f, NULL);\n",
      "     }\n",
      "       if (strstrn(str, t, i, -1)))        ret = s[i];\n",
      "         m->ifValue =\n",
      "          if (m !=  s[i]) {\n",
      "          if (m == 7)\n",
      "            ret = s[i] ? p;\n",
      "           ret;\n",
      "           ret = t;\n",
      "             return -1;\n",
      "      }\n",
      "      \n",
      "      if (if ==  c)\n",
      "           if (strstrn(str, t, i, -1)))         ret = s[i];\n",
      "      \n",
      "     j = l;\n",
      "     \n",
      "    \n",
      "     if (strstrn(str, t, i, -1)))      \n",
      "     if (if ==  t) {\n",
      "          \n",
      "      }\n",
      "   \n",
      "\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3300 | 731.56] validation loss = 2.25\n",
      "[3300 | 731.62] loss=2.73 avg=2.08\n",
      "[3301 | 731.67] loss=2.38 avg=2.09\n",
      "[3302 | 731.73] loss=3.10 avg=2.10\n",
      "[3303 | 731.78] loss=2.86 avg=2.11\n",
      "[3304 | 731.83] loss=2.22 avg=2.11\n",
      "[3305 | 731.89] loss=1.49 avg=2.10\n",
      "[3306 | 731.95] loss=0.74 avg=2.09\n",
      "[3307 | 732.00] loss=1.99 avg=2.09\n",
      "[3308 | 732.06] loss=1.83 avg=2.08\n",
      "[3309 | 732.11] loss=2.91 avg=2.09\n",
      "[3310 | 732.16] loss=2.59 avg=2.10\n",
      "[3311 | 732.22] loss=2.33 avg=2.10\n",
      "[3312 | 732.28] loss=2.22 avg=2.10\n",
      "[3313 | 732.33] loss=2.71 avg=2.11\n",
      "[3314 | 732.38] loss=2.83 avg=2.11\n",
      "[3315 | 732.44] loss=0.64 avg=2.10\n",
      "[3316 | 732.50] loss=2.08 avg=2.10\n",
      "[3317 | 732.56] loss=1.62 avg=2.09\n",
      "[3318 | 732.61] loss=1.80 avg=2.09\n",
      "[3319 | 732.67] loss=2.18 avg=2.09\n",
      "[3320 | 732.72] loss=2.18 avg=2.09\n",
      "[3321 | 732.78] loss=2.71 avg=2.10\n",
      "[3322 | 732.83] loss=2.51 avg=2.10\n",
      "[3323 | 732.88] loss=2.01 avg=2.10\n",
      "[3324 | 732.94] loss=3.04 avg=2.11\n",
      "[3325 | 733.00] loss=3.24 avg=2.12\n",
      "[3326 | 733.05] loss=2.54 avg=2.13\n",
      "[3327 | 733.11] loss=1.25 avg=2.12\n",
      "[3328 | 733.16] loss=2.25 avg=2.12\n",
      "[3329 | 733.21] loss=2.26 avg=2.12\n",
      "[3330 | 733.27] loss=2.69 avg=2.13\n",
      "[3331 | 733.33] loss=2.02 avg=2.13\n",
      "[3332 | 733.38] loss=3.15 avg=2.14\n",
      "[3333 | 733.43] loss=2.48 avg=2.14\n",
      "[3334 | 733.49] loss=0.78 avg=2.13\n",
      "[3335 | 733.55] loss=2.06 avg=2.12\n",
      "[3336 | 733.60] loss=1.09 avg=2.11\n",
      "[3337 | 733.66] loss=2.22 avg=2.12\n",
      "[3338 | 733.71] loss=2.94 avg=2.12\n",
      "[3339 | 733.77] loss=2.07 avg=2.12\n",
      "[3340 | 733.82] loss=1.56 avg=2.12\n",
      "[3341 | 733.87] loss=2.44 avg=2.12\n",
      "[3342 | 733.93] loss=2.49 avg=2.12\n",
      "[3343 | 733.98] loss=3.04 avg=2.13\n",
      "[3344 | 734.04] loss=2.41 avg=2.14\n",
      "[3345 | 734.10] loss=1.65 avg=2.13\n",
      "[3346 | 734.15] loss=2.27 avg=2.13\n",
      "[3347 | 734.21] loss=2.76 avg=2.14\n",
      "[3348 | 734.27] loss=2.47 avg=2.14\n",
      "[3349 | 734.32] loss=1.91 avg=2.14\n",
      "[3350 | 734.38] loss=1.41 avg=2.13\n",
      "[3351 | 734.43] loss=2.32 avg=2.14\n",
      "[3352 | 734.49] loss=2.17 avg=2.14\n",
      "[3353 | 734.54] loss=4.26 avg=2.16\n",
      "[3354 | 734.59] loss=3.34 avg=2.17\n",
      "[3355 | 734.64] loss=2.18 avg=2.17\n",
      "[3356 | 734.70] loss=1.33 avg=2.16\n",
      "[3357 | 734.75] loss=2.08 avg=2.16\n",
      "[3358 | 734.81] loss=2.76 avg=2.17\n",
      "[3359 | 734.86] loss=1.64 avg=2.16\n",
      "[3360 | 734.92] loss=2.99 avg=2.17\n",
      "[3361 | 734.97] loss=1.51 avg=2.16\n",
      "[3362 | 735.03] loss=1.03 avg=2.15\n",
      "[3363 | 735.08] loss=2.53 avg=2.15\n",
      "[3364 | 735.14] loss=3.72 avg=2.17\n",
      "[3365 | 735.19] loss=3.04 avg=2.18\n",
      "[3366 | 735.24] loss=1.64 avg=2.17\n",
      "[3367 | 735.29] loss=1.41 avg=2.17\n",
      "[3368 | 735.35] loss=1.77 avg=2.16\n",
      "[3369 | 735.40] loss=0.93 avg=2.15\n",
      "[3370 | 735.45] loss=2.84 avg=2.16\n",
      "[3371 | 735.50] loss=1.04 avg=2.15\n",
      "[3372 | 735.56] loss=2.44 avg=2.15\n",
      "[3373 | 735.62] loss=1.49 avg=2.14\n",
      "[3374 | 735.67] loss=3.00 avg=2.15\n",
      "[3375 | 735.72] loss=2.47 avg=2.15\n",
      "[3376 | 735.78] loss=1.49 avg=2.15\n",
      "[3377 | 735.83] loss=1.80 avg=2.14\n",
      "[3378 | 735.89] loss=2.58 avg=2.15\n",
      "[3379 | 735.94] loss=2.90 avg=2.16\n",
      "[3380 | 736.00] loss=1.96 avg=2.15\n",
      "[3381 | 736.05] loss=2.41 avg=2.16\n",
      "[3382 | 736.11] loss=2.00 avg=2.15\n",
      "[3383 | 736.16] loss=1.94 avg=2.15\n",
      "[3384 | 736.22] loss=2.47 avg=2.16\n",
      "[3385 | 736.27] loss=1.64 avg=2.15\n",
      "[3386 | 736.32] loss=1.46 avg=2.14\n",
      "[3387 | 736.38] loss=2.04 avg=2.14\n",
      "[3388 | 736.43] loss=3.00 avg=2.15\n",
      "[3389 | 736.48] loss=1.99 avg=2.15\n",
      "[3390 | 736.54] loss=1.94 avg=2.15\n",
      "[3391 | 736.59] loss=1.81 avg=2.14\n",
      "[3392 | 736.64] loss=2.02 avg=2.14\n",
      "[3393 | 736.70] loss=2.37 avg=2.14\n",
      "[3394 | 736.75] loss=3.11 avg=2.15\n",
      "[3395 | 736.81] loss=2.41 avg=2.16\n",
      "[3396 | 736.86] loss=2.10 avg=2.16\n",
      "[3397 | 736.91] loss=2.72 avg=2.16\n",
      "[3398 | 736.97] loss=2.21 avg=2.16\n",
      "[3399 | 737.02] loss=1.70 avg=2.16\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "        int retries = 0;\n",
      "      for (artorto & tv : tv.getArtortoCount()) {\n",
      "      if (!(retries > 1)) {\n",
      "       else\n",
      "        \n",
      "         if ((artorto[artorto.length]) >      retries) {\n",
      "         \n",
      "        \n",
      "         } else {\n",
      "         \n",
      "         }\n",
      "         retries = 1   \n",
      "        if (!(retries > 1))) {\n",
      "        \n",
      "   }\n",
      "        retries = 0  \n",
      "         if (!(retries > 0)))) {\n",
      "        \n",
      "     }else if ((artorto[artorto.length]) >    retries) {\n",
      "       \n",
      "       \n",
      "\n",
      "        \n",
      "       \n",
      "      \n",
      "       retries = 1  \n",
      "      \n",
      "      \n",
      "\n",
      "       if (retries >     }      \n",
      "      \n",
      "        while ( retries < 1)))) {\n",
      "      \n",
      "     \n",
      "     \n",
      "      if (exists.count) {\n",
      "        retries = Exists[1];\n",
      "     \n",
      "     \n",
      "     \n",
      "      if ((valm == 1) || (!retries > 0)) {\n",
      "      \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     retries = 1 \n",
      "     \n",
      "    \n",
      "   \n",
      "     retries = 0\n",
      "      else\n",
      "      \n",
      "\n",
      "      \n",
      "    \n",
      "     if ((valm == 1) || (!retries > 0)) {\n",
      "       \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "      }else {\n",
      "      \n",
      "       \n",
      "      \n",
      "       retries = 1\n",
      "       retries = 2 \n",
      "      } else {\n",
      "      \n",
      "     \n",
      "      retries = 1\n",
      "      return    ;\n",
      "     }\n",
      "      \n",
      "     if (retries >   retries) {\n",
      "      \n",
      "      \n",
      "       retries = 1 \n",
      "     \n",
      "      \n",
      "       retries = 0\n",
      "      }\n",
      "       }\n",
      "     \n",
      "     \n",
      "       break;\n",
      "   }\n",
      "    \n",
      "   {\n",
      "     if ((valm == 1) || (!retries > 0))\n",
      "    \n",
      "   \n",
      "    if ((valm == 2) || (!retries > 0))\n",
      "     \n",
      "     {\n",
      "      \n",
      "     \n",
      "       retries = 1 \n",
      "     \n",
      "    \n",
      "       retries = 2 \n",
      "    \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "      retries = 1\n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "      if (valm ==  Retries) {\n",
      "       \n",
      " \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3400 | 752.99] validation loss = 2.25\n",
      "[3400 | 753.05] loss=1.88 avg=2.15\n",
      "[3401 | 753.10] loss=2.36 avg=2.16\n",
      "[3402 | 753.16] loss=2.44 avg=2.16\n",
      "[3403 | 753.21] loss=1.78 avg=2.16\n",
      "[3404 | 753.27] loss=2.36 avg=2.16\n",
      "[3405 | 753.32] loss=1.18 avg=2.15\n",
      "[3406 | 753.37] loss=1.66 avg=2.14\n",
      "[3407 | 753.43] loss=2.43 avg=2.15\n",
      "[3408 | 753.48] loss=1.86 avg=2.14\n",
      "[3409 | 753.54] loss=2.70 avg=2.15\n",
      "[3410 | 753.59] loss=2.99 avg=2.16\n",
      "[3411 | 753.65] loss=2.76 avg=2.16\n",
      "[3412 | 753.70] loss=2.34 avg=2.17\n",
      "[3413 | 753.76] loss=2.83 avg=2.17\n",
      "[3414 | 753.81] loss=2.43 avg=2.17\n",
      "[3415 | 753.87] loss=2.57 avg=2.18\n",
      "[3416 | 753.92] loss=2.95 avg=2.19\n",
      "[3417 | 753.98] loss=2.10 avg=2.19\n",
      "[3418 | 754.03] loss=1.78 avg=2.18\n",
      "[3419 | 754.09] loss=2.57 avg=2.18\n",
      "[3420 | 754.14] loss=2.19 avg=2.19\n",
      "[3421 | 754.19] loss=1.51 avg=2.18\n",
      "[3422 | 754.24] loss=2.11 avg=2.18\n",
      "[3423 | 754.30] loss=1.84 avg=2.17\n",
      "[3424 | 754.35] loss=2.82 avg=2.18\n",
      "[3425 | 754.41] loss=1.93 avg=2.18\n",
      "[3426 | 754.46] loss=1.84 avg=2.17\n",
      "[3427 | 754.52] loss=3.01 avg=2.18\n",
      "[3428 | 754.57] loss=2.25 avg=2.18\n",
      "[3429 | 754.63] loss=2.17 avg=2.18\n",
      "[3430 | 754.68] loss=2.56 avg=2.19\n",
      "[3431 | 754.73] loss=2.42 avg=2.19\n",
      "[3432 | 754.79] loss=1.90 avg=2.19\n",
      "[3433 | 754.84] loss=2.87 avg=2.19\n",
      "[3434 | 754.89] loss=1.06 avg=2.18\n",
      "[3435 | 754.95] loss=2.08 avg=2.18\n",
      "[3436 | 755.00] loss=2.89 avg=2.19\n",
      "[3437 | 755.06] loss=2.86 avg=2.20\n",
      "[3438 | 755.12] loss=2.83 avg=2.20\n",
      "[3439 | 755.17] loss=2.50 avg=2.20\n",
      "[3440 | 755.23] loss=2.13 avg=2.20\n",
      "[3441 | 755.28] loss=1.61 avg=2.20\n",
      "[3442 | 755.34] loss=1.67 avg=2.19\n",
      "[3443 | 755.39] loss=2.69 avg=2.20\n",
      "[3444 | 755.45] loss=1.41 avg=2.19\n",
      "[3445 | 755.50] loss=1.17 avg=2.18\n",
      "[3446 | 755.55] loss=2.83 avg=2.19\n",
      "[3447 | 755.61] loss=1.36 avg=2.18\n",
      "[3448 | 755.66] loss=1.26 avg=2.17\n",
      "[3449 | 755.72] loss=3.21 avg=2.18\n",
      "[3450 | 755.78] loss=1.98 avg=2.18\n",
      "[3451 | 755.83] loss=1.97 avg=2.17\n",
      "[3452 | 755.88] loss=2.28 avg=2.18\n",
      "[3453 | 755.94] loss=1.85 avg=2.17\n",
      "[3454 | 756.00] loss=2.42 avg=2.18\n",
      "[3455 | 756.06] loss=2.13 avg=2.17\n",
      "[3456 | 756.11] loss=1.64 avg=2.17\n",
      "[3457 | 756.16] loss=3.24 avg=2.18\n",
      "[3458 | 756.22] loss=1.78 avg=2.18\n",
      "[3459 | 756.27] loss=1.56 avg=2.17\n",
      "[3460 | 756.32] loss=2.22 avg=2.17\n",
      "[3461 | 756.38] loss=3.07 avg=2.18\n",
      "[3462 | 756.44] loss=2.07 avg=2.18\n",
      "[3463 | 756.49] loss=1.86 avg=2.17\n",
      "[3464 | 756.54] loss=4.20 avg=2.20\n",
      "[3465 | 756.60] loss=3.14 avg=2.20\n",
      "[3466 | 756.65] loss=2.39 avg=2.21\n",
      "[3467 | 756.71] loss=1.06 avg=2.20\n",
      "[3468 | 756.77] loss=2.02 avg=2.19\n",
      "[3469 | 756.83] loss=2.00 avg=2.19\n",
      "[3470 | 756.88] loss=2.10 avg=2.19\n",
      "[3471 | 756.93] loss=1.29 avg=2.18\n",
      "[3472 | 756.99] loss=2.15 avg=2.18\n",
      "[3473 | 757.04] loss=1.57 avg=2.18\n",
      "[3474 | 757.09] loss=3.20 avg=2.19\n",
      "[3475 | 757.15] loss=1.20 avg=2.18\n",
      "[3476 | 757.20] loss=1.39 avg=2.17\n",
      "[3477 | 757.25] loss=0.68 avg=2.15\n",
      "[3478 | 757.31] loss=1.56 avg=2.15\n",
      "[3479 | 757.37] loss=2.51 avg=2.15\n",
      "[3480 | 757.42] loss=1.65 avg=2.15\n",
      "[3481 | 757.48] loss=1.76 avg=2.14\n",
      "[3482 | 757.53] loss=1.96 avg=2.14\n",
      "[3483 | 757.59] loss=2.42 avg=2.14\n",
      "[3484 | 757.65] loss=2.80 avg=2.15\n",
      "[3485 | 757.70] loss=2.23 avg=2.15\n",
      "[3486 | 757.75] loss=3.53 avg=2.16\n",
      "[3487 | 757.81] loss=1.87 avg=2.16\n",
      "[3488 | 757.86] loss=1.23 avg=2.15\n",
      "[3489 | 757.91] loss=2.72 avg=2.16\n",
      "[3490 | 757.97] loss=2.89 avg=2.16\n",
      "[3491 | 758.02] loss=1.89 avg=2.16\n",
      "[3492 | 758.08] loss=1.88 avg=2.16\n",
      "[3493 | 758.13] loss=1.81 avg=2.16\n",
      "[3494 | 758.18] loss=2.84 avg=2.16\n",
      "[3495 | 758.24] loss=3.06 avg=2.17\n",
      "[3496 | 758.29] loss=1.72 avg=2.17\n",
      "[3497 | 758.34] loss=1.96 avg=2.16\n",
      "[3498 | 758.40] loss=1.91 avg=2.16\n",
      "[3499 | 758.45] loss=2.12 avg=2.16\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " >=\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t((((((n)))))));;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              )\n",
      "*                                                      \n",
      "                                           \n",
      "                                    }\n",
      "                               }\n",
      "                             return    \n",
      "             ;  \n",
      "                                         \n",
      "                                                \n",
      "     \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3500 | 774.59] validation loss = 2.24\n",
      "[3500 | 774.65] loss=1.97 avg=2.16\n",
      "[3501 | 774.71] loss=2.10 avg=2.16\n",
      "[3502 | 774.76] loss=1.00 avg=2.15\n",
      "[3503 | 774.82] loss=1.69 avg=2.14\n",
      "[3504 | 774.87] loss=2.14 avg=2.14\n",
      "[3505 | 774.92] loss=1.45 avg=2.14\n",
      "[3506 | 774.98] loss=2.86 avg=2.14\n",
      "[3507 | 775.03] loss=2.47 avg=2.15\n",
      "[3508 | 775.09] loss=3.09 avg=2.16\n",
      "[3509 | 775.14] loss=2.64 avg=2.16\n",
      "[3510 | 775.20] loss=2.12 avg=2.16\n",
      "[3511 | 775.25] loss=1.54 avg=2.15\n",
      "[3512 | 775.30] loss=2.42 avg=2.16\n",
      "[3513 | 775.36] loss=2.96 avg=2.17\n",
      "[3514 | 775.42] loss=1.44 avg=2.16\n",
      "[3515 | 775.47] loss=1.77 avg=2.15\n",
      "[3516 | 775.53] loss=2.95 avg=2.16\n",
      "[3517 | 775.58] loss=2.35 avg=2.16\n",
      "[3518 | 775.64] loss=1.10 avg=2.15\n",
      "[3519 | 775.69] loss=2.20 avg=2.15\n",
      "[3520 | 775.75] loss=1.93 avg=2.15\n",
      "[3521 | 775.80] loss=2.18 avg=2.15\n",
      "[3522 | 775.85] loss=0.70 avg=2.14\n",
      "[3523 | 775.91] loss=3.03 avg=2.15\n",
      "[3524 | 775.97] loss=1.73 avg=2.14\n",
      "[3525 | 776.02] loss=2.06 avg=2.14\n",
      "[3526 | 776.07] loss=2.57 avg=2.15\n",
      "[3527 | 776.13] loss=2.25 avg=2.15\n",
      "[3528 | 776.18] loss=1.62 avg=2.14\n",
      "[3529 | 776.24] loss=2.08 avg=2.14\n",
      "[3530 | 776.29] loss=1.71 avg=2.14\n",
      "[3531 | 776.34] loss=2.81 avg=2.14\n",
      "[3532 | 776.40] loss=1.90 avg=2.14\n",
      "[3533 | 776.45] loss=3.43 avg=2.15\n",
      "[3534 | 776.51] loss=1.80 avg=2.15\n",
      "[3535 | 776.56] loss=2.37 avg=2.15\n",
      "[3536 | 776.62] loss=1.94 avg=2.15\n",
      "[3537 | 776.67] loss=2.65 avg=2.16\n",
      "[3538 | 776.73] loss=2.82 avg=2.16\n",
      "[3539 | 776.78] loss=3.20 avg=2.17\n",
      "[3540 | 776.84] loss=2.85 avg=2.18\n",
      "[3541 | 776.89] loss=2.30 avg=2.18\n",
      "[3542 | 776.95] loss=2.55 avg=2.18\n",
      "[3543 | 777.00] loss=1.40 avg=2.18\n",
      "[3544 | 777.06] loss=2.29 avg=2.18\n",
      "[3545 | 777.11] loss=1.63 avg=2.17\n",
      "[3546 | 777.17] loss=1.29 avg=2.16\n",
      "[3547 | 777.22] loss=2.59 avg=2.17\n",
      "[3548 | 777.28] loss=2.76 avg=2.17\n",
      "[3549 | 777.33] loss=2.72 avg=2.18\n",
      "[3550 | 777.38] loss=2.58 avg=2.18\n",
      "[3551 | 777.44] loss=2.46 avg=2.19\n",
      "[3552 | 777.49] loss=2.52 avg=2.19\n",
      "[3553 | 777.55] loss=1.14 avg=2.18\n",
      "[3554 | 777.60] loss=3.11 avg=2.19\n",
      "[3555 | 777.65] loss=2.48 avg=2.19\n",
      "[3556 | 777.71] loss=2.86 avg=2.20\n",
      "[3557 | 777.76] loss=2.59 avg=2.20\n",
      "[3558 | 777.82] loss=3.86 avg=2.22\n",
      "[3559 | 777.87] loss=1.05 avg=2.21\n",
      "[3560 | 777.93] loss=1.20 avg=2.20\n",
      "[3561 | 777.98] loss=0.83 avg=2.18\n",
      "[3562 | 778.04] loss=1.91 avg=2.18\n",
      "[3563 | 778.09] loss=2.58 avg=2.18\n",
      "[3564 | 778.15] loss=3.03 avg=2.19\n",
      "[3565 | 778.20] loss=3.21 avg=2.20\n",
      "[3566 | 778.26] loss=2.72 avg=2.21\n",
      "[3567 | 778.31] loss=1.62 avg=2.20\n",
      "[3568 | 778.37] loss=3.37 avg=2.21\n",
      "[3569 | 778.42] loss=2.39 avg=2.21\n",
      "[3570 | 778.48] loss=2.04 avg=2.21\n",
      "[3571 | 778.53] loss=2.28 avg=2.21\n",
      "[3572 | 778.59] loss=2.70 avg=2.22\n",
      "[3573 | 778.64] loss=1.89 avg=2.22\n",
      "[3574 | 778.69] loss=1.90 avg=2.21\n",
      "[3575 | 778.75] loss=3.12 avg=2.22\n",
      "[3576 | 778.80] loss=2.20 avg=2.22\n",
      "[3577 | 778.86] loss=1.61 avg=2.21\n",
      "[3578 | 778.92] loss=2.35 avg=2.22\n",
      "[3579 | 778.98] loss=2.29 avg=2.22\n",
      "[3580 | 779.03] loss=2.20 avg=2.22\n",
      "[3581 | 779.09] loss=1.11 avg=2.21\n",
      "[3582 | 779.14] loss=2.21 avg=2.21\n",
      "[3583 | 779.20] loss=2.23 avg=2.21\n",
      "[3584 | 779.25] loss=2.42 avg=2.21\n",
      "[3585 | 779.30] loss=1.93 avg=2.21\n",
      "[3586 | 779.36] loss=1.95 avg=2.20\n",
      "[3587 | 779.41] loss=1.87 avg=2.20\n",
      "[3588 | 779.47] loss=2.25 avg=2.20\n",
      "[3589 | 779.52] loss=0.61 avg=2.18\n",
      "[3590 | 779.57] loss=0.89 avg=2.17\n",
      "[3591 | 779.63] loss=1.66 avg=2.17\n",
      "[3592 | 779.68] loss=1.83 avg=2.16\n",
      "[3593 | 779.74] loss=1.63 avg=2.16\n",
      "[3594 | 779.79] loss=2.74 avg=2.16\n",
      "[3595 | 779.85] loss=1.84 avg=2.16\n",
      "[3596 | 779.90] loss=1.24 avg=2.15\n",
      "[3597 | 779.96] loss=2.76 avg=2.16\n",
      "[3598 | 780.01] loss=2.86 avg=2.16\n",
      "[3599 | 780.07] loss=2.57 avg=2.17\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      ");\n",
      "      /*\n",
      "\\  \n",
      "     public void run(java.lang.System.out.println(\"Run the application\");\n",
      ") {\n",
      "      android.os.Binder.class org.charlene.jr.clarkman.charlene.java2.ComboBox\n",
      "        java.lang.String javaPath = \"org.charlene.charlene.java2\n",
      "        java.lang.String filePath = \"org.charlene.charlene.java2\n",
      "        }\n",
      "       /*\n",
      "\\       org.hug.hug1.org.internal.hug.hug1.hug1_hug_info org.hug.hug.soup.HugInfo_get_name, gid, gid_type, gids,\n",
      "          org.hug.hug1.org.internal.hug.hug.hug.soup.InternalHugInfo_new_hud_type,\n",
      "           org.hug.hug1.org.internal.hug.hug.hug.soup.HugInfo_get_name,\n",
      "      \n",
      "          id = org.hug.hug0.org.internal.hug.hug.soup.internalHugInfo.get_name);\n",
      "         gid_path = gid_type;\n",
      "         gid_type = org.hug.hug0.org.internal.hug.hug.soup.internalHugInfo.get_name;\n",
      "       }\n",
      "    \n",
      "  \n",
      "    gid_type = org.hug.hug0.org.internal.hug.hug.soup.internalHugInfo.get_name;\n",
      "   \n",
      " \n",
      "  \n",
      "   org.hug.hug0.org.internal.hug.hug.soup.internalHugInfo.init(gid_name,\n",
      "      \n",
      "     \n",
      "   \n",
      "  \n",
      "   org.hug0.org.internal.hug.hug.hug.soup.internalHugInfo_get_name\n",
      "      \n",
      "  \n",
      "   org.hhn0.combinator.combinator.internal.internal.hug.hug_HugInfo_get_name) {\n",
      "     \n",
      "     }\n",
      "   \n",
      "  \n",
      "   cl  {\n",
      "      org.hug.hug1.org.internal.hug.hug.hug.soup.internalHugInfo_get_name = gid_type;\n",
      "     \n",
      "    org.hhn0.combinator.org.internal.hug.hug.hug.soup.internalHugInfo_get_name(gid_type);\n",
      "    \n",
      "    org.hhn0.org.internal.hug.hug.soup.internalHugInfo_get_name(\"org.hhn0.org.internal.hug.hug.soup.internalHugInfo_nub_get_name\");\n",
      "     \n",
      "    org.hhn0.logger.logger.internal.hug.hug.hug.hug.SoupInfo_get_name();\n",
      "      org.hhn0.logger.logger.internal.hug.hug.hug.soup.internalHugInfo_init(gid_name,\n",
      "     \n",
      "   \n",
      "   \n",
      "   org.hhn0.org.internal.hug.hug.hug.soup.internalHugInfo_get_name);\n",
      "    }\n",
      "   \n",
      " \n",
      "  else\n",
      "   \n",
      "  \n",
      " \n",
      "  org.hug.hug0.org.internal.hug.hug.soup.internalHugInfo_get_name.name = gid_type;\n",
      "   \n",
      " \n",
      " \n",
      "  android.lang.String\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3600 | 796.12] validation loss = 2.24\n",
      "[3600 | 796.18] loss=2.21 avg=2.17\n",
      "[3601 | 796.23] loss=3.48 avg=2.18\n",
      "[3602 | 796.29] loss=2.03 avg=2.18\n",
      "[3603 | 796.34] loss=1.92 avg=2.18\n",
      "[3604 | 796.40] loss=2.66 avg=2.18\n",
      "[3605 | 796.46] loss=3.35 avg=2.19\n",
      "[3606 | 796.51] loss=0.80 avg=2.18\n",
      "[3607 | 796.57] loss=2.70 avg=2.19\n",
      "[3608 | 796.62] loss=2.00 avg=2.18\n",
      "[3609 | 796.68] loss=3.59 avg=2.20\n",
      "[3610 | 796.73] loss=2.73 avg=2.20\n",
      "[3611 | 796.79] loss=3.37 avg=2.21\n",
      "[3612 | 796.85] loss=1.83 avg=2.21\n",
      "[3613 | 796.90] loss=1.83 avg=2.21\n",
      "[3614 | 796.96] loss=2.44 avg=2.21\n",
      "[3615 | 797.01] loss=1.58 avg=2.20\n",
      "[3616 | 797.06] loss=1.98 avg=2.20\n",
      "[3617 | 797.12] loss=1.79 avg=2.20\n",
      "[3618 | 797.17] loss=3.28 avg=2.21\n",
      "[3619 | 797.22] loss=2.40 avg=2.21\n",
      "[3620 | 797.28] loss=1.23 avg=2.20\n",
      "[3621 | 797.34] loss=1.66 avg=2.19\n",
      "[3622 | 797.39] loss=2.71 avg=2.20\n",
      "[3623 | 797.45] loss=1.02 avg=2.19\n",
      "[3624 | 797.50] loss=2.41 avg=2.19\n",
      "[3625 | 797.56] loss=2.18 avg=2.19\n",
      "[3626 | 797.61] loss=1.39 avg=2.18\n",
      "[3627 | 797.67] loss=2.30 avg=2.18\n",
      "[3628 | 797.73] loss=3.13 avg=2.19\n",
      "[3629 | 797.78] loss=3.38 avg=2.20\n",
      "[3630 | 797.84] loss=2.24 avg=2.20\n",
      "[3631 | 797.89] loss=3.34 avg=2.22\n",
      "[3632 | 797.95] loss=2.01 avg=2.21\n",
      "[3633 | 798.00] loss=1.58 avg=2.21\n",
      "[3634 | 798.06] loss=1.93 avg=2.20\n",
      "[3635 | 798.12] loss=1.94 avg=2.20\n",
      "[3636 | 798.17] loss=1.48 avg=2.19\n",
      "[3637 | 798.22] loss=2.19 avg=2.19\n",
      "[3638 | 798.28] loss=2.13 avg=2.19\n",
      "[3639 | 798.33] loss=2.36 avg=2.20\n",
      "[3640 | 798.39] loss=1.88 avg=2.19\n",
      "[3641 | 798.44] loss=3.76 avg=2.21\n",
      "[3642 | 798.50] loss=2.98 avg=2.22\n",
      "[3643 | 798.55] loss=2.09 avg=2.21\n",
      "[3644 | 798.61] loss=3.34 avg=2.23\n",
      "[3645 | 798.67] loss=1.71 avg=2.22\n",
      "[3646 | 798.72] loss=2.54 avg=2.22\n",
      "[3647 | 798.77] loss=1.34 avg=2.22\n",
      "[3648 | 798.83] loss=2.55 avg=2.22\n",
      "[3649 | 798.89] loss=2.04 avg=2.22\n",
      "[3650 | 798.94] loss=3.14 avg=2.23\n",
      "[3651 | 799.00] loss=2.42 avg=2.23\n",
      "[3652 | 799.05] loss=2.43 avg=2.23\n",
      "[3653 | 799.11] loss=2.69 avg=2.23\n",
      "[3654 | 799.17] loss=3.37 avg=2.25\n",
      "[3655 | 799.22] loss=2.72 avg=2.25\n",
      "[3656 | 799.28] loss=1.38 avg=2.24\n",
      "[3657 | 799.33] loss=1.72 avg=2.24\n",
      "[3658 | 799.39] loss=1.98 avg=2.23\n",
      "[3659 | 799.44] loss=1.63 avg=2.23\n",
      "[3660 | 799.49] loss=2.17 avg=2.23\n",
      "[3661 | 799.54] loss=1.64 avg=2.22\n",
      "[3662 | 799.60] loss=2.86 avg=2.23\n",
      "[3663 | 799.65] loss=1.74 avg=2.22\n",
      "[3664 | 799.71] loss=1.62 avg=2.22\n",
      "[3665 | 799.76] loss=2.94 avg=2.22\n",
      "[3666 | 799.82] loss=2.40 avg=2.23\n",
      "[3667 | 799.88] loss=1.35 avg=2.22\n",
      "[3668 | 799.93] loss=1.52 avg=2.21\n",
      "[3669 | 799.99] loss=2.50 avg=2.21\n",
      "[3670 | 800.05] loss=1.25 avg=2.20\n",
      "[3671 | 800.10] loss=1.87 avg=2.20\n",
      "[3672 | 800.16] loss=2.44 avg=2.20\n",
      "[3673 | 800.22] loss=2.38 avg=2.20\n",
      "[3674 | 800.27] loss=1.62 avg=2.20\n",
      "[3675 | 800.33] loss=1.77 avg=2.19\n",
      "[3676 | 800.38] loss=2.68 avg=2.20\n",
      "[3677 | 800.44] loss=2.21 avg=2.20\n",
      "[3678 | 800.49] loss=2.26 avg=2.20\n",
      "[3679 | 800.54] loss=2.66 avg=2.20\n",
      "[3680 | 800.60] loss=2.24 avg=2.20\n",
      "[3681 | 800.65] loss=1.97 avg=2.20\n",
      "[3682 | 800.70] loss=1.70 avg=2.20\n",
      "[3683 | 800.76] loss=2.90 avg=2.20\n",
      "[3684 | 800.82] loss=1.08 avg=2.19\n",
      "[3685 | 800.87] loss=2.10 avg=2.19\n",
      "[3686 | 800.93] loss=2.17 avg=2.19\n",
      "[3687 | 800.98] loss=1.74 avg=2.19\n",
      "[3688 | 801.03] loss=2.37 avg=2.19\n",
      "[3689 | 801.09] loss=2.67 avg=2.19\n",
      "[3690 | 801.14] loss=2.04 avg=2.19\n",
      "[3691 | 801.20] loss=2.32 avg=2.19\n",
      "[3692 | 801.25] loss=1.94 avg=2.19\n",
      "[3693 | 801.31] loss=2.40 avg=2.19\n",
      "[3694 | 801.36] loss=2.18 avg=2.19\n",
      "[3695 | 801.42] loss=1.92 avg=2.19\n",
      "[3696 | 801.47] loss=2.06 avg=2.19\n",
      "[3697 | 801.52] loss=0.96 avg=2.18\n",
      "[3698 | 801.58] loss=2.75 avg=2.18\n",
      "[3699 | 801.63] loss=1.45 avg=2.18\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "->\n",
      "\n",
      "            {\n",
      "                            }\n",
      "\n",
      "                        \n",
      "                             \n",
      "                           \n",
      "              }\n",
      "\n",
      "                       \n",
      "        if (!(!(org.apache.maven.internal.common.event.EventEmitter.events.EventEmitEmitEmit(event.args, (event.args)))) {\n",
      "                                \n",
      "                       else\n",
      "                              \n",
      "                          \n",
      "                       if (!(org.apache.maven.internal.common.event.EventEmitter.events.EventEmitEmit(event.args, (event.args)))) {\n",
      "                    \n",
      "                    }\n",
      "                  \n",
      "                  \n",
      "      }\n",
      "\n",
      "      \n",
      "    org.apache.maven.internal.common.event.EventEmitter.eventEventEvent eventArgs args = new org.apache.maven.internal.common.event.EventEmitter.eventChange(event.args, event.args, 2, ));\n",
      "\n",
      "   \n",
      "            \n",
      "              \n",
      "           :\n",
      "        \n",
      "              \n",
      "           }\n",
      "  \n",
      "\n",
      "      \n",
      "\n",
      "                   \n",
      "\n",
      "                \n",
      "\n",
      "                     \n",
      "                       \n",
      "\n",
      "              \n",
      "               \n",
      "               \n",
      "              \n",
      "              \n",
      "           \n",
      "              \n",
      "          \n",
      "             \n",
      "           \n",
      "          \n",
      "         \n",
      "           \n",
      "         \n",
      "         \n",
      "       \n",
      "      \n",
      "\n",
      "         \n",
      "     \n",
      "\n",
      "       \n",
      "\n",
      "      \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3700 | 817.60] validation loss = 2.23\n",
      "[3700 | 817.66] loss=1.08 avg=2.16\n",
      "[3701 | 817.72] loss=1.39 avg=2.16\n",
      "[3702 | 817.77] loss=1.78 avg=2.15\n",
      "[3703 | 817.82] loss=3.00 avg=2.16\n",
      "[3704 | 817.88] loss=2.92 avg=2.17\n",
      "[3705 | 817.93] loss=2.10 avg=2.17\n",
      "[3706 | 817.99] loss=3.16 avg=2.18\n",
      "[3707 | 818.04] loss=2.37 avg=2.18\n",
      "[3708 | 818.09] loss=2.38 avg=2.18\n",
      "[3709 | 818.15] loss=2.00 avg=2.18\n",
      "[3710 | 818.20] loss=3.13 avg=2.19\n",
      "[3711 | 818.26] loss=1.24 avg=2.18\n",
      "[3712 | 818.32] loss=2.47 avg=2.18\n",
      "[3713 | 818.37] loss=1.84 avg=2.18\n",
      "[3714 | 818.43] loss=1.75 avg=2.18\n",
      "[3715 | 818.48] loss=2.29 avg=2.18\n",
      "[3716 | 818.53] loss=2.65 avg=2.18\n",
      "[3717 | 818.59] loss=2.58 avg=2.19\n",
      "[3718 | 818.64] loss=2.22 avg=2.19\n",
      "[3719 | 818.70] loss=1.64 avg=2.18\n",
      "[3720 | 818.76] loss=3.17 avg=2.19\n",
      "[3721 | 818.81] loss=2.48 avg=2.19\n",
      "[3722 | 818.86] loss=2.60 avg=2.20\n",
      "[3723 | 818.92] loss=1.66 avg=2.19\n",
      "[3724 | 818.97] loss=2.46 avg=2.19\n",
      "[3725 | 819.03] loss=2.36 avg=2.20\n",
      "[3726 | 819.09] loss=3.29 avg=2.21\n",
      "[3727 | 819.14] loss=2.25 avg=2.21\n",
      "[3728 | 819.19] loss=1.58 avg=2.20\n",
      "[3729 | 819.24] loss=2.15 avg=2.20\n",
      "[3730 | 819.30] loss=2.45 avg=2.20\n",
      "[3731 | 819.35] loss=2.85 avg=2.21\n",
      "[3732 | 819.41] loss=2.15 avg=2.21\n",
      "[3733 | 819.46] loss=2.31 avg=2.21\n",
      "[3734 | 819.52] loss=1.96 avg=2.21\n",
      "[3735 | 819.58] loss=2.64 avg=2.21\n",
      "[3736 | 819.63] loss=1.43 avg=2.20\n",
      "[3737 | 819.69] loss=2.36 avg=2.21\n",
      "[3738 | 819.75] loss=0.49 avg=2.19\n",
      "[3739 | 819.81] loss=2.41 avg=2.19\n",
      "[3740 | 819.86] loss=2.31 avg=2.19\n",
      "[3741 | 819.92] loss=2.11 avg=2.19\n",
      "[3742 | 819.97] loss=1.65 avg=2.19\n",
      "[3743 | 820.03] loss=2.57 avg=2.19\n",
      "[3744 | 820.08] loss=2.74 avg=2.20\n",
      "[3745 | 820.14] loss=2.76 avg=2.20\n",
      "[3746 | 820.19] loss=2.81 avg=2.21\n",
      "[3747 | 820.24] loss=1.17 avg=2.20\n",
      "[3748 | 820.30] loss=1.80 avg=2.19\n",
      "[3749 | 820.35] loss=1.30 avg=2.18\n",
      "[3750 | 820.41] loss=1.86 avg=2.18\n",
      "[3751 | 820.47] loss=2.33 avg=2.18\n",
      "[3752 | 820.52] loss=2.89 avg=2.19\n",
      "[3753 | 820.58] loss=1.13 avg=2.18\n",
      "[3754 | 820.63] loss=2.04 avg=2.18\n",
      "[3755 | 820.69] loss=1.91 avg=2.17\n",
      "[3756 | 820.74] loss=1.36 avg=2.17\n",
      "[3757 | 820.80] loss=2.26 avg=2.17\n",
      "[3758 | 820.85] loss=1.62 avg=2.16\n",
      "[3759 | 820.91] loss=2.34 avg=2.16\n",
      "[3760 | 820.96] loss=2.11 avg=2.16\n",
      "[3761 | 821.02] loss=1.54 avg=2.16\n",
      "[3762 | 821.07] loss=1.79 avg=2.15\n",
      "[3763 | 821.13] loss=2.68 avg=2.16\n",
      "[3764 | 821.18] loss=2.99 avg=2.17\n",
      "[3765 | 821.24] loss=2.41 avg=2.17\n",
      "[3766 | 821.29] loss=2.43 avg=2.17\n",
      "[3767 | 821.35] loss=3.54 avg=2.19\n",
      "[3768 | 821.40] loss=2.10 avg=2.18\n",
      "[3769 | 821.45] loss=2.53 avg=2.19\n",
      "[3770 | 821.51] loss=2.62 avg=2.19\n",
      "[3771 | 821.56] loss=2.21 avg=2.19\n",
      "[3772 | 821.62] loss=1.47 avg=2.19\n",
      "[3773 | 821.67] loss=2.49 avg=2.19\n",
      "[3774 | 821.73] loss=2.33 avg=2.19\n",
      "[3775 | 821.78] loss=1.68 avg=2.18\n",
      "[3776 | 821.84] loss=1.63 avg=2.18\n",
      "[3777 | 821.89] loss=2.41 avg=2.18\n",
      "[3778 | 821.94] loss=2.36 avg=2.18\n",
      "[3779 | 822.00] loss=2.74 avg=2.19\n",
      "[3780 | 822.05] loss=2.72 avg=2.19\n",
      "[3781 | 822.10] loss=2.55 avg=2.20\n",
      "[3782 | 822.16] loss=2.00 avg=2.20\n",
      "[3783 | 822.22] loss=1.82 avg=2.19\n",
      "[3784 | 822.27] loss=1.34 avg=2.18\n",
      "[3785 | 822.33] loss=1.25 avg=2.17\n",
      "[3786 | 822.39] loss=1.65 avg=2.17\n",
      "[3787 | 822.44] loss=2.87 avg=2.18\n",
      "[3788 | 822.49] loss=2.36 avg=2.18\n",
      "[3789 | 822.55] loss=1.52 avg=2.17\n",
      "[3790 | 822.60] loss=1.77 avg=2.17\n",
      "[3791 | 822.66] loss=2.21 avg=2.17\n",
      "[3792 | 822.71] loss=3.12 avg=2.18\n",
      "[3793 | 822.77] loss=1.07 avg=2.17\n",
      "[3794 | 822.83] loss=3.02 avg=2.17\n",
      "[3795 | 822.88] loss=1.85 avg=2.17\n",
      "[3796 | 822.94] loss=2.79 avg=2.18\n",
      "[3797 | 822.99] loss=2.53 avg=2.18\n",
      "[3798 | 823.05] loss=1.67 avg=2.18\n",
      "[3799 | 823.10] loss=1.94 avg=2.17\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "                                                                                          (\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)-- (LR\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t的\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t的\t\t)-- (CL\t\t\t\t\t的\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t的\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)--(\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t的\t\t\t\t\t\t的\t)--})))))));\n",
      "    (\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)-- (LR\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)--\n",
      "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t)--\n",
      "--(LR\t\t\t\t\t\t\t\t\t\t\t\t--)--(LR\t\t\t\t\t\t\t\t)--}))))((\t\t\t\t\t\t\t\t\t\t\t)--) (LR\t\t\t\t\t\t\t\t--) (LR\t\t\t\t\t\t)--;\n",
      ")--((<?\t\t\t\t\t\t\t)--)--)--}))))\n",
      "\n",
      "                                                                                                                                                                                                   else\n",
      "                                                                                                 \n",
      "                                                                   \n",
      "                                                          \n",
      "                                          } */\n",
      "                                    \n",
      "                                           \n",
      "                   \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3800 | 839.09] validation loss = 2.23\n",
      "[3800 | 839.15] loss=1.21 avg=2.16\n",
      "[3801 | 839.21] loss=2.34 avg=2.17\n",
      "[3802 | 839.26] loss=1.64 avg=2.16\n",
      "[3803 | 839.32] loss=2.90 avg=2.17\n",
      "[3804 | 839.37] loss=2.40 avg=2.17\n",
      "[3805 | 839.43] loss=2.45 avg=2.17\n",
      "[3806 | 839.49] loss=3.36 avg=2.18\n",
      "[3807 | 839.55] loss=2.19 avg=2.18\n",
      "[3808 | 839.60] loss=1.84 avg=2.18\n",
      "[3809 | 839.66] loss=3.43 avg=2.19\n",
      "[3810 | 839.71] loss=2.54 avg=2.20\n",
      "[3811 | 839.77] loss=2.19 avg=2.20\n",
      "[3812 | 839.82] loss=2.48 avg=2.20\n",
      "[3813 | 839.88] loss=2.78 avg=2.21\n",
      "[3814 | 839.93] loss=1.84 avg=2.20\n",
      "[3815 | 839.98] loss=2.05 avg=2.20\n",
      "[3816 | 840.04] loss=2.11 avg=2.20\n",
      "[3817 | 840.09] loss=3.24 avg=2.21\n",
      "[3818 | 840.15] loss=3.00 avg=2.22\n",
      "[3819 | 840.20] loss=1.52 avg=2.21\n",
      "[3820 | 840.26] loss=1.39 avg=2.20\n",
      "[3821 | 840.31] loss=3.86 avg=2.22\n",
      "[3822 | 840.37] loss=2.97 avg=2.23\n",
      "[3823 | 840.43] loss=2.89 avg=2.23\n",
      "[3824 | 840.48] loss=3.03 avg=2.24\n",
      "[3825 | 840.54] loss=2.09 avg=2.24\n",
      "[3826 | 840.59] loss=1.67 avg=2.23\n",
      "[3827 | 840.65] loss=2.62 avg=2.24\n",
      "[3828 | 840.70] loss=1.55 avg=2.23\n",
      "[3829 | 840.76] loss=2.93 avg=2.24\n",
      "[3830 | 840.81] loss=2.17 avg=2.24\n",
      "[3831 | 840.87] loss=2.28 avg=2.24\n",
      "[3832 | 840.92] loss=2.83 avg=2.24\n",
      "[3833 | 840.97] loss=0.83 avg=2.23\n",
      "[3834 | 841.03] loss=2.43 avg=2.23\n",
      "[3835 | 841.09] loss=2.47 avg=2.23\n",
      "[3836 | 841.14] loss=2.50 avg=2.24\n",
      "[3837 | 841.20] loss=1.31 avg=2.23\n",
      "[3838 | 841.25] loss=2.24 avg=2.23\n",
      "[3839 | 841.31] loss=2.11 avg=2.23\n",
      "[3840 | 841.36] loss=2.11 avg=2.23\n",
      "[3841 | 841.41] loss=2.64 avg=2.23\n",
      "[3842 | 841.47] loss=1.41 avg=2.22\n",
      "[3843 | 841.52] loss=2.17 avg=2.22\n",
      "[3844 | 841.58] loss=1.62 avg=2.21\n",
      "[3845 | 841.63] loss=2.16 avg=2.21\n",
      "[3846 | 841.68] loss=1.75 avg=2.21\n",
      "[3847 | 841.74] loss=2.81 avg=2.22\n",
      "[3848 | 841.79] loss=2.28 avg=2.22\n",
      "[3849 | 841.84] loss=1.01 avg=2.20\n",
      "[3850 | 841.89] loss=2.12 avg=2.20\n",
      "[3851 | 841.95] loss=2.54 avg=2.21\n",
      "[3852 | 842.00] loss=2.01 avg=2.20\n",
      "[3853 | 842.06] loss=1.35 avg=2.20\n",
      "[3854 | 842.11] loss=2.06 avg=2.19\n",
      "[3855 | 842.16] loss=3.72 avg=2.21\n",
      "[3856 | 842.22] loss=1.53 avg=2.20\n",
      "[3857 | 842.28] loss=2.68 avg=2.21\n",
      "[3858 | 842.33] loss=1.47 avg=2.20\n",
      "[3859 | 842.39] loss=1.31 avg=2.19\n",
      "[3860 | 842.44] loss=2.39 avg=2.19\n",
      "[3861 | 842.50] loss=2.29 avg=2.19\n",
      "[3862 | 842.55] loss=2.14 avg=2.19\n",
      "[3863 | 842.60] loss=2.30 avg=2.20\n",
      "[3864 | 842.66] loss=1.19 avg=2.19\n",
      "[3865 | 842.71] loss=2.01 avg=2.18\n",
      "[3866 | 842.77] loss=2.00 avg=2.18\n",
      "[3867 | 842.83] loss=1.31 avg=2.17\n",
      "[3868 | 842.89] loss=1.16 avg=2.16\n",
      "[3869 | 842.94] loss=2.44 avg=2.17\n",
      "[3870 | 842.99] loss=1.44 avg=2.16\n",
      "[3871 | 843.05] loss=1.75 avg=2.15\n",
      "[3872 | 843.11] loss=2.41 avg=2.16\n",
      "[3873 | 843.16] loss=1.21 avg=2.15\n",
      "[3874 | 843.22] loss=2.55 avg=2.15\n",
      "[3875 | 843.27] loss=1.80 avg=2.15\n",
      "[3876 | 843.33] loss=2.78 avg=2.15\n",
      "[3877 | 843.38] loss=2.31 avg=2.16\n",
      "[3878 | 843.44] loss=4.31 avg=2.18\n",
      "[3879 | 843.49] loss=1.61 avg=2.17\n",
      "[3880 | 843.54] loss=2.01 avg=2.17\n",
      "[3881 | 843.60] loss=3.00 avg=2.18\n",
      "[3882 | 843.65] loss=1.97 avg=2.18\n",
      "[3883 | 843.71] loss=2.09 avg=2.18\n",
      "[3884 | 843.76] loss=2.85 avg=2.18\n",
      "[3885 | 843.82] loss=2.41 avg=2.18\n",
      "[3886 | 843.87] loss=3.00 avg=2.19\n",
      "[3887 | 843.93] loss=1.22 avg=2.18\n",
      "[3888 | 843.98] loss=1.62 avg=2.18\n",
      "[3889 | 844.04] loss=2.31 avg=2.18\n",
      "[3890 | 844.09] loss=2.86 avg=2.19\n",
      "[3891 | 844.15] loss=2.80 avg=2.19\n",
      "[3892 | 844.20] loss=2.75 avg=2.20\n",
      "[3893 | 844.26] loss=3.03 avg=2.21\n",
      "[3894 | 844.31] loss=2.78 avg=2.21\n",
      "[3895 | 844.36] loss=3.49 avg=2.22\n",
      "[3896 | 844.42] loss=1.16 avg=2.21\n",
      "[3897 | 844.48] loss=2.78 avg=2.22\n",
      "[3898 | 844.53] loss=1.32 avg=2.21\n",
      "[3899 | 844.59] loss=2.18 avg=2.21\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "\t {\n",
      "    return null;\n",
      "   }\n",
      "}\n",
      "/**\n",
      "* @evolve(data, value)\n",
      "* @throws\n",
      "* @param (data) this\n",
      "* @return boolean\n",
      "*/\n",
      "public boolean onData (final Context context) {\n",
      "    int *value = (((value *) (data) (value)));\n",
      "    try {\n",
      "        \n",
      "           if ( data [ 0 ] < 0) {\n",
      "               if ( data [\n",
      "                           (value *) (data) (data))\n",
      "                         }\n",
      "                         else {\n",
      "                                                          \n",
      "                             return value == 0 ? value : data;\n",
      "                        }\n",
      "                                 if ((data == data.length)) {\n",
      "                                                        \n",
      "                                                 \n",
      "                                  else {\n",
      "                                              \n",
      "                                               \n",
      "                                           }\n",
      "                                }\n",
      "                           }\n",
      "                          \n",
      "    }\n",
      "                   }\n",
      "       }\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "    if (true)\n",
      "    {\n",
      "    public org.javafxpress.jdbc.core.media.MediaContentPanel mediaPanel = new org.javafxpress.jdbc.media.MediaContentPanel();\n",
      "    org.jafxpress.jnsc.MediaEngineMediaEngine mediaEngine = new org.javafxpress.jnsc.MediaEngineMediaEngine();\n",
      "    if (((mediaPanel == null) &&\n",
      "     new org.javafxpress.jnl.media.MediaEngineMediaEngine  ) <\n",
      "     * 1 &&\n",
      "     new org.javafxpress.jnl.media.MediaEngineMediaEngine\n",
      "    &&\n",
      "     org.javafxpress.jnl.media.MediaEngineMediaEngine    * 1)) {\n",
      "       if (((mediaPanel != null) &&\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3900 | 860.58] validation loss = 2.23\n",
      "[3900 | 860.64] loss=2.70 avg=2.21\n",
      "[3901 | 860.70] loss=2.97 avg=2.22\n",
      "[3902 | 860.76] loss=1.16 avg=2.21\n",
      "[3903 | 860.81] loss=2.26 avg=2.21\n",
      "[3904 | 860.87] loss=2.19 avg=2.21\n",
      "[3905 | 860.92] loss=2.70 avg=2.22\n",
      "[3906 | 860.97] loss=1.94 avg=2.21\n",
      "[3907 | 861.03] loss=2.85 avg=2.22\n",
      "[3908 | 861.09] loss=2.23 avg=2.22\n",
      "[3909 | 861.14] loss=3.12 avg=2.23\n",
      "[3910 | 861.19] loss=3.04 avg=2.24\n",
      "[3911 | 861.24] loss=1.86 avg=2.23\n",
      "[3912 | 861.30] loss=1.19 avg=2.22\n",
      "[3913 | 861.36] loss=2.13 avg=2.22\n",
      "[3914 | 861.41] loss=2.51 avg=2.23\n",
      "[3915 | 861.47] loss=2.91 avg=2.23\n",
      "[3916 | 861.52] loss=2.18 avg=2.23\n",
      "[3917 | 861.58] loss=2.41 avg=2.23\n",
      "[3918 | 861.63] loss=2.93 avg=2.24\n",
      "[3919 | 861.69] loss=3.94 avg=2.26\n",
      "[3920 | 861.74] loss=3.01 avg=2.26\n",
      "[3921 | 861.80] loss=2.07 avg=2.26\n",
      "[3922 | 861.85] loss=2.26 avg=2.26\n",
      "[3923 | 861.91] loss=2.58 avg=2.27\n",
      "[3924 | 861.96] loss=2.44 avg=2.27\n",
      "[3925 | 862.02] loss=2.12 avg=2.27\n",
      "[3926 | 862.07] loss=1.50 avg=2.26\n",
      "[3927 | 862.13] loss=1.15 avg=2.25\n",
      "[3928 | 862.18] loss=2.12 avg=2.25\n",
      "[3929 | 862.23] loss=3.36 avg=2.26\n",
      "[3930 | 862.29] loss=3.09 avg=2.27\n",
      "[3931 | 862.34] loss=1.92 avg=2.26\n",
      "[3932 | 862.40] loss=0.84 avg=2.25\n",
      "[3933 | 862.45] loss=1.78 avg=2.24\n",
      "[3934 | 862.51] loss=1.35 avg=2.23\n",
      "[3935 | 862.57] loss=1.52 avg=2.23\n",
      "[3936 | 862.62] loss=1.58 avg=2.22\n",
      "[3937 | 862.67] loss=2.98 avg=2.23\n",
      "[3938 | 862.73] loss=1.28 avg=2.22\n",
      "[3939 | 862.78] loss=3.15 avg=2.23\n",
      "[3940 | 862.84] loss=1.64 avg=2.22\n",
      "[3941 | 862.90] loss=1.42 avg=2.21\n",
      "[3942 | 862.95] loss=0.97 avg=2.20\n",
      "[3943 | 863.00] loss=1.80 avg=2.20\n",
      "[3944 | 863.06] loss=1.73 avg=2.19\n",
      "[3945 | 863.12] loss=1.38 avg=2.18\n",
      "[3946 | 863.17] loss=2.17 avg=2.18\n",
      "[3947 | 863.23] loss=2.60 avg=2.19\n",
      "[3948 | 863.28] loss=2.33 avg=2.19\n",
      "[3949 | 863.34] loss=2.11 avg=2.19\n",
      "[3950 | 863.39] loss=1.20 avg=2.18\n",
      "[3951 | 863.45] loss=1.31 avg=2.17\n",
      "[3952 | 863.50] loss=2.24 avg=2.17\n",
      "[3953 | 863.56] loss=2.91 avg=2.18\n",
      "[3954 | 863.61] loss=0.53 avg=2.16\n",
      "[3955 | 863.66] loss=2.23 avg=2.16\n",
      "[3956 | 863.72] loss=2.18 avg=2.16\n",
      "[3957 | 863.77] loss=1.33 avg=2.16\n",
      "[3958 | 863.83] loss=1.58 avg=2.15\n",
      "[3959 | 863.89] loss=3.47 avg=2.16\n",
      "[3960 | 863.94] loss=3.07 avg=2.17\n",
      "[3961 | 863.99] loss=2.90 avg=2.18\n",
      "[3962 | 864.05] loss=1.71 avg=2.17\n",
      "[3963 | 864.10] loss=1.99 avg=2.17\n",
      "[3964 | 864.16] loss=1.54 avg=2.17\n",
      "[3965 | 864.21] loss=1.24 avg=2.16\n",
      "[3966 | 864.27] loss=1.43 avg=2.15\n",
      "[3967 | 864.33] loss=2.86 avg=2.16\n",
      "[3968 | 864.38] loss=1.65 avg=2.15\n",
      "[3969 | 864.44] loss=2.22 avg=2.15\n",
      "[3970 | 864.49] loss=1.73 avg=2.15\n",
      "[3971 | 864.55] loss=2.44 avg=2.15\n",
      "[3972 | 864.60] loss=2.63 avg=2.16\n",
      "[3973 | 864.66] loss=2.18 avg=2.16\n",
      "[3974 | 864.71] loss=2.65 avg=2.16\n",
      "[3975 | 864.77] loss=1.68 avg=2.16\n",
      "[3976 | 864.83] loss=2.05 avg=2.16\n",
      "[3977 | 864.88] loss=1.16 avg=2.15\n",
      "[3978 | 864.94] loss=1.51 avg=2.14\n",
      "[3979 | 864.99] loss=1.86 avg=2.14\n",
      "[3980 | 865.05] loss=2.10 avg=2.14\n",
      "[3981 | 865.10] loss=1.37 avg=2.13\n",
      "[3982 | 865.15] loss=2.69 avg=2.13\n",
      "[3983 | 865.21] loss=1.60 avg=2.13\n",
      "[3984 | 865.26] loss=2.53 avg=2.13\n",
      "[3985 | 865.32] loss=2.06 avg=2.13\n",
      "[3986 | 865.37] loss=2.64 avg=2.14\n",
      "[3987 | 865.43] loss=2.36 avg=2.14\n",
      "[3988 | 865.48] loss=1.99 avg=2.14\n",
      "[3989 | 865.53] loss=2.15 avg=2.14\n",
      "[3990 | 865.59] loss=1.21 avg=2.13\n",
      "[3991 | 865.64] loss=2.38 avg=2.13\n",
      "[3992 | 865.70] loss=1.22 avg=2.12\n",
      "[3993 | 865.75] loss=1.46 avg=2.11\n",
      "[3994 | 865.81] loss=1.89 avg=2.11\n",
      "[3995 | 865.87] loss=2.11 avg=2.11\n",
      "[3996 | 865.92] loss=3.58 avg=2.13\n",
      "[3997 | 865.97] loss=2.52 avg=2.13\n",
      "[3998 | 866.03] loss=2.03 avg=2.13\n",
      "[3999 | 866.08] loss=2.11 avg=2.13\n",
      "Saving checkpoint/m1_vulnerability/model-4000\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "  \n",
      "if (!(t->getTp() != NULL)) return ;\n",
      "if (t->getName() != NULL) return ;\n",
      "return -1;\n",
      "/*\n",
      "* return the top left value of the Tp\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t、 (sizeof(t->name) + (sizeof(t->name == NULL))) + 1)))) */\n",
      "/*\n",
      "*/\n",
      "/*\n",
      "* return a list of Tp\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t是\t\t\t\t\t\t\t\t\t\t\t\t、(sizeof(t->name) + (sizeof(t->name == NULL))) + 1) */\n",
      "/*\n",
      "*\n",
      "* If the top left value of the Tp\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t→\t→\t→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→\n",
      "*\n",
      "* if the top left value of the Tp\t\t\t\t\t\t\t\t\t\tPg\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t→\t→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→↌→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→→ ********\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000 | 887.71] validation loss = 2.22\n",
      "[4000 | 887.77] loss=3.29 avg=2.14\n",
      "[4001 | 887.83] loss=3.00 avg=2.15\n",
      "[4002 | 887.88] loss=2.24 avg=2.15\n",
      "[4003 | 887.93] loss=1.75 avg=2.15\n",
      "[4004 | 887.98] loss=1.50 avg=2.14\n",
      "[4005 | 888.04] loss=3.19 avg=2.15\n",
      "[4006 | 888.09] loss=1.70 avg=2.15\n",
      "[4007 | 888.15] loss=2.13 avg=2.15\n",
      "[4008 | 888.20] loss=2.96 avg=2.15\n",
      "[4009 | 888.26] loss=1.88 avg=2.15\n",
      "[4010 | 888.32] loss=2.27 avg=2.15\n",
      "[4011 | 888.37] loss=2.14 avg=2.15\n",
      "[4012 | 888.43] loss=1.87 avg=2.15\n",
      "[4013 | 888.48] loss=1.16 avg=2.14\n",
      "[4014 | 888.54] loss=3.08 avg=2.15\n",
      "[4015 | 888.59] loss=2.36 avg=2.15\n",
      "[4016 | 888.64] loss=2.09 avg=2.15\n",
      "[4017 | 888.70] loss=2.14 avg=2.15\n",
      "[4018 | 888.76] loss=1.78 avg=2.15\n",
      "[4019 | 888.81] loss=1.78 avg=2.14\n",
      "[4020 | 888.86] loss=1.78 avg=2.14\n",
      "[4021 | 888.92] loss=2.26 avg=2.14\n",
      "[4022 | 888.98] loss=2.41 avg=2.14\n",
      "[4023 | 889.03] loss=3.54 avg=2.16\n",
      "[4024 | 889.08] loss=2.12 avg=2.16\n",
      "[4025 | 889.14] loss=2.63 avg=2.16\n",
      "[4026 | 889.19] loss=2.56 avg=2.17\n",
      "[4027 | 889.24] loss=2.04 avg=2.16\n",
      "[4028 | 889.30] loss=2.08 avg=2.16\n",
      "[4029 | 889.35] loss=2.77 avg=2.17\n",
      "[4030 | 889.40] loss=3.09 avg=2.18\n",
      "[4031 | 889.46] loss=2.44 avg=2.18\n",
      "[4032 | 889.52] loss=1.67 avg=2.18\n",
      "[4033 | 889.58] loss=1.14 avg=2.17\n",
      "[4034 | 889.64] loss=1.12 avg=2.16\n",
      "[4035 | 889.69] loss=1.61 avg=2.15\n",
      "[4036 | 889.75] loss=2.11 avg=2.15\n",
      "[4037 | 889.81] loss=2.05 avg=2.15\n",
      "[4038 | 889.87] loss=2.92 avg=2.16\n",
      "[4039 | 889.92] loss=3.62 avg=2.17\n",
      "[4040 | 889.98] loss=1.58 avg=2.17\n",
      "[4041 | 890.04] loss=1.36 avg=2.16\n",
      "[4042 | 890.09] loss=2.21 avg=2.16\n",
      "[4043 | 890.14] loss=1.54 avg=2.15\n",
      "[4044 | 890.20] loss=2.82 avg=2.16\n",
      "[4045 | 890.26] loss=1.59 avg=2.15\n",
      "[4046 | 890.31] loss=1.30 avg=2.14\n",
      "[4047 | 890.36] loss=2.26 avg=2.15\n",
      "[4048 | 890.42] loss=0.94 avg=2.13\n",
      "[4049 | 890.47] loss=1.73 avg=2.13\n",
      "[4050 | 890.52] loss=2.90 avg=2.14\n",
      "[4051 | 890.58] loss=2.20 avg=2.14\n",
      "[4052 | 890.64] loss=2.10 avg=2.14\n",
      "[4053 | 890.69] loss=2.39 avg=2.14\n",
      "[4054 | 890.74] loss=1.64 avg=2.13\n",
      "[4055 | 890.80] loss=3.15 avg=2.14\n",
      "[4056 | 890.86] loss=2.16 avg=2.14\n",
      "[4057 | 890.91] loss=2.59 avg=2.15\n",
      "[4058 | 890.97] loss=2.52 avg=2.15\n",
      "[4059 | 891.02] loss=1.57 avg=2.15\n",
      "[4060 | 891.07] loss=2.47 avg=2.15\n",
      "[4061 | 891.13] loss=2.03 avg=2.15\n",
      "[4062 | 891.18] loss=3.48 avg=2.16\n",
      "[4063 | 891.24] loss=1.33 avg=2.15\n",
      "[4064 | 891.29] loss=1.53 avg=2.15\n",
      "[4065 | 891.35] loss=1.50 avg=2.14\n",
      "[4066 | 891.40] loss=2.24 avg=2.14\n",
      "[4067 | 891.45] loss=2.15 avg=2.14\n",
      "[4068 | 891.51] loss=3.23 avg=2.15\n",
      "[4069 | 891.57] loss=2.91 avg=2.16\n",
      "[4070 | 891.63] loss=1.88 avg=2.16\n",
      "[4071 | 891.68] loss=1.52 avg=2.15\n",
      "[4072 | 891.74] loss=2.36 avg=2.15\n",
      "[4073 | 891.79] loss=3.71 avg=2.17\n",
      "[4074 | 891.85] loss=2.85 avg=2.18\n",
      "[4075 | 891.90] loss=1.13 avg=2.17\n",
      "[4076 | 891.96] loss=1.62 avg=2.16\n",
      "[4077 | 892.02] loss=0.90 avg=2.15\n",
      "[4078 | 892.07] loss=2.10 avg=2.15\n",
      "[4079 | 892.13] loss=2.10 avg=2.15\n",
      "[4080 | 892.18] loss=3.14 avg=2.16\n",
      "[4081 | 892.24] loss=3.02 avg=2.17\n",
      "[4082 | 892.29] loss=2.90 avg=2.17\n",
      "[4083 | 892.35] loss=3.04 avg=2.18\n",
      "[4084 | 892.41] loss=0.52 avg=2.16\n",
      "[4085 | 892.46] loss=2.44 avg=2.17\n",
      "[4086 | 892.51] loss=1.65 avg=2.16\n",
      "[4087 | 892.57] loss=3.21 avg=2.17\n",
      "[4088 | 892.62] loss=1.00 avg=2.16\n",
      "[4089 | 892.68] loss=2.48 avg=2.16\n",
      "[4090 | 892.73] loss=2.91 avg=2.17\n",
      "[4091 | 892.78] loss=1.99 avg=2.17\n",
      "[4092 | 892.84] loss=1.83 avg=2.17\n",
      "[4093 | 892.89] loss=3.00 avg=2.17\n",
      "[4094 | 892.95] loss=2.06 avg=2.17\n",
      "[4095 | 893.00] loss=1.00 avg=2.16\n",
      "[4096 | 893.06] loss=1.31 avg=2.15\n",
      "[4097 | 893.12] loss=0.67 avg=2.14\n",
      "[4098 | 893.17] loss=3.51 avg=2.15\n",
      "[4099 | 893.23] loss=1.87 avg=2.15\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "_      }\n",
      "  \n",
      "  }\n",
      "}\n",
      "\n",
      "  \n",
      "/* *\n",
      "        public static void\n",
      "                  sendEvent             msg_to                                           end\n",
      "\n",
      "                                              else\n",
      "\n",
      "                                                  else\n",
      "    {                                                         \n",
      "    {                                                    }\n",
      "                    }\n",
      "   \n",
      "             }\n",
      "  \n",
      "\n",
      "    if (      }  {\n",
      "                                           \n",
      "                                 \n",
      "  \n",
      "\n",
      "        \n",
      "   \n",
      "    \n",
      "    if (!(                                     \n",
      "\n",
      "       \n",
      "     \n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      " }\n",
      "\n",
      "[\t(\"\\n\", {\n",
      "  \"/*) - {  } */\n",
      "  \" \n",
      " \n",
      "  \n",
      "  \n",
      "   }\n",
      "\n",
      "   }\n",
      "\n",
      "   \n",
      "\n",
      "  \"\n",
      "\n",
      "  \n",
      "\n",
      "  )\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "       \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "      \n",
      "\n",
      "     \n",
      "\n",
      "       \n",
      "\n",
      "  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 44.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4100 | 909.12] validation loss = 2.22\n",
      "[4100 | 909.18] loss=2.37 avg=2.15\n",
      "[4101 | 909.24] loss=2.96 avg=2.16\n",
      "[4102 | 909.29] loss=2.70 avg=2.17\n",
      "[4103 | 909.35] loss=3.08 avg=2.17\n",
      "[4104 | 909.40] loss=2.85 avg=2.18\n",
      "[4105 | 909.45] loss=2.16 avg=2.18\n",
      "[4106 | 909.51] loss=1.68 avg=2.18\n",
      "[4107 | 909.56] loss=2.29 avg=2.18\n",
      "[4108 | 909.62] loss=1.21 avg=2.17\n",
      "[4109 | 909.68] loss=1.57 avg=2.16\n",
      "[4110 | 909.73] loss=2.02 avg=2.16\n",
      "[4111 | 909.79] loss=3.83 avg=2.18\n",
      "[4112 | 909.84] loss=2.55 avg=2.18\n",
      "[4113 | 909.90] loss=2.50 avg=2.18\n",
      "[4114 | 909.95] loss=1.70 avg=2.18\n",
      "[4115 | 910.01] loss=2.45 avg=2.18\n",
      "[4116 | 910.06] loss=2.14 avg=2.18\n",
      "[4117 | 910.11] loss=2.90 avg=2.19\n",
      "[4118 | 910.17] loss=1.19 avg=2.18\n",
      "[4119 | 910.22] loss=2.14 avg=2.18\n",
      "[4120 | 910.28] loss=2.76 avg=2.18\n",
      "[4121 | 910.33] loss=1.06 avg=2.17\n",
      "[4122 | 910.39] loss=2.87 avg=2.18\n",
      "[4123 | 910.45] loss=1.47 avg=2.17\n",
      "[4124 | 910.50] loss=2.38 avg=2.17\n",
      "[4125 | 910.56] loss=2.37 avg=2.18\n",
      "[4126 | 910.61] loss=2.22 avg=2.18\n",
      "[4127 | 910.67] loss=1.83 avg=2.17\n",
      "[4128 | 910.72] loss=2.68 avg=2.18\n",
      "[4129 | 910.78] loss=2.09 avg=2.18\n",
      "[4130 | 910.83] loss=1.25 avg=2.17\n",
      "[4131 | 910.88] loss=0.34 avg=2.15\n",
      "[4132 | 910.94] loss=0.66 avg=2.14\n",
      "[4133 | 910.99] loss=2.03 avg=2.13\n",
      "[4134 | 911.04] loss=2.64 avg=2.14\n",
      "[4135 | 911.10] loss=1.64 avg=2.13\n",
      "[4136 | 911.15] loss=3.15 avg=2.14\n",
      "[4137 | 911.21] loss=2.29 avg=2.15\n",
      "[4138 | 911.26] loss=2.44 avg=2.15\n",
      "[4139 | 911.31] loss=0.97 avg=2.14\n",
      "[4140 | 911.38] loss=1.34 avg=2.13\n",
      "[4141 | 911.43] loss=1.94 avg=2.13\n",
      "[4142 | 911.49] loss=2.80 avg=2.13\n",
      "[4143 | 911.54] loss=2.76 avg=2.14\n",
      "[4144 | 911.60] loss=3.02 avg=2.15\n",
      "[4145 | 911.66] loss=2.66 avg=2.15\n",
      "[4146 | 911.71] loss=1.66 avg=2.15\n",
      "[4147 | 911.77] loss=1.02 avg=2.14\n",
      "[4148 | 911.82] loss=2.54 avg=2.14\n",
      "[4149 | 911.87] loss=2.06 avg=2.14\n",
      "[4150 | 911.93] loss=1.06 avg=2.13\n",
      "[4151 | 911.99] loss=1.35 avg=2.12\n",
      "[4152 | 912.04] loss=1.87 avg=2.12\n",
      "[4153 | 912.10] loss=3.21 avg=2.13\n",
      "[4154 | 912.16] loss=1.43 avg=2.12\n",
      "[4155 | 912.21] loss=2.83 avg=2.13\n",
      "[4156 | 912.26] loss=1.99 avg=2.13\n",
      "[4157 | 912.31] loss=2.04 avg=2.13\n",
      "[4158 | 912.37] loss=2.03 avg=2.13\n",
      "[4159 | 912.43] loss=2.09 avg=2.13\n",
      "[4160 | 912.48] loss=3.23 avg=2.14\n",
      "[4161 | 912.54] loss=1.61 avg=2.13\n",
      "[4162 | 912.59] loss=2.38 avg=2.14\n",
      "[4163 | 912.65] loss=1.67 avg=2.13\n",
      "[4164 | 912.70] loss=1.26 avg=2.12\n",
      "[4165 | 912.76] loss=2.65 avg=2.13\n",
      "[4166 | 912.82] loss=1.34 avg=2.12\n",
      "[4167 | 912.88] loss=3.07 avg=2.13\n",
      "[4168 | 912.93] loss=1.95 avg=2.13\n",
      "[4169 | 912.99] loss=2.67 avg=2.13\n",
      "[4170 | 913.04] loss=1.54 avg=2.13\n",
      "[4171 | 913.10] loss=2.71 avg=2.13\n",
      "[4172 | 913.15] loss=2.56 avg=2.14\n",
      "[4173 | 913.21] loss=2.18 avg=2.14\n",
      "[4174 | 913.26] loss=1.24 avg=2.13\n",
      "[4175 | 913.32] loss=2.40 avg=2.13\n",
      "[4176 | 913.37] loss=2.69 avg=2.14\n",
      "[4177 | 913.42] loss=1.24 avg=2.13\n",
      "[4178 | 913.48] loss=2.52 avg=2.13\n",
      "[4179 | 913.53] loss=2.79 avg=2.14\n",
      "[4180 | 913.59] loss=1.83 avg=2.13\n",
      "[4181 | 913.64] loss=1.42 avg=2.13\n",
      "[4182 | 913.70] loss=3.20 avg=2.14\n",
      "[4183 | 913.76] loss=1.78 avg=2.13\n",
      "[4184 | 913.81] loss=2.09 avg=2.13\n",
      "[4185 | 913.86] loss=1.91 avg=2.13\n",
      "[4186 | 913.91] loss=1.94 avg=2.13\n",
      "[4187 | 913.97] loss=1.94 avg=2.13\n",
      "[4188 | 914.02] loss=2.16 avg=2.13\n",
      "[4189 | 914.08] loss=2.36 avg=2.13\n",
      "[4190 | 914.13] loss=2.02 avg=2.13\n",
      "[4191 | 914.19] loss=1.94 avg=2.13\n",
      "[4192 | 914.24] loss=2.38 avg=2.13\n",
      "[4193 | 914.30] loss=2.77 avg=2.14\n",
      "[4194 | 914.35] loss=1.19 avg=2.13\n",
      "[4195 | 914.41] loss=1.87 avg=2.12\n",
      "[4196 | 914.46] loss=2.33 avg=2.13\n",
      "[4197 | 914.51] loss=2.74 avg=2.13\n",
      "[4198 | 914.57] loss=4.35 avg=2.16\n",
      "[4199 | 914.62] loss=1.14 avg=2.14\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "text\\{d}g__t\\d(\\d*g_t\\d * ix\\d ix\\d */ + g_t) {\n",
      "         g_t.end();\n",
      "    }\n",
      "     g_t_d = g_t;\n",
      "     g_t_n = 0;\n",
      "    \n",
      "   g_(t)->n != 1;\n",
      "    g_n++;\n",
      "}\n",
      "}\n",
      "/*\n",
      "#else\n",
      "/*\n",
      "#endif\n",
      "/* ************************          \n",
      "     g_t_n\\d          g_u              g_a            g_j                                           */\n",
      "/*                     1                                                      }\n",
      "/*                */\n",
      "if (((/*)) & 5 && (/*)) & 6) {\n",
      "     g_u_c = 2;\n",
      "     g_a_c = 5;\n",
      "    g_j_c = 3;\n",
      "    g_j_c = 6;\n",
      "    g_n_c = 1;\n",
      "    g_b_c = 4;\n",
      "    g_n_c = 1;\n",
      "     g_p_c = 1;\n",
      "    g_pp_c = 1;\n",
      "    g_q_c = 4;\n",
      "    g_if_c         case G_W_I_G_C :\n",
      "      g_n_c = 2;\n",
      "       g_e_c = 2;\n",
      "        g_k_c = 3;\n",
      "        g_o_c = 4;\n",
      "       g_p_c        case G_W_I_G_P_C :\n",
      "       g_n_p_c      case G_W_I_G_P_S :\n",
      "        g_p_p_c     case G_W_I_G_P_C :\n",
      "          g_pn_p_p     case G_W_I_G_P_S :\n",
      "           g_pn_pp       case G_W_I_G_P_S :\n",
      "         g_pp_p     case G_W_I_G_P_S :\n",
      "           g_f_pp      case G_W_I_G_P_S :\n",
      "            g_f_pp       case G_W_I_G_P_S :\n",
      "              g_x       case G_W_I_G_P_S :\n",
      "            g_xh      case G_W_I_G_P_S :\n",
      "                       case G_W_I_G_P_S :\n",
      " case G_W_I_G_P_S :\n",
      "                 case G_W_I_G_P->t:\n",
      " case G_W_I_G_P->t :\n",
      "           \n",
      "         case G_W_I_G_P->t_s :\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4200 | 930.69] validation loss = 2.22\n",
      "[4200 | 930.75] loss=2.41 avg=2.15\n",
      "[4201 | 930.81] loss=3.12 avg=2.16\n",
      "[4202 | 930.86] loss=1.92 avg=2.15\n",
      "[4203 | 930.92] loss=2.03 avg=2.15\n",
      "[4204 | 930.97] loss=1.96 avg=2.15\n",
      "[4205 | 931.03] loss=2.51 avg=2.16\n",
      "[4206 | 931.08] loss=1.38 avg=2.15\n",
      "[4207 | 931.14] loss=2.92 avg=2.16\n",
      "[4208 | 931.20] loss=1.52 avg=2.15\n",
      "[4209 | 931.25] loss=1.02 avg=2.14\n",
      "[4210 | 931.31] loss=1.59 avg=2.13\n",
      "[4211 | 931.36] loss=2.79 avg=2.14\n",
      "[4212 | 931.41] loss=2.60 avg=2.14\n",
      "[4213 | 931.47] loss=2.60 avg=2.15\n",
      "[4214 | 931.53] loss=1.58 avg=2.14\n",
      "[4215 | 931.58] loss=2.98 avg=2.15\n",
      "[4216 | 931.64] loss=2.09 avg=2.15\n",
      "[4217 | 931.70] loss=1.98 avg=2.15\n",
      "[4218 | 931.75] loss=2.57 avg=2.15\n",
      "[4219 | 931.81] loss=2.57 avg=2.16\n",
      "[4220 | 931.86] loss=2.68 avg=2.16\n",
      "[4221 | 931.91] loss=2.31 avg=2.16\n",
      "[4222 | 931.97] loss=1.48 avg=2.16\n",
      "[4223 | 932.02] loss=1.33 avg=2.15\n",
      "[4224 | 932.07] loss=1.43 avg=2.14\n",
      "[4225 | 932.13] loss=1.62 avg=2.14\n",
      "[4226 | 932.18] loss=2.17 avg=2.14\n",
      "[4227 | 932.24] loss=1.97 avg=2.13\n",
      "[4228 | 932.30] loss=1.82 avg=2.13\n",
      "[4229 | 932.35] loss=1.73 avg=2.13\n",
      "[4230 | 932.41] loss=2.95 avg=2.14\n",
      "[4231 | 932.46] loss=2.30 avg=2.14\n",
      "[4232 | 932.52] loss=1.34 avg=2.13\n",
      "[4233 | 932.58] loss=2.25 avg=2.13\n",
      "[4234 | 932.63] loss=2.56 avg=2.13\n",
      "[4235 | 932.69] loss=3.09 avg=2.14\n",
      "[4236 | 932.74] loss=3.09 avg=2.15\n",
      "[4237 | 932.80] loss=3.41 avg=2.17\n",
      "[4238 | 932.86] loss=1.65 avg=2.16\n",
      "[4239 | 932.92] loss=1.77 avg=2.16\n",
      "[4240 | 932.98] loss=2.23 avg=2.16\n",
      "[4241 | 933.03] loss=1.05 avg=2.15\n",
      "[4242 | 933.09] loss=2.22 avg=2.15\n",
      "[4243 | 933.14] loss=1.86 avg=2.14\n",
      "[4244 | 933.20] loss=3.10 avg=2.15\n",
      "[4245 | 933.26] loss=2.17 avg=2.15\n",
      "[4246 | 933.31] loss=3.43 avg=2.17\n",
      "[4247 | 933.36] loss=2.14 avg=2.17\n",
      "[4248 | 933.42] loss=1.57 avg=2.16\n",
      "[4249 | 933.48] loss=1.98 avg=2.16\n",
      "[4250 | 933.54] loss=2.26 avg=2.16\n",
      "[4251 | 933.59] loss=1.27 avg=2.15\n",
      "[4252 | 933.65] loss=2.22 avg=2.15\n",
      "[4253 | 933.70] loss=2.08 avg=2.15\n",
      "[4254 | 933.76] loss=4.28 avg=2.17\n",
      "[4255 | 933.81] loss=3.16 avg=2.18\n",
      "[4256 | 933.87] loss=2.16 avg=2.18\n",
      "[4257 | 933.93] loss=2.42 avg=2.18\n",
      "[4258 | 933.98] loss=2.25 avg=2.19\n",
      "[4259 | 934.04] loss=2.31 avg=2.19\n",
      "[4260 | 934.10] loss=2.89 avg=2.19\n",
      "[4261 | 934.15] loss=1.21 avg=2.18\n",
      "[4262 | 934.20] loss=1.88 avg=2.18\n",
      "[4263 | 934.26] loss=2.67 avg=2.19\n",
      "[4264 | 934.31] loss=1.44 avg=2.18\n",
      "[4265 | 934.37] loss=1.80 avg=2.17\n",
      "[4266 | 934.42] loss=1.75 avg=2.17\n",
      "[4267 | 934.48] loss=2.43 avg=2.17\n",
      "[4268 | 934.53] loss=3.04 avg=2.18\n",
      "[4269 | 934.59] loss=1.70 avg=2.18\n",
      "[4270 | 934.64] loss=2.00 avg=2.17\n",
      "[4271 | 934.70] loss=1.16 avg=2.16\n",
      "[4272 | 934.75] loss=1.93 avg=2.16\n",
      "[4273 | 934.81] loss=2.35 avg=2.16\n",
      "[4274 | 934.86] loss=2.83 avg=2.17\n",
      "[4275 | 934.92] loss=3.19 avg=2.18\n",
      "[4276 | 934.98] loss=2.22 avg=2.18\n",
      "[4277 | 935.03] loss=1.93 avg=2.18\n",
      "[4278 | 935.09] loss=1.81 avg=2.18\n",
      "[4279 | 935.14] loss=2.80 avg=2.18\n",
      "[4280 | 935.20] loss=1.59 avg=2.18\n",
      "[4281 | 935.26] loss=2.10 avg=2.17\n",
      "[4282 | 935.31] loss=1.25 avg=2.17\n",
      "[4283 | 935.37] loss=2.11 avg=2.17\n",
      "[4284 | 935.42] loss=2.17 avg=2.17\n",
      "[4285 | 935.48] loss=1.19 avg=2.16\n",
      "[4286 | 935.53] loss=1.20 avg=2.15\n",
      "[4287 | 935.59] loss=2.34 avg=2.15\n",
      "[4288 | 935.65] loss=1.61 avg=2.14\n",
      "[4289 | 935.71] loss=1.99 avg=2.14\n",
      "[4290 | 935.76] loss=2.39 avg=2.14\n",
      "[4291 | 935.82] loss=2.46 avg=2.15\n",
      "[4292 | 935.88] loss=3.27 avg=2.16\n",
      "[4293 | 935.93] loss=2.80 avg=2.16\n",
      "[4294 | 935.98] loss=2.51 avg=2.17\n",
      "[4295 | 936.04] loss=1.34 avg=2.16\n",
      "[4296 | 936.09] loss=1.35 avg=2.15\n",
      "[4297 | 936.14] loss=2.87 avg=2.16\n",
      "[4298 | 936.20] loss=2.18 avg=2.16\n",
      "[4299 | 936.25] loss=3.20 avg=2.17\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " ?           typeName_for(typeName);   \n",
      "               } catch (java.io.FileNotFoundException e) {                                                 e.printStackTrace(); }\n",
      "                                                                \n",
      "                                   }\n",
      "                                                      public void handleError(java.io.FileNotFoundException e) throws IOException {                                 case 'fail' :                                                       \n",
      "                                                         \n",
      "                                                             \n",
      "                                                    \n",
      "                                                    \n",
      "                                                     \n",
      "                                                    \n",
      "                                             \n",
      "                                           \n",
      "                                                \n",
      "                                                      \n",
      "                                                  \n",
      "                                               \n",
      "                        \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4300 | 952.35] validation loss = 2.21\n",
      "[4300 | 952.41] loss=2.21 avg=2.17\n",
      "[4301 | 952.46] loss=3.58 avg=2.18\n",
      "[4302 | 952.52] loss=2.87 avg=2.19\n",
      "[4303 | 952.57] loss=1.56 avg=2.18\n",
      "[4304 | 952.63] loss=2.25 avg=2.19\n",
      "[4305 | 952.68] loss=2.84 avg=2.19\n",
      "[4306 | 952.74] loss=1.77 avg=2.19\n",
      "[4307 | 952.79] loss=2.21 avg=2.19\n",
      "[4308 | 952.84] loss=1.47 avg=2.18\n",
      "[4309 | 952.90] loss=1.53 avg=2.17\n",
      "[4310 | 952.95] loss=2.25 avg=2.17\n",
      "[4311 | 953.01] loss=1.32 avg=2.17\n",
      "[4312 | 953.07] loss=3.06 avg=2.18\n",
      "[4313 | 953.12] loss=1.31 avg=2.17\n",
      "[4314 | 953.18] loss=1.35 avg=2.16\n",
      "[4315 | 953.23] loss=2.98 avg=2.17\n",
      "[4316 | 953.29] loss=1.30 avg=2.16\n",
      "[4317 | 953.34] loss=2.62 avg=2.16\n",
      "[4318 | 953.40] loss=2.27 avg=2.16\n",
      "[4319 | 953.46] loss=1.87 avg=2.16\n",
      "[4320 | 953.51] loss=1.75 avg=2.16\n",
      "[4321 | 953.57] loss=2.16 avg=2.16\n",
      "[4322 | 953.63] loss=2.62 avg=2.16\n",
      "[4323 | 953.68] loss=1.68 avg=2.16\n",
      "[4324 | 953.74] loss=2.85 avg=2.16\n",
      "[4325 | 953.79] loss=2.98 avg=2.17\n",
      "[4326 | 953.85] loss=2.05 avg=2.17\n",
      "[4327 | 953.91] loss=0.96 avg=2.16\n",
      "[4328 | 953.96] loss=1.74 avg=2.15\n",
      "[4329 | 954.02] loss=2.70 avg=2.16\n",
      "[4330 | 954.07] loss=3.72 avg=2.18\n",
      "[4331 | 954.13] loss=1.30 avg=2.17\n",
      "[4332 | 954.18] loss=1.42 avg=2.16\n",
      "[4333 | 954.24] loss=2.03 avg=2.16\n",
      "[4334 | 954.29] loss=2.90 avg=2.17\n",
      "[4335 | 954.35] loss=2.11 avg=2.16\n",
      "[4336 | 954.41] loss=1.32 avg=2.16\n",
      "[4337 | 954.46] loss=2.43 avg=2.16\n",
      "[4338 | 954.52] loss=2.06 avg=2.16\n",
      "[4339 | 954.57] loss=2.15 avg=2.16\n",
      "[4340 | 954.63] loss=1.28 avg=2.15\n",
      "[4341 | 954.68] loss=2.04 avg=2.15\n",
      "[4342 | 954.74] loss=2.82 avg=2.15\n",
      "[4343 | 954.80] loss=2.58 avg=2.16\n",
      "[4344 | 954.85] loss=2.20 avg=2.16\n",
      "[4345 | 954.91] loss=3.48 avg=2.17\n",
      "[4346 | 954.96] loss=2.23 avg=2.17\n",
      "[4347 | 955.01] loss=1.08 avg=2.16\n",
      "[4348 | 955.07] loss=2.31 avg=2.16\n",
      "[4349 | 955.12] loss=1.70 avg=2.16\n",
      "[4350 | 955.18] loss=2.28 avg=2.16\n",
      "[4351 | 955.23] loss=2.28 avg=2.16\n",
      "[4352 | 955.29] loss=2.62 avg=2.17\n",
      "[4353 | 955.34] loss=1.45 avg=2.16\n",
      "[4354 | 955.40] loss=2.49 avg=2.16\n",
      "[4355 | 955.45] loss=2.39 avg=2.16\n",
      "[4356 | 955.51] loss=1.43 avg=2.16\n",
      "[4357 | 955.56] loss=2.13 avg=2.16\n",
      "[4358 | 955.61] loss=2.15 avg=2.16\n",
      "[4359 | 955.67] loss=2.32 avg=2.16\n",
      "[4360 | 955.73] loss=1.84 avg=2.16\n",
      "[4361 | 955.79] loss=2.59 avg=2.16\n",
      "[4362 | 955.84] loss=1.92 avg=2.16\n",
      "[4363 | 955.90] loss=1.93 avg=2.15\n",
      "[4364 | 955.95] loss=1.35 avg=2.15\n",
      "[4365 | 956.01] loss=0.92 avg=2.13\n",
      "[4366 | 956.06] loss=2.86 avg=2.14\n",
      "[4367 | 956.11] loss=3.28 avg=2.15\n",
      "[4368 | 956.17] loss=1.85 avg=2.15\n",
      "[4369 | 956.22] loss=2.91 avg=2.16\n",
      "[4370 | 956.28] loss=2.04 avg=2.16\n",
      "[4371 | 956.33] loss=3.02 avg=2.17\n",
      "[4372 | 956.38] loss=2.24 avg=2.17\n",
      "[4373 | 956.43] loss=3.02 avg=2.17\n",
      "[4374 | 956.49] loss=1.93 avg=2.17\n",
      "[4375 | 956.54] loss=1.62 avg=2.17\n",
      "[4376 | 956.60] loss=1.52 avg=2.16\n",
      "[4377 | 956.65] loss=1.91 avg=2.16\n",
      "[4378 | 956.71] loss=2.01 avg=2.16\n",
      "[4379 | 956.76] loss=0.47 avg=2.14\n",
      "[4380 | 956.82] loss=2.27 avg=2.14\n",
      "[4381 | 956.87] loss=1.12 avg=2.13\n",
      "[4382 | 956.92] loss=2.21 avg=2.13\n",
      "[4383 | 956.98] loss=1.85 avg=2.13\n",
      "[4384 | 957.03] loss=2.88 avg=2.14\n",
      "[4385 | 957.09] loss=1.97 avg=2.13\n",
      "[4386 | 957.14] loss=2.72 avg=2.14\n",
      "[4387 | 957.20] loss=2.77 avg=2.15\n",
      "[4388 | 957.26] loss=2.81 avg=2.15\n",
      "[4389 | 957.31] loss=0.97 avg=2.14\n",
      "[4390 | 957.36] loss=2.35 avg=2.14\n",
      "[4391 | 957.42] loss=1.72 avg=2.14\n",
      "[4392 | 957.47] loss=1.59 avg=2.13\n",
      "[4393 | 957.53] loss=1.91 avg=2.13\n",
      "[4394 | 957.58] loss=0.90 avg=2.12\n",
      "[4395 | 957.63] loss=1.90 avg=2.12\n",
      "[4396 | 957.69] loss=0.85 avg=2.10\n",
      "[4397 | 957.74] loss=2.76 avg=2.11\n",
      "[4398 | 957.79] loss=3.05 avg=2.12\n",
      "[4399 | 957.85] loss=2.15 avg=2.12\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 48.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " them.\n",
      "       /*    */\n",
      "      switch (i) {\n",
      "     // * * * *\n",
      "      // * *\n",
      "case    case ;\n",
      "// *\n",
      "      while (count > 0)\n",
      "if (i)\n",
      "     {\n",
      "     \n",
      "     switch (count)\n",
      "       break\n",
      "     case ;\n",
      "     \n",
      "    \n",
      "     if (((i + 2) / i) == 0) && (!((i == i) || 0) != 0)) { /*\n",
      "  */\n",
      "     break\n",
      "      case *;\n",
      "      }\n",
      "    break\n",
      " case ?;\n",
      "    break\n",
      "// *\n",
      " case  if (((i % n) == -1) && !(i != ((i * (i)) / (i)) * (i)) || 1)) {\n",
      "      if (i) {\n",
      "         i += 1\n",
      "         break\n",
      "              if (i)\n",
      "                endif\n",
      "                   } else\n",
      "                    if (*i != (i * (i)))\n",
      "                   (if (i)\n",
      "                   else\n",
      "                                                     }\n",
      "                           \n",
      "                                   \n",
      "                               \n",
      "                           \n",
      "        }\n",
      "        else\n",
      "                       \n",
      "                       \n",
      "                  \n",
      "                   \n",
      "                 \n",
      "             \n",
      "             \n",
      "          \n",
      "          \n",
      "        \n",
      "         \n",
      "      \n",
      "      \n",
      "      \n",
      "     \n",
      "     \n",
      "     \n",
      "     \n",
      "   \n",
      "   \n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "    \n",
      "    \n",
      "    \n",
      "   }\n",
      "   else\n",
      "   \n",
      "    \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "    \n",
      "    \n",
      "    \n",
      "     \n",
      "   }\n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "    instance->addItem(function          {\n",
      "      \n",
      "       +\t[(if((i * (i)))) % (i * (i))) % (((i\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4400 | 973.80] validation loss = 2.21\n",
      "[4400 | 973.86] loss=2.02 avg=2.12\n",
      "[4401 | 973.92] loss=2.48 avg=2.12\n",
      "[4402 | 973.97] loss=2.86 avg=2.13\n",
      "[4403 | 974.03] loss=2.82 avg=2.14\n",
      "[4404 | 974.09] loss=2.09 avg=2.14\n",
      "[4405 | 974.14] loss=2.96 avg=2.14\n",
      "[4406 | 974.20] loss=1.09 avg=2.13\n",
      "[4407 | 974.25] loss=1.87 avg=2.13\n",
      "[4408 | 974.31] loss=0.73 avg=2.12\n",
      "[4409 | 974.36] loss=2.95 avg=2.13\n",
      "[4410 | 974.41] loss=3.85 avg=2.14\n",
      "[4411 | 974.47] loss=2.20 avg=2.14\n",
      "[4412 | 974.53] loss=1.91 avg=2.14\n",
      "[4413 | 974.59] loss=2.47 avg=2.14\n",
      "[4414 | 974.64] loss=2.55 avg=2.15\n",
      "[4415 | 974.70] loss=1.03 avg=2.14\n",
      "[4416 | 974.75] loss=1.81 avg=2.13\n",
      "[4417 | 974.81] loss=3.41 avg=2.15\n",
      "[4418 | 974.86] loss=2.25 avg=2.15\n",
      "[4419 | 974.92] loss=1.91 avg=2.15\n",
      "[4420 | 974.97] loss=1.19 avg=2.14\n",
      "[4421 | 975.03] loss=1.75 avg=2.13\n",
      "[4422 | 975.08] loss=2.70 avg=2.14\n",
      "[4423 | 975.14] loss=1.46 avg=2.13\n",
      "[4424 | 975.20] loss=1.44 avg=2.12\n",
      "[4425 | 975.25] loss=2.59 avg=2.13\n",
      "[4426 | 975.31] loss=1.92 avg=2.13\n",
      "[4427 | 975.37] loss=1.12 avg=2.12\n",
      "[4428 | 975.43] loss=3.52 avg=2.13\n",
      "[4429 | 975.48] loss=2.49 avg=2.13\n",
      "[4430 | 975.54] loss=2.69 avg=2.14\n",
      "[4431 | 975.59] loss=1.63 avg=2.13\n",
      "[4432 | 975.65] loss=1.18 avg=2.13\n",
      "[4433 | 975.71] loss=1.73 avg=2.12\n",
      "[4434 | 975.76] loss=1.86 avg=2.12\n",
      "[4435 | 975.81] loss=2.40 avg=2.12\n",
      "[4436 | 975.87] loss=2.71 avg=2.13\n",
      "[4437 | 975.92] loss=2.55 avg=2.13\n",
      "[4438 | 975.98] loss=1.51 avg=2.13\n",
      "[4439 | 976.03] loss=1.72 avg=2.12\n",
      "[4440 | 976.09] loss=1.97 avg=2.12\n",
      "[4441 | 976.15] loss=2.18 avg=2.12\n",
      "[4442 | 976.20] loss=1.37 avg=2.11\n",
      "[4443 | 976.25] loss=2.06 avg=2.11\n",
      "[4444 | 976.31] loss=0.84 avg=2.10\n",
      "[4445 | 976.36] loss=2.09 avg=2.10\n",
      "[4446 | 976.42] loss=2.40 avg=2.10\n",
      "[4447 | 976.47] loss=1.57 avg=2.10\n",
      "[4448 | 976.53] loss=1.76 avg=2.09\n",
      "[4449 | 976.59] loss=2.10 avg=2.09\n",
      "[4450 | 976.64] loss=1.48 avg=2.09\n",
      "[4451 | 976.70] loss=1.67 avg=2.08\n",
      "[4452 | 976.76] loss=1.57 avg=2.08\n",
      "[4453 | 976.81] loss=2.35 avg=2.08\n",
      "[4454 | 976.86] loss=1.79 avg=2.08\n",
      "[4455 | 976.92] loss=1.40 avg=2.07\n",
      "[4456 | 976.97] loss=0.84 avg=2.06\n",
      "[4457 | 977.03] loss=1.84 avg=2.06\n",
      "[4458 | 977.08] loss=1.59 avg=2.05\n",
      "[4459 | 977.14] loss=1.76 avg=2.05\n",
      "[4460 | 977.19] loss=2.16 avg=2.05\n",
      "[4461 | 977.25] loss=1.27 avg=2.04\n",
      "[4462 | 977.30] loss=2.52 avg=2.05\n",
      "[4463 | 977.36] loss=1.91 avg=2.05\n",
      "[4464 | 977.41] loss=1.00 avg=2.04\n",
      "[4465 | 977.46] loss=2.57 avg=2.04\n",
      "[4466 | 977.52] loss=0.66 avg=2.03\n",
      "[4467 | 977.57] loss=2.02 avg=2.03\n",
      "[4468 | 977.62] loss=1.68 avg=2.02\n",
      "[4469 | 977.68] loss=3.69 avg=2.04\n",
      "[4470 | 977.73] loss=2.26 avg=2.04\n",
      "[4471 | 977.79] loss=1.45 avg=2.04\n",
      "[4472 | 977.84] loss=3.27 avg=2.05\n",
      "[4473 | 977.90] loss=1.36 avg=2.04\n",
      "[4474 | 977.95] loss=1.12 avg=2.03\n",
      "[4475 | 978.01] loss=2.02 avg=2.03\n",
      "[4476 | 978.06] loss=2.30 avg=2.04\n",
      "[4477 | 978.12] loss=2.33 avg=2.04\n",
      "[4478 | 978.18] loss=2.66 avg=2.04\n",
      "[4479 | 978.23] loss=2.48 avg=2.05\n",
      "[4480 | 978.28] loss=1.37 avg=2.04\n",
      "[4481 | 978.34] loss=1.73 avg=2.04\n",
      "[4482 | 978.39] loss=1.75 avg=2.04\n",
      "[4483 | 978.45] loss=2.68 avg=2.04\n",
      "[4484 | 978.51] loss=1.90 avg=2.04\n",
      "[4485 | 978.57] loss=2.34 avg=2.04\n",
      "[4486 | 978.62] loss=2.57 avg=2.05\n",
      "[4487 | 978.67] loss=1.86 avg=2.05\n",
      "[4488 | 978.73] loss=3.32 avg=2.06\n",
      "[4489 | 978.78] loss=1.84 avg=2.06\n",
      "[4490 | 978.84] loss=1.81 avg=2.06\n",
      "[4491 | 978.89] loss=2.28 avg=2.06\n",
      "[4492 | 978.95] loss=2.60 avg=2.06\n",
      "[4493 | 979.00] loss=1.76 avg=2.06\n",
      "[4494 | 979.06] loss=1.15 avg=2.05\n",
      "[4495 | 979.11] loss=1.54 avg=2.05\n",
      "[4496 | 979.17] loss=1.61 avg=2.04\n",
      "[4497 | 979.22] loss=2.67 avg=2.05\n",
      "[4498 | 979.28] loss=2.05 avg=2.05\n",
      "[4499 | 979.34] loss=2.26 avg=2.05\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 48.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "gers' \"no longer exists\",\n",
      "         \"no longer exists\",\n",
      "          \"no longer exists\",\n",
      "          \n",
      "         \n",
      "      else\n",
      "         if (( (structure) & (structure->get())) == NULL || (structure->put(\"\")) || (structure->get(\"\") != NULL))) {\n",
      "      \n",
      "        } else\n",
      "       \n",
      "        (dazzle())\n",
      "       \n",
      "     else\n",
      "        \n",
      "       else\n",
      "        if (structure->get() == strlen(struct->get())) {\n",
      "        } else if (structure->get() == strlen(struct->get())) {\n",
      "       \n",
      "         int base = strlen(strlen(struct->get())) + 1;\n",
      "   } else if (structure->get() == strlen(struct->get())) {\n",
      "      \n",
      "         base = strlen((struct->get() * 9));\n",
      "    } else\n",
      "       if (structure->get() == strlen(struct->get)))) {\n",
      "      \n",
      "        base = strencode_to_string(strlen(structure->get(), 0));\n",
      "   } else {\n",
      "      \n",
      "     \n",
      "      \n",
      "   }\n",
      "   }\n",
      "   return 0;\n",
      "  }\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      " }\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " -p\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "      \n",
      "\n",
      "      \n",
      "\n",
      "      \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "     \n",
      "\n",
      "      \n",
      "\n",
      "     \n",
      "\n",
      "       \n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "       \n",
      "\n",
      "      \n",
      "\n",
      "          \n",
      "\n",
      "       \n",
      "       \n",
      "\n",
      "       \n",
      "\n",
      "    \n",
      "\n",
      "      \n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      " \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500 | 995.21] validation loss = 2.21\n",
      "[4500 | 995.27] loss=0.95 avg=2.04\n",
      "[4501 | 995.33] loss=1.91 avg=2.04\n",
      "[4502 | 995.38] loss=1.33 avg=2.03\n",
      "[4503 | 995.44] loss=2.78 avg=2.04\n",
      "[4504 | 995.49] loss=2.45 avg=2.04\n",
      "[4505 | 995.55] loss=2.11 avg=2.04\n",
      "[4506 | 995.60] loss=2.02 avg=2.04\n",
      "[4507 | 995.65] loss=2.98 avg=2.05\n",
      "[4508 | 995.71] loss=2.17 avg=2.05\n",
      "[4509 | 995.76] loss=1.92 avg=2.05\n",
      "[4510 | 995.82] loss=2.30 avg=2.05\n",
      "[4511 | 995.87] loss=2.55 avg=2.06\n",
      "[4512 | 995.93] loss=1.08 avg=2.05\n",
      "[4513 | 995.99] loss=2.66 avg=2.06\n",
      "[4514 | 996.05] loss=2.43 avg=2.06\n",
      "[4515 | 996.10] loss=2.40 avg=2.06\n",
      "[4516 | 996.15] loss=1.64 avg=2.06\n",
      "[4517 | 996.20] loss=3.00 avg=2.07\n",
      "[4518 | 996.26] loss=2.20 avg=2.07\n",
      "[4519 | 996.32] loss=2.67 avg=2.08\n",
      "[4520 | 996.38] loss=1.91 avg=2.07\n",
      "[4521 | 996.43] loss=2.45 avg=2.08\n",
      "[4522 | 996.49] loss=2.76 avg=2.08\n",
      "[4523 | 996.54] loss=3.01 avg=2.09\n",
      "[4524 | 996.60] loss=0.93 avg=2.08\n",
      "[4525 | 996.65] loss=2.77 avg=2.09\n",
      "[4526 | 996.70] loss=1.95 avg=2.09\n",
      "[4527 | 996.76] loss=1.72 avg=2.08\n",
      "[4528 | 996.82] loss=2.73 avg=2.09\n",
      "[4529 | 996.87] loss=2.63 avg=2.10\n",
      "[4530 | 996.93] loss=2.53 avg=2.10\n",
      "[4531 | 996.99] loss=1.64 avg=2.10\n",
      "[4532 | 997.05] loss=1.26 avg=2.09\n",
      "[4533 | 997.10] loss=1.67 avg=2.08\n",
      "[4534 | 997.16] loss=2.64 avg=2.09\n",
      "[4535 | 997.21] loss=2.46 avg=2.09\n",
      "[4536 | 997.27] loss=2.12 avg=2.09\n",
      "[4537 | 997.32] loss=0.87 avg=2.08\n",
      "[4538 | 997.38] loss=3.73 avg=2.10\n",
      "[4539 | 997.43] loss=1.61 avg=2.09\n",
      "[4540 | 997.49] loss=0.62 avg=2.08\n",
      "[4541 | 997.54] loss=2.23 avg=2.08\n",
      "[4542 | 997.60] loss=2.90 avg=2.09\n",
      "[4543 | 997.65] loss=2.06 avg=2.09\n",
      "[4544 | 997.71] loss=2.67 avg=2.09\n",
      "[4545 | 997.76] loss=2.22 avg=2.09\n",
      "[4546 | 997.81] loss=1.93 avg=2.09\n",
      "[4547 | 997.86] loss=1.29 avg=2.08\n",
      "[4548 | 997.92] loss=2.83 avg=2.09\n",
      "[4549 | 997.97] loss=1.69 avg=2.09\n",
      "[4550 | 998.03] loss=2.10 avg=2.09\n",
      "[4551 | 998.08] loss=1.99 avg=2.09\n",
      "[4552 | 998.14] loss=3.16 avg=2.10\n",
      "[4553 | 998.20] loss=1.21 avg=2.09\n",
      "[4554 | 998.25] loss=1.58 avg=2.08\n",
      "[4555 | 998.31] loss=2.74 avg=2.09\n",
      "[4556 | 998.37] loss=1.63 avg=2.09\n",
      "[4557 | 998.42] loss=0.53 avg=2.07\n",
      "[4558 | 998.48] loss=2.79 avg=2.08\n",
      "[4559 | 998.53] loss=3.28 avg=2.09\n",
      "[4560 | 998.59] loss=2.69 avg=2.10\n",
      "[4561 | 998.64] loss=2.63 avg=2.10\n",
      "[4562 | 998.70] loss=1.43 avg=2.09\n",
      "[4563 | 998.76] loss=1.72 avg=2.09\n",
      "[4564 | 998.81] loss=2.16 avg=2.09\n",
      "[4565 | 998.86] loss=2.96 avg=2.10\n",
      "[4566 | 998.92] loss=2.90 avg=2.11\n",
      "[4567 | 998.97] loss=1.82 avg=2.10\n",
      "[4568 | 999.03] loss=2.41 avg=2.11\n",
      "[4569 | 999.09] loss=1.24 avg=2.10\n",
      "[4570 | 999.15] loss=2.09 avg=2.10\n",
      "[4571 | 999.20] loss=0.94 avg=2.09\n",
      "[4572 | 999.26] loss=1.38 avg=2.08\n",
      "[4573 | 999.32] loss=1.71 avg=2.08\n",
      "[4574 | 999.37] loss=3.00 avg=2.09\n",
      "[4575 | 999.43] loss=1.86 avg=2.08\n",
      "[4576 | 999.48] loss=1.70 avg=2.08\n",
      "[4577 | 999.54] loss=1.92 avg=2.08\n",
      "[4578 | 999.59] loss=2.47 avg=2.08\n",
      "[4579 | 999.65] loss=1.52 avg=2.08\n",
      "[4580 | 999.71] loss=2.25 avg=2.08\n",
      "[4581 | 999.76] loss=1.49 avg=2.07\n",
      "[4582 | 999.82] loss=2.14 avg=2.07\n",
      "[4583 | 999.87] loss=2.89 avg=2.08\n",
      "[4584 | 999.93] loss=3.35 avg=2.09\n",
      "[4585 | 999.99] loss=1.76 avg=2.09\n",
      "[4586 | 1000.04] loss=1.97 avg=2.09\n",
      "[4587 | 1000.10] loss=1.56 avg=2.08\n",
      "[4588 | 1000.15] loss=2.30 avg=2.09\n",
      "[4589 | 1000.21] loss=3.04 avg=2.10\n",
      "[4590 | 1000.26] loss=2.13 avg=2.10\n",
      "[4591 | 1000.32] loss=3.08 avg=2.11\n",
      "[4592 | 1000.37] loss=3.35 avg=2.12\n",
      "[4593 | 1000.43] loss=2.09 avg=2.12\n",
      "[4594 | 1000.48] loss=1.76 avg=2.11\n",
      "[4595 | 1000.54] loss=0.83 avg=2.10\n",
      "[4596 | 1000.59] loss=1.91 avg=2.10\n",
      "[4597 | 1000.65] loss=3.09 avg=2.11\n",
      "[4598 | 1000.70] loss=2.62 avg=2.11\n",
      "[4599 | 1000.76] loss=1.96 avg=2.11\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " VALT_SHIFT)++;\n",
      "                                      \n",
      "                                           jfree = jfree_new (0, 100);\n",
      "                                          \n",
      "                                       \n",
      "                                   \n",
      "                      }else\n",
      "                                  jfree = jfree_new (jfree_size_narrowwise_inflate_size (jfree_size_size_n, 1000);\n",
      "                     \n",
      "                      \n",
      "                      \n",
      "               }\n",
      "                  public void jfree(jfree_size_narrowwise_inflate_size_n) {\n",
      "                          \n",
      "                        \n",
      "                        \n",
      "                       \n",
      "                     \n",
      "                     \n",
      "          }\n",
      "            \n",
      "   }\n",
      "    ;\n",
      "    /* jfree, jfree, jfree */\n",
      "   /* break */\n",
      "   break;\n",
      "  \n",
      "   break;\n",
      "   -(jfree_free_size);\n",
      "   if (jfree_inflate_size_n) {\n",
      "     break;\n",
      "    }\n",
      "   if (jfree_size_n) {\n",
      "     break;  \n",
      "   break;\n",
      "   break;\n",
      "  \n",
      "   jfree = 9;\n",
      "  \n",
      "   if ((jfree_size_n) > 9) {\n",
      "      break |= 9;\n",
      "   \n",
      "   } else {\n",
      "    \n",
      "    if ((jfree_size_n) > 9) {\n",
      "      break;    \n",
      "           } else {\n",
      "      \n",
      "                  \n",
      "                 \n",
      "                      \n",
      "                       \n",
      "                  \n",
      "         } else {\n",
      "       \n",
      "                 \n",
      "                } else {\n",
      "       \n",
      "                  \n",
      "       \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4600 | 1016.64] validation loss = 2.20\n",
      "[4600 | 1016.69] loss=3.13 avg=2.12\n",
      "[4601 | 1016.75] loss=3.30 avg=2.14\n",
      "[4602 | 1016.81] loss=2.19 avg=2.14\n",
      "[4603 | 1016.87] loss=2.14 avg=2.14\n",
      "[4604 | 1016.92] loss=2.52 avg=2.14\n",
      "[4605 | 1016.98] loss=1.99 avg=2.14\n",
      "[4606 | 1017.03] loss=2.22 avg=2.14\n",
      "[4607 | 1017.09] loss=1.86 avg=2.14\n",
      "[4608 | 1017.14] loss=2.38 avg=2.14\n",
      "[4609 | 1017.20] loss=2.35 avg=2.14\n",
      "[4610 | 1017.26] loss=1.47 avg=2.13\n",
      "[4611 | 1017.31] loss=1.02 avg=2.12\n",
      "[4612 | 1017.37] loss=1.43 avg=2.12\n",
      "[4613 | 1017.42] loss=2.87 avg=2.12\n",
      "[4614 | 1017.47] loss=1.74 avg=2.12\n",
      "[4615 | 1017.53] loss=1.53 avg=2.11\n",
      "[4616 | 1017.59] loss=1.92 avg=2.11\n",
      "[4617 | 1017.64] loss=2.56 avg=2.12\n",
      "[4618 | 1017.69] loss=1.80 avg=2.11\n",
      "[4619 | 1017.74] loss=2.36 avg=2.12\n",
      "[4620 | 1017.80] loss=2.07 avg=2.12\n",
      "[4621 | 1017.86] loss=2.19 avg=2.12\n",
      "[4622 | 1017.91] loss=0.67 avg=2.10\n",
      "[4623 | 1017.97] loss=2.91 avg=2.11\n",
      "[4624 | 1018.02] loss=1.31 avg=2.10\n",
      "[4625 | 1018.08] loss=1.63 avg=2.10\n",
      "[4626 | 1018.13] loss=1.95 avg=2.10\n",
      "[4627 | 1018.19] loss=2.10 avg=2.10\n",
      "[4628 | 1018.24] loss=3.07 avg=2.11\n",
      "[4629 | 1018.30] loss=3.60 avg=2.12\n",
      "[4630 | 1018.35] loss=1.40 avg=2.11\n",
      "[4631 | 1018.41] loss=2.70 avg=2.12\n",
      "[4632 | 1018.46] loss=1.22 avg=2.11\n",
      "[4633 | 1018.52] loss=2.15 avg=2.11\n",
      "[4634 | 1018.57] loss=2.78 avg=2.12\n",
      "[4635 | 1018.62] loss=0.89 avg=2.10\n",
      "[4636 | 1018.68] loss=2.18 avg=2.11\n",
      "[4637 | 1018.73] loss=1.88 avg=2.10\n",
      "[4638 | 1018.79] loss=2.76 avg=2.11\n",
      "[4639 | 1018.84] loss=2.56 avg=2.11\n",
      "[4640 | 1018.90] loss=4.93 avg=2.14\n",
      "[4641 | 1018.95] loss=3.40 avg=2.15\n",
      "[4642 | 1019.01] loss=0.40 avg=2.14\n",
      "[4643 | 1019.07] loss=2.75 avg=2.14\n",
      "[4644 | 1019.12] loss=1.40 avg=2.14\n",
      "[4645 | 1019.17] loss=2.53 avg=2.14\n",
      "[4646 | 1019.23] loss=3.33 avg=2.15\n",
      "[4647 | 1019.28] loss=1.81 avg=2.15\n",
      "[4648 | 1019.34] loss=2.11 avg=2.15\n",
      "[4649 | 1019.39] loss=2.09 avg=2.15\n",
      "[4650 | 1019.45] loss=1.49 avg=2.14\n",
      "[4651 | 1019.50] loss=2.57 avg=2.15\n",
      "[4652 | 1019.56] loss=1.67 avg=2.14\n",
      "[4653 | 1019.62] loss=2.55 avg=2.14\n",
      "[4654 | 1019.68] loss=2.86 avg=2.15\n",
      "[4655 | 1019.74] loss=1.18 avg=2.14\n",
      "[4656 | 1019.79] loss=1.65 avg=2.14\n",
      "[4657 | 1019.84] loss=1.95 avg=2.14\n",
      "[4658 | 1019.90] loss=2.42 avg=2.14\n",
      "[4659 | 1019.95] loss=1.72 avg=2.13\n",
      "[4660 | 1020.01] loss=2.11 avg=2.13\n",
      "[4661 | 1020.07] loss=1.85 avg=2.13\n",
      "[4662 | 1020.12] loss=1.87 avg=2.13\n",
      "[4663 | 1020.17] loss=0.79 avg=2.11\n",
      "[4664 | 1020.23] loss=2.27 avg=2.12\n",
      "[4665 | 1020.29] loss=2.84 avg=2.12\n",
      "[4666 | 1020.34] loss=2.61 avg=2.13\n",
      "[4667 | 1020.40] loss=1.14 avg=2.12\n",
      "[4668 | 1020.46] loss=2.48 avg=2.12\n",
      "[4669 | 1020.52] loss=1.69 avg=2.12\n",
      "[4670 | 1020.57] loss=2.47 avg=2.12\n",
      "[4671 | 1020.63] loss=2.19 avg=2.12\n",
      "[4672 | 1020.69] loss=2.92 avg=2.13\n",
      "[4673 | 1020.74] loss=2.08 avg=2.13\n",
      "[4674 | 1020.79] loss=1.12 avg=2.12\n",
      "[4675 | 1020.85] loss=1.93 avg=2.12\n",
      "[4676 | 1020.90] loss=3.41 avg=2.13\n",
      "[4677 | 1020.96] loss=2.04 avg=2.13\n",
      "[4678 | 1021.01] loss=3.15 avg=2.14\n",
      "[4679 | 1021.07] loss=2.22 avg=2.14\n",
      "[4680 | 1021.13] loss=1.92 avg=2.14\n",
      "[4681 | 1021.18] loss=1.16 avg=2.13\n",
      "[4682 | 1021.24] loss=2.35 avg=2.13\n",
      "[4683 | 1021.29] loss=2.34 avg=2.13\n",
      "[4684 | 1021.35] loss=1.21 avg=2.12\n",
      "[4685 | 1021.40] loss=1.71 avg=2.12\n",
      "[4686 | 1021.46] loss=1.71 avg=2.12\n",
      "[4687 | 1021.51] loss=1.82 avg=2.11\n",
      "[4688 | 1021.57] loss=2.22 avg=2.11\n",
      "[4689 | 1021.62] loss=1.75 avg=2.11\n",
      "[4690 | 1021.68] loss=0.94 avg=2.10\n",
      "[4691 | 1021.73] loss=1.82 avg=2.10\n",
      "[4692 | 1021.79] loss=1.61 avg=2.09\n",
      "[4693 | 1021.85] loss=1.59 avg=2.09\n",
      "[4694 | 1021.90] loss=2.01 avg=2.08\n",
      "[4695 | 1021.95] loss=2.04 avg=2.08\n",
      "[4696 | 1022.01] loss=1.64 avg=2.08\n",
      "[4697 | 1022.07] loss=2.70 avg=2.09\n",
      "[4698 | 1022.12] loss=3.15 avg=2.10\n",
      "[4699 | 1022.17] loss=1.30 avg=2.09\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " ++\n",
      "                 long lastTime = (lat, long) - (lat / (lat)-- - 1);\n",
      "                long startTime = 0 - lat * startTime;\n",
      "               long stopTime = 0 - lat * stopTime;\n",
      "              for (int ii = 0; ii < (lat); ++i) {\n",
      "                          long offset = 0;\n",
      "                               long  = (long), 0;\n",
      "                               long delta = (delta / (lat)--);\n",
      "                                 long end = (end)--;\n",
      "                             }\n",
      "                            if (                        left = (lat, long));\n",
      "                           else\n",
      "                                return ;\n",
      "                     \n",
      "                            return ;\n",
      "                            long end = (end)--;\n",
      "                             else\n",
      "                 } */\n",
      "                     \n",
      "               }\n",
      "            }\n",
      "              }\n",
      "       \n",
      "      \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "      \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "       \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "    \n",
      "    \n",
      "     \n",
      "     \n",
      "    \n",
      "    \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "   \n",
      "    (if (strcmp(date, (long) + (lat) / (long)).size() / (lat)--))++;\n",
      "    \n",
      "   \n",
      "    \n",
      "   \n",
      "   \n",
      "   \n",
      "    return ;\n",
      "   \n",
      "    }\n",
      "    \n",
      "    \n",
      "    return ;\n",
      "} else\n",
      "     \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      "   \n",
      "    \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4700 | 1038.18] validation loss = 2.20\n",
      "[4700 | 1038.24] loss=2.63 avg=2.09\n",
      "[4701 | 1038.30] loss=2.34 avg=2.10\n",
      "[4702 | 1038.35] loss=2.27 avg=2.10\n",
      "[4703 | 1038.41] loss=1.12 avg=2.09\n",
      "[4704 | 1038.46] loss=2.44 avg=2.09\n",
      "[4705 | 1038.52] loss=1.99 avg=2.09\n",
      "[4706 | 1038.57] loss=2.02 avg=2.09\n",
      "[4707 | 1038.63] loss=2.57 avg=2.10\n",
      "[4708 | 1038.69] loss=2.98 avg=2.10\n",
      "[4709 | 1038.74] loss=1.89 avg=2.10\n",
      "[4710 | 1038.79] loss=2.03 avg=2.10\n",
      "[4711 | 1038.85] loss=2.53 avg=2.11\n",
      "[4712 | 1038.90] loss=1.40 avg=2.10\n",
      "[4713 | 1038.96] loss=2.75 avg=2.11\n",
      "[4714 | 1039.01] loss=1.08 avg=2.09\n",
      "[4715 | 1039.07] loss=2.97 avg=2.10\n",
      "[4716 | 1039.12] loss=1.22 avg=2.09\n",
      "[4717 | 1039.18] loss=1.36 avg=2.09\n",
      "[4718 | 1039.23] loss=2.52 avg=2.09\n",
      "[4719 | 1039.29] loss=2.68 avg=2.10\n",
      "[4720 | 1039.34] loss=2.40 avg=2.10\n",
      "[4721 | 1039.40] loss=1.08 avg=2.09\n",
      "[4722 | 1039.46] loss=1.41 avg=2.08\n",
      "[4723 | 1039.51] loss=1.90 avg=2.08\n",
      "[4724 | 1039.57] loss=1.70 avg=2.08\n",
      "[4725 | 1039.63] loss=1.96 avg=2.08\n",
      "[4726 | 1039.68] loss=2.11 avg=2.08\n",
      "[4727 | 1039.74] loss=1.38 avg=2.07\n",
      "[4728 | 1039.80] loss=3.67 avg=2.09\n",
      "[4729 | 1039.85] loss=2.09 avg=2.09\n",
      "[4730 | 1039.91] loss=3.28 avg=2.10\n",
      "[4731 | 1039.97] loss=1.63 avg=2.09\n",
      "[4732 | 1040.02] loss=1.45 avg=2.09\n",
      "[4733 | 1040.08] loss=1.88 avg=2.08\n",
      "[4734 | 1040.13] loss=2.24 avg=2.09\n",
      "[4735 | 1040.18] loss=2.56 avg=2.09\n",
      "[4736 | 1040.24] loss=1.92 avg=2.09\n",
      "[4737 | 1040.30] loss=1.88 avg=2.09\n",
      "[4738 | 1040.35] loss=1.74 avg=2.08\n",
      "[4739 | 1040.41] loss=1.66 avg=2.08\n",
      "[4740 | 1040.47] loss=1.68 avg=2.08\n",
      "[4741 | 1040.52] loss=1.88 avg=2.07\n",
      "[4742 | 1040.57] loss=2.83 avg=2.08\n",
      "[4743 | 1040.63] loss=0.87 avg=2.07\n",
      "[4744 | 1040.69] loss=1.87 avg=2.07\n",
      "[4745 | 1040.74] loss=1.13 avg=2.06\n",
      "[4746 | 1040.79] loss=1.61 avg=2.05\n",
      "[4747 | 1040.85] loss=1.61 avg=2.05\n",
      "[4748 | 1040.90] loss=1.93 avg=2.05\n",
      "[4749 | 1040.96] loss=2.96 avg=2.06\n",
      "[4750 | 1041.01] loss=3.07 avg=2.07\n",
      "[4751 | 1041.07] loss=2.71 avg=2.07\n",
      "[4752 | 1041.12] loss=2.53 avg=2.08\n",
      "[4753 | 1041.18] loss=3.10 avg=2.09\n",
      "[4754 | 1041.24] loss=1.43 avg=2.08\n",
      "[4755 | 1041.29] loss=3.20 avg=2.09\n",
      "[4756 | 1041.35] loss=1.21 avg=2.08\n",
      "[4757 | 1041.40] loss=3.39 avg=2.10\n",
      "[4758 | 1041.46] loss=3.18 avg=2.11\n",
      "[4759 | 1041.52] loss=2.39 avg=2.11\n",
      "[4760 | 1041.57] loss=1.64 avg=2.11\n",
      "[4761 | 1041.63] loss=3.00 avg=2.11\n",
      "[4762 | 1041.68] loss=1.57 avg=2.11\n",
      "[4763 | 1041.74] loss=1.00 avg=2.10\n",
      "[4764 | 1041.79] loss=0.90 avg=2.09\n",
      "[4765 | 1041.85] loss=1.82 avg=2.08\n",
      "[4766 | 1041.91] loss=2.12 avg=2.08\n",
      "[4767 | 1041.96] loss=2.46 avg=2.09\n",
      "[4768 | 1042.02] loss=2.38 avg=2.09\n",
      "[4769 | 1042.08] loss=3.15 avg=2.10\n",
      "[4770 | 1042.14] loss=2.11 avg=2.10\n",
      "[4771 | 1042.19] loss=1.97 avg=2.10\n",
      "[4772 | 1042.25] loss=1.67 avg=2.10\n",
      "[4773 | 1042.30] loss=1.86 avg=2.09\n",
      "[4774 | 1042.36] loss=1.77 avg=2.09\n",
      "[4775 | 1042.41] loss=1.26 avg=2.08\n",
      "[4776 | 1042.47] loss=0.19 avg=2.06\n",
      "[4777 | 1042.52] loss=2.18 avg=2.06\n",
      "[4778 | 1042.58] loss=1.80 avg=2.06\n",
      "[4779 | 1042.63] loss=1.61 avg=2.06\n",
      "[4780 | 1042.69] loss=2.40 avg=2.06\n",
      "[4781 | 1042.74] loss=2.29 avg=2.06\n",
      "[4782 | 1042.80] loss=2.83 avg=2.07\n",
      "[4783 | 1042.85] loss=3.42 avg=2.08\n",
      "[4784 | 1042.91] loss=1.47 avg=2.08\n",
      "[4785 | 1042.97] loss=1.52 avg=2.07\n",
      "[4786 | 1043.02] loss=1.38 avg=2.07\n",
      "[4787 | 1043.08] loss=2.62 avg=2.07\n",
      "[4788 | 1043.14] loss=2.31 avg=2.07\n",
      "[4789 | 1043.19] loss=2.28 avg=2.08\n",
      "[4790 | 1043.25] loss=2.15 avg=2.08\n",
      "[4791 | 1043.30] loss=2.02 avg=2.08\n",
      "[4792 | 1043.36] loss=2.26 avg=2.08\n",
      "[4793 | 1043.41] loss=2.79 avg=2.08\n",
      "[4794 | 1043.47] loss=1.41 avg=2.08\n",
      "[4795 | 1043.52] loss=2.40 avg=2.08\n",
      "[4796 | 1043.58] loss=1.34 avg=2.07\n",
      "[4797 | 1043.63] loss=2.77 avg=2.08\n",
      "[4798 | 1043.69] loss=2.40 avg=2.08\n",
      "[4799 | 1043.74] loss=2.56 avg=2.09\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " NULLs[i] = i + 1; break;\n",
      "   \n",
      "  \n",
      "     for (i = 0; i < num_mall_data_cores; i++) {\n",
      "        switch (num_mall_data_cores[i] ||  num_mall_data_cores[i].value) {\n",
      "                   break;\n",
      "                    case \"M10: \", NULLs[i] = i; break;\n",
      "             case \"M14 : \", NULLs[i] = i; break;\n",
      "            case \"M17 : \", NULLs[i] = i; break;\n",
      "            case \"M14: \", \"M16\" = NULLs[i]; break;\n",
      "            case \"M2: \", NULLs[i] = i; break;\n",
      "         case \"M2: \", \".value\"; case \"M10 : \", \".value\"; case \"M14: \", \"M10\";\n",
      "         case \"M17: \", \".value\"; default:\n",
      "                   delete (num_mall_data_cores[i].value);\n",
      "      case \"M1: \", NULLs[i] = i; break;\n",
      "       case \"M1: \", \"\";\n",
      "     case case case\n",
      "         case java.lang.String str = str.replace(/\\0', '%u','%c'));\n",
      "      else\n",
      "                    delete (str);\n",
      "            case \"M1: \", \"\";\n",
      "    }\n",
      "   \n",
      "    case null\n",
      "      case null:\n",
      "            case null:\n",
      "            case null:\n",
      "       )\n",
      "           case null:\n",
      "              case null:\n",
      "             case null :\n",
      "             case null :\n",
      "           case null - i = 0; while (!(num_mall_data_cores[i] != 0))) {\n",
      "                   delete (num_mall_data_cores[i].value);\n",
      "                delete (num_mall_data_cores[i].value);\n",
      "             case null:\n",
      "               case null :\n",
      "               case null :\n",
      "               case null :\n",
      "             case null @if (num_mall_data_cores[i] ||  num_mall_data_cores[i].value) {\n",
      "               delete (num_mall_data_cores[i].value);\n",
      "             case null - i = 0; while ((num_mall_data_cores[i] != 0) {\n",
      "                delete (num_mall_data_cores[i].value);\n",
      "               case null - i = 0; while ((num_mall_data_cores[i] != 0) {\n",
      "             delete (num_mall_data_cores[i].value);\n",
      "             //if\n",
      "             case null -\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 47.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4800 | 1059.69] validation loss = 2.20\n",
      "[4800 | 1059.75] loss=2.55 avg=2.09\n",
      "[4801 | 1059.81] loss=2.32 avg=2.10\n",
      "[4802 | 1059.87] loss=1.64 avg=2.09\n",
      "[4803 | 1059.93] loss=2.37 avg=2.09\n",
      "[4804 | 1059.99] loss=2.88 avg=2.10\n",
      "[4805 | 1060.04] loss=2.64 avg=2.11\n",
      "[4806 | 1060.10] loss=1.20 avg=2.10\n",
      "[4807 | 1060.15] loss=2.12 avg=2.10\n",
      "[4808 | 1060.21] loss=2.73 avg=2.10\n",
      "[4809 | 1060.26] loss=1.58 avg=2.10\n",
      "[4810 | 1060.31] loss=2.15 avg=2.10\n",
      "[4811 | 1060.37] loss=2.08 avg=2.10\n",
      "[4812 | 1060.43] loss=0.79 avg=2.09\n",
      "[4813 | 1060.49] loss=1.34 avg=2.08\n",
      "[4814 | 1060.54] loss=2.39 avg=2.08\n",
      "[4815 | 1060.59] loss=2.29 avg=2.08\n",
      "[4816 | 1060.65] loss=1.37 avg=2.08\n",
      "[4817 | 1060.71] loss=2.73 avg=2.08\n",
      "[4818 | 1060.76] loss=3.00 avg=2.09\n",
      "[4819 | 1060.82] loss=1.41 avg=2.09\n",
      "[4820 | 1060.88] loss=1.72 avg=2.08\n",
      "[4821 | 1060.93] loss=2.45 avg=2.09\n",
      "[4822 | 1060.99] loss=1.90 avg=2.08\n",
      "[4823 | 1061.05] loss=2.00 avg=2.08\n",
      "[4824 | 1061.10] loss=1.49 avg=2.08\n",
      "[4825 | 1061.15] loss=1.69 avg=2.07\n",
      "[4826 | 1061.21] loss=1.37 avg=2.07\n",
      "[4827 | 1061.26] loss=1.49 avg=2.06\n",
      "[4828 | 1061.32] loss=2.03 avg=2.06\n",
      "[4829 | 1061.37] loss=2.22 avg=2.06\n",
      "[4830 | 1061.43] loss=1.03 avg=2.05\n",
      "[4831 | 1061.49] loss=0.77 avg=2.04\n",
      "[4832 | 1061.54] loss=3.71 avg=2.06\n",
      "[4833 | 1061.60] loss=1.91 avg=2.05\n",
      "[4834 | 1061.65] loss=2.42 avg=2.06\n",
      "[4835 | 1061.71] loss=0.77 avg=2.04\n",
      "[4836 | 1061.77] loss=1.99 avg=2.04\n",
      "[4837 | 1061.82] loss=2.50 avg=2.05\n",
      "[4838 | 1061.88] loss=2.36 avg=2.05\n",
      "[4839 | 1061.93] loss=1.63 avg=2.05\n",
      "[4840 | 1061.99] loss=1.85 avg=2.05\n",
      "[4841 | 1062.04] loss=1.13 avg=2.04\n",
      "[4842 | 1062.10] loss=2.32 avg=2.04\n",
      "[4843 | 1062.15] loss=2.02 avg=2.04\n",
      "[4844 | 1062.21] loss=1.87 avg=2.04\n",
      "[4845 | 1062.27] loss=2.30 avg=2.04\n",
      "[4846 | 1062.32] loss=1.50 avg=2.03\n",
      "[4847 | 1062.38] loss=2.67 avg=2.04\n",
      "[4848 | 1062.43] loss=3.05 avg=2.05\n",
      "[4849 | 1062.49] loss=1.73 avg=2.05\n",
      "[4850 | 1062.55] loss=2.93 avg=2.06\n",
      "[4851 | 1062.60] loss=2.79 avg=2.06\n",
      "[4852 | 1062.66] loss=1.63 avg=2.06\n",
      "[4853 | 1062.72] loss=1.86 avg=2.06\n",
      "[4854 | 1062.77] loss=2.80 avg=2.07\n",
      "[4855 | 1062.83] loss=2.94 avg=2.07\n",
      "[4856 | 1062.88] loss=1.71 avg=2.07\n",
      "[4857 | 1062.94] loss=3.02 avg=2.08\n",
      "[4858 | 1062.99] loss=2.59 avg=2.08\n",
      "[4859 | 1063.05] loss=1.87 avg=2.08\n",
      "[4860 | 1063.11] loss=1.54 avg=2.08\n",
      "[4861 | 1063.16] loss=3.56 avg=2.09\n",
      "[4862 | 1063.22] loss=2.75 avg=2.10\n",
      "[4863 | 1063.28] loss=2.69 avg=2.10\n",
      "[4864 | 1063.33] loss=2.01 avg=2.10\n",
      "[4865 | 1063.39] loss=2.97 avg=2.11\n",
      "[4866 | 1063.44] loss=1.98 avg=2.11\n",
      "[4867 | 1063.50] loss=1.80 avg=2.11\n",
      "[4868 | 1063.56] loss=2.16 avg=2.11\n",
      "[4869 | 1063.61] loss=1.67 avg=2.10\n",
      "[4870 | 1063.67] loss=1.05 avg=2.09\n",
      "[4871 | 1063.72] loss=2.54 avg=2.10\n",
      "[4872 | 1063.78] loss=2.27 avg=2.10\n",
      "[4873 | 1063.83] loss=2.44 avg=2.10\n",
      "[4874 | 1063.89] loss=2.12 avg=2.10\n",
      "[4875 | 1063.95] loss=1.71 avg=2.10\n",
      "[4876 | 1064.00] loss=2.71 avg=2.11\n",
      "[4877 | 1064.06] loss=2.69 avg=2.11\n",
      "[4878 | 1064.11] loss=3.04 avg=2.12\n",
      "[4879 | 1064.17] loss=2.24 avg=2.12\n",
      "[4880 | 1064.22] loss=1.35 avg=2.11\n",
      "[4881 | 1064.28] loss=3.05 avg=2.12\n",
      "[4882 | 1064.33] loss=1.52 avg=2.12\n",
      "[4883 | 1064.39] loss=2.72 avg=2.12\n",
      "[4884 | 1064.44] loss=2.07 avg=2.12\n",
      "[4885 | 1064.49] loss=3.09 avg=2.13\n",
      "[4886 | 1064.55] loss=1.33 avg=2.12\n",
      "[4887 | 1064.61] loss=1.51 avg=2.12\n",
      "[4888 | 1064.66] loss=3.39 avg=2.13\n",
      "[4889 | 1064.72] loss=2.12 avg=2.13\n",
      "[4890 | 1064.77] loss=2.67 avg=2.14\n",
      "[4891 | 1064.82] loss=1.89 avg=2.13\n",
      "[4892 | 1064.88] loss=2.26 avg=2.13\n",
      "[4893 | 1064.93] loss=1.20 avg=2.13\n",
      "[4894 | 1064.99] loss=1.30 avg=2.12\n",
      "[4895 | 1065.04] loss=2.46 avg=2.12\n",
      "[4896 | 1065.10] loss=1.47 avg=2.11\n",
      "[4897 | 1065.16] loss=2.61 avg=2.12\n",
      "[4898 | 1065.21] loss=1.61 avg=2.11\n",
      "[4899 | 1065.26] loss=1.52 avg=2.11\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 44.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      ");\n",
      "    android.widget.LayoutParamsLayoutParamsLayoutParamsLayoutParamsLayoutParamsLayoutParams layoutParams = null;\n",
      "   m_id = m_m_i_mf_n_n_n_m_d_d_d_n_i_m_f_n_n_m_n_m_n_m_b_w_w_w_c_w_c_w_c_w_c_w_c_m_f_n_b_b_b_w_a_k_d_c_c_c_c_d_d_d_u_s_s_s_w_i_n_n_m_n_m_n_m_n_n_n_n_m_f_n_n_n_m_n_r_r_r_r_r__r_r_r_r_r_r_r_r_r_r_t_t_t_t_t_t_t_t_)\n",
      "    return n_id;\n",
      "    android.widget.LayoutParamsLayoutParamsLayoutParamslayoutParamsLayoutParams layoutParams = {\n",
      "       __final android.widget.LayoutParamsLayoutParams {\n",
      "      layout = new android.widget.Layout(android.widget.LayoutParamsLayoutParams.PAD_ALL_PAD_ARGS_FOR_S_N_S_N_N_BOW_S_S_C_S_C_C_N_N_G_S_G_S_B_B_B_B_B_B_S_L_L_L_S_R_R_L_R_R_S_N_S_N_B_B_V_V_L_V_B_N_V_N_V_B_S_S__N_M_M_L_L_L_M_S_M_L_M_Y_Y_Y_M_T_T_Y_Y_Y_Y_M_S_S_N_S_N_M_M_L_L_L_M_N_M_M_U_L_M_L_       _f_m =        if ((m_id != 0 ) || (m_id < 0 ) || ((m_id < 0 ) || ((m_id < 0 )) || ((m_id < 0 ))) || ((m_id < 0 _n_n_d_m_d_m_d_n_n_n_d_n_m_d_d_n_u_s_m_f_n_n_b_b_w_w_c_d_d_u_s_w_c_w_c_c_f_l_st_t_u_s_p_p_i_d_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_r_    _r_r = n_id;\n",
      "       _s_m = _m_m_m_n_m_n_m_n_m_n_m_n_m_f_n_n_n_n_s_s_S_s_C_C_C_N_S_L_L_S_G_S_G_S_S_G_S_B_B_B_B_B_B_B_B_B_B_A_B_A_B_A_B_A_K_K_S_S_S_N_S_N_S_N_M_M_M_M_M_M_V_L_L_L_L_L_S_G_S_G_S_B_B_B_B_B_B_B_B_L_L_L_L_S_G_F_F_R_R_R_R_R_R_R_r_r_r_r      }\n",
      "   if ((m_id != 0 ) || (m_id < 0)) || ((m_id < 0 _N_n_n_n_\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4900 | 1081.28] validation loss = 2.19\n",
      "[4900 | 1081.34] loss=2.48 avg=2.11\n",
      "[4901 | 1081.40] loss=2.21 avg=2.11\n",
      "[4902 | 1081.45] loss=2.16 avg=2.11\n",
      "[4903 | 1081.51] loss=2.30 avg=2.12\n",
      "[4904 | 1081.56] loss=2.35 avg=2.12\n",
      "[4905 | 1081.62] loss=2.13 avg=2.12\n",
      "[4906 | 1081.67] loss=2.36 avg=2.12\n",
      "[4907 | 1081.72] loss=2.70 avg=2.13\n",
      "[4908 | 1081.78] loss=1.98 avg=2.12\n",
      "[4909 | 1081.84] loss=2.65 avg=2.13\n",
      "[4910 | 1081.89] loss=2.52 avg=2.13\n",
      "[4911 | 1081.95] loss=2.09 avg=2.13\n",
      "[4912 | 1082.00] loss=3.06 avg=2.14\n",
      "[4913 | 1082.06] loss=3.13 avg=2.15\n",
      "[4914 | 1082.12] loss=2.82 avg=2.16\n",
      "[4915 | 1082.17] loss=1.41 avg=2.15\n",
      "[4916 | 1082.23] loss=1.08 avg=2.14\n",
      "[4917 | 1082.28] loss=2.35 avg=2.14\n",
      "[4918 | 1082.34] loss=1.54 avg=2.14\n",
      "[4919 | 1082.39] loss=2.75 avg=2.14\n",
      "[4920 | 1082.45] loss=2.80 avg=2.15\n",
      "[4921 | 1082.50] loss=2.11 avg=2.15\n",
      "[4922 | 1082.55] loss=1.22 avg=2.14\n",
      "[4923 | 1082.61] loss=3.19 avg=2.15\n",
      "[4924 | 1082.67] loss=2.32 avg=2.15\n",
      "[4925 | 1082.72] loss=2.30 avg=2.15\n",
      "[4926 | 1082.78] loss=2.69 avg=2.16\n",
      "[4927 | 1082.84] loss=2.37 avg=2.16\n",
      "[4928 | 1082.89] loss=2.64 avg=2.17\n",
      "[4929 | 1082.95] loss=0.93 avg=2.15\n",
      "[4930 | 1083.01] loss=2.37 avg=2.16\n",
      "[4931 | 1083.06] loss=2.29 avg=2.16\n",
      "[4932 | 1083.12] loss=2.50 avg=2.16\n",
      "[4933 | 1083.18] loss=1.90 avg=2.16\n",
      "[4934 | 1083.23] loss=1.60 avg=2.15\n",
      "[4935 | 1083.28] loss=2.27 avg=2.15\n",
      "[4936 | 1083.34] loss=1.04 avg=2.14\n",
      "[4937 | 1083.39] loss=1.65 avg=2.14\n",
      "[4938 | 1083.45] loss=1.70 avg=2.13\n",
      "[4939 | 1083.50] loss=1.33 avg=2.12\n",
      "[4940 | 1083.56] loss=2.48 avg=2.13\n",
      "[4941 | 1083.62] loss=2.70 avg=2.13\n",
      "[4942 | 1083.68] loss=1.28 avg=2.13\n",
      "[4943 | 1083.73] loss=2.65 avg=2.13\n",
      "[4944 | 1083.79] loss=2.46 avg=2.13\n",
      "[4945 | 1083.85] loss=2.43 avg=2.14\n",
      "[4946 | 1083.90] loss=1.95 avg=2.13\n",
      "[4947 | 1083.96] loss=2.69 avg=2.14\n",
      "[4948 | 1084.02] loss=2.76 avg=2.15\n",
      "[4949 | 1084.08] loss=2.20 avg=2.15\n",
      "[4950 | 1084.13] loss=1.88 avg=2.14\n",
      "[4951 | 1084.19] loss=2.60 avg=2.15\n",
      "[4952 | 1084.25] loss=1.95 avg=2.15\n",
      "[4953 | 1084.30] loss=1.88 avg=2.14\n",
      "[4954 | 1084.36] loss=2.47 avg=2.15\n",
      "[4955 | 1084.41] loss=2.11 avg=2.15\n",
      "[4956 | 1084.47] loss=2.54 avg=2.15\n",
      "[4957 | 1084.52] loss=2.23 avg=2.15\n",
      "[4958 | 1084.58] loss=2.49 avg=2.16\n",
      "[4959 | 1084.64] loss=1.12 avg=2.15\n",
      "[4960 | 1084.69] loss=1.64 avg=2.14\n",
      "[4961 | 1084.75] loss=1.98 avg=2.14\n",
      "[4962 | 1084.81] loss=1.81 avg=2.14\n",
      "[4963 | 1084.86] loss=3.22 avg=2.15\n",
      "[4964 | 1084.92] loss=2.12 avg=2.15\n",
      "[4965 | 1084.97] loss=3.59 avg=2.16\n",
      "[4966 | 1085.03] loss=1.82 avg=2.16\n",
      "[4967 | 1085.09] loss=1.89 avg=2.15\n",
      "[4968 | 1085.14] loss=2.23 avg=2.15\n",
      "[4969 | 1085.20] loss=2.06 avg=2.15\n",
      "[4970 | 1085.25] loss=2.22 avg=2.15\n",
      "[4971 | 1085.31] loss=2.51 avg=2.16\n",
      "[4972 | 1085.36] loss=2.22 avg=2.16\n",
      "[4973 | 1085.42] loss=1.65 avg=2.15\n",
      "[4974 | 1085.47] loss=1.95 avg=2.15\n",
      "[4975 | 1085.53] loss=1.48 avg=2.15\n",
      "[4976 | 1085.58] loss=1.96 avg=2.14\n",
      "[4977 | 1085.63] loss=2.32 avg=2.14\n",
      "[4978 | 1085.69] loss=1.64 avg=2.14\n",
      "[4979 | 1085.75] loss=2.87 avg=2.15\n",
      "[4980 | 1085.80] loss=2.50 avg=2.15\n",
      "[4981 | 1085.86] loss=2.56 avg=2.15\n",
      "[4982 | 1085.91] loss=1.76 avg=2.15\n",
      "[4983 | 1085.96] loss=1.69 avg=2.15\n",
      "[4984 | 1086.02] loss=3.32 avg=2.16\n",
      "[4985 | 1086.07] loss=1.14 avg=2.15\n",
      "[4986 | 1086.13] loss=1.82 avg=2.14\n",
      "[4987 | 1086.19] loss=2.48 avg=2.15\n",
      "[4988 | 1086.24] loss=1.93 avg=2.15\n",
      "[4989 | 1086.30] loss=2.22 avg=2.15\n",
      "[4990 | 1086.35] loss=3.88 avg=2.16\n",
      "[4991 | 1086.41] loss=2.10 avg=2.16\n",
      "[4992 | 1086.47] loss=2.41 avg=2.17\n",
      "[4993 | 1086.52] loss=1.47 avg=2.16\n",
      "[4994 | 1086.58] loss=2.10 avg=2.16\n",
      "[4995 | 1086.63] loss=2.49 avg=2.16\n",
      "[4996 | 1086.69] loss=0.96 avg=2.15\n",
      "[4997 | 1086.74] loss=1.12 avg=2.14\n",
      "[4998 | 1086.80] loss=2.23 avg=2.14\n",
      "[4999 | 1086.86] loss=1.63 avg=2.13\n",
      "Saving checkpoint/m1_vulnerability/model-5000\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "ulif\n",
      "    for ( int k = 0; k < 1000; k++) { //\t\n",
      "for (int j = 0; j < 1000; j++) {\n",
      "//\t\n",
      "if (new_n_n_n_0 (__new_n___n_1)) {\n",
      "if (new_n_n_n_0) {\n",
      "if (new_n_n1) {\n",
      "for (int n = 0; n < 1000; n++) { //\t\n",
      "if (new_n_n_0) {\n",
      "if (new_n_n_0) {\n",
      "n = 0;\n",
      "} else {\n",
      "new_n_n_0 ________________         //\t\n",
      "}  } }\n",
      "}\n",
      "if (new_n_n_0) {\n",
      "if (                         for (int i = 0; i < 1000; i++) {                               if (new_n_n_0)   j = - 1;\n",
      "                            \n",
      "                                  }\n",
      "                    } } else {\n",
      "                                 \n",
      "                            \n",
      "                       \n",
      "                        \n",
      "}  }\n",
      "}\n",
      "if (!(new_n_n_0))) { //\t\n",
      "if (                  \n",
      "for (int j = 0; j < 1000; j++) {\n",
      "                    \n",
      "if (                           \n",
      "                            else\n",
      "                            \n",
      "                           \n",
      "                     ) *    }\n",
      "if ( new_n_n_0)  j = 0;\n",
      "for (int k = 0; k < 1000; k++) {              \n",
      "                       \n",
      "                            \n",
      "                  } else (                          \n",
      "                   \n",
      "               }\n",
      "if (                                       \n",
      "                 \n",
      "               \n",
      "            }\n",
      "if (                                \n",
      "         )\n",
      "   ;\n",
      "}\n",
      "             \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5000 | 1108.40] validation loss = 2.19\n",
      "[5000 | 1108.46] loss=1.43 avg=2.13\n",
      "[5001 | 1108.52] loss=1.47 avg=2.12\n",
      "[5002 | 1108.58] loss=1.31 avg=2.11\n",
      "[5003 | 1108.63] loss=1.40 avg=2.11\n",
      "[5004 | 1108.69] loss=2.63 avg=2.11\n",
      "[5005 | 1108.74] loss=1.45 avg=2.10\n",
      "[5006 | 1108.80] loss=1.80 avg=2.10\n",
      "[5007 | 1108.85] loss=2.56 avg=2.11\n",
      "[5008 | 1108.91] loss=2.28 avg=2.11\n",
      "[5009 | 1108.97] loss=1.92 avg=2.11\n",
      "[5010 | 1109.02] loss=1.43 avg=2.10\n",
      "[5011 | 1109.07] loss=1.97 avg=2.10\n",
      "[5012 | 1109.13] loss=1.96 avg=2.10\n",
      "[5013 | 1109.19] loss=1.26 avg=2.09\n",
      "[5014 | 1109.24] loss=2.59 avg=2.09\n",
      "[5015 | 1109.30] loss=2.00 avg=2.09\n",
      "[5016 | 1109.35] loss=1.77 avg=2.09\n",
      "[5017 | 1109.41] loss=2.17 avg=2.09\n",
      "[5018 | 1109.46] loss=1.40 avg=2.08\n",
      "[5019 | 1109.52] loss=2.54 avg=2.09\n",
      "[5020 | 1109.58] loss=2.39 avg=2.09\n",
      "[5021 | 1109.63] loss=3.42 avg=2.10\n",
      "[5022 | 1109.69] loss=3.09 avg=2.11\n",
      "[5023 | 1109.74] loss=2.09 avg=2.11\n",
      "[5024 | 1109.80] loss=0.82 avg=2.10\n",
      "[5025 | 1109.86] loss=2.65 avg=2.11\n",
      "[5026 | 1109.91] loss=2.59 avg=2.11\n",
      "[5027 | 1109.97] loss=3.24 avg=2.12\n",
      "[5028 | 1110.03] loss=0.79 avg=2.11\n",
      "[5029 | 1110.08] loss=2.20 avg=2.11\n",
      "[5030 | 1110.14] loss=3.01 avg=2.12\n",
      "[5031 | 1110.20] loss=2.09 avg=2.12\n",
      "[5032 | 1110.25] loss=1.91 avg=2.12\n",
      "[5033 | 1110.31] loss=2.35 avg=2.12\n",
      "[5034 | 1110.36] loss=1.59 avg=2.11\n",
      "[5035 | 1110.42] loss=1.79 avg=2.11\n",
      "[5036 | 1110.47] loss=2.11 avg=2.11\n",
      "[5037 | 1110.52] loss=2.66 avg=2.12\n",
      "[5038 | 1110.58] loss=1.72 avg=2.11\n",
      "[5039 | 1110.64] loss=1.31 avg=2.10\n",
      "[5040 | 1110.69] loss=1.40 avg=2.10\n",
      "[5041 | 1110.75] loss=1.35 avg=2.09\n",
      "[5042 | 1110.81] loss=1.81 avg=2.09\n",
      "[5043 | 1110.86] loss=2.00 avg=2.09\n",
      "[5044 | 1110.92] loss=2.10 avg=2.09\n",
      "[5045 | 1110.97] loss=3.19 avg=2.10\n",
      "[5046 | 1111.03] loss=0.87 avg=2.08\n",
      "[5047 | 1111.09] loss=2.56 avg=2.09\n",
      "[5048 | 1111.14] loss=0.98 avg=2.08\n",
      "[5049 | 1111.20] loss=1.43 avg=2.07\n",
      "[5050 | 1111.25] loss=1.49 avg=2.07\n",
      "[5051 | 1111.31] loss=1.21 avg=2.06\n",
      "[5052 | 1111.36] loss=1.66 avg=2.05\n",
      "[5053 | 1111.42] loss=1.48 avg=2.05\n",
      "[5054 | 1111.47] loss=1.90 avg=2.05\n",
      "[5055 | 1111.53] loss=1.27 avg=2.04\n",
      "[5056 | 1111.59] loss=1.64 avg=2.03\n",
      "[5057 | 1111.64] loss=2.64 avg=2.04\n",
      "[5058 | 1111.70] loss=1.29 avg=2.03\n",
      "[5059 | 1111.76] loss=2.45 avg=2.04\n",
      "[5060 | 1111.81] loss=2.30 avg=2.04\n",
      "[5061 | 1111.87] loss=1.91 avg=2.04\n",
      "[5062 | 1111.92] loss=1.54 avg=2.03\n",
      "[5063 | 1111.98] loss=1.67 avg=2.03\n",
      "[5064 | 1112.04] loss=2.41 avg=2.03\n",
      "[5065 | 1112.09] loss=1.68 avg=2.03\n",
      "[5066 | 1112.15] loss=2.10 avg=2.03\n",
      "[5067 | 1112.21] loss=1.72 avg=2.03\n",
      "[5068 | 1112.26] loss=1.79 avg=2.03\n",
      "[5069 | 1112.32] loss=2.38 avg=2.03\n",
      "[5070 | 1112.37] loss=2.67 avg=2.04\n",
      "[5071 | 1112.43] loss=2.87 avg=2.04\n",
      "[5072 | 1112.49] loss=2.19 avg=2.05\n",
      "[5073 | 1112.55] loss=3.02 avg=2.05\n",
      "[5074 | 1112.61] loss=2.37 avg=2.06\n",
      "[5075 | 1112.67] loss=2.53 avg=2.06\n",
      "[5076 | 1112.72] loss=2.33 avg=2.07\n",
      "[5077 | 1112.78] loss=2.06 avg=2.07\n",
      "[5078 | 1112.84] loss=3.14 avg=2.08\n",
      "[5079 | 1112.90] loss=1.68 avg=2.07\n",
      "[5080 | 1112.95] loss=2.12 avg=2.07\n",
      "[5081 | 1113.01] loss=2.20 avg=2.07\n",
      "[5082 | 1113.07] loss=1.87 avg=2.07\n",
      "[5083 | 1113.12] loss=2.15 avg=2.07\n",
      "[5084 | 1113.18] loss=1.13 avg=2.06\n",
      "[5085 | 1113.24] loss=1.72 avg=2.06\n",
      "[5086 | 1113.29] loss=1.36 avg=2.05\n",
      "[5087 | 1113.35] loss=1.84 avg=2.05\n",
      "[5088 | 1113.40] loss=2.13 avg=2.05\n",
      "[5089 | 1113.46] loss=3.55 avg=2.07\n",
      "[5090 | 1113.52] loss=2.55 avg=2.07\n",
      "[5091 | 1113.57] loss=1.11 avg=2.06\n",
      "[5092 | 1113.62] loss=2.90 avg=2.07\n",
      "[5093 | 1113.68] loss=1.50 avg=2.06\n",
      "[5094 | 1113.73] loss=1.47 avg=2.06\n",
      "[5095 | 1113.79] loss=1.43 avg=2.05\n",
      "[5096 | 1113.85] loss=2.48 avg=2.06\n",
      "[5097 | 1113.90] loss=1.45 avg=2.05\n",
      "[5098 | 1113.96] loss=2.15 avg=2.05\n",
      "[5099 | 1114.01] loss=1.95 avg=2.05\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " ret_base.get_name ()).get_name ( 0x9 ).set_default ( 0x6f );\n",
      "\n",
      "}\n",
      "         \n",
      "          if ((pdata == NULL || pdata == NULL) && (!(sizeof (pdata->pdata_size) <= 0)) && (pdata != NULL))) {\n",
      "            lvalue, data;\n",
      "           data += strlen (file) ? file_type : strlen (file_type);\n",
      "    }\n",
      "    case 0 :\n",
      "         write_strlen (pdata) <<= sizeof (pdata_size)) + PDATA;\n",
      "         return write_strlen (pdata);\n",
      "    }\n",
      "    this->pdata = pdata;\n",
      "       return write_strlen (pdata->pdata_size);\n",
      "    }\n",
      "   \n",
      "   if (i >= -1)\n",
      "      if (i <= 0) {\n",
      "      retval_count = i;\n",
      "      return retval_count;\n",
      "    }\n",
      "     return write_strlen (pdata);\n",
      "   }\n",
      "  \n",
      "   if (lvalue)\n",
      "     b;\n",
      "     if (sizeof (file_size)) {\n",
      "      b = lvalue + sizeof (file_size);\n",
      "     b = sizeof (file);\n",
      "     return cread_strlen (pdata.pdata_size, nc, sizeof (file));\n",
      "   } else\n",
      "       write_strlen (pdata);\n",
      "  \n",
      "   b = lvalue - sizeof (file_size); else\n",
      "      b = nc - sizeof (file);\n",
      "   }\n",
      "  \n",
      "   b = cread_strlen (pdata.pdata_size, nc, sizeof (file));\n",
      "  \n",
      "   int i, j;\n",
      "    for (jj = 0; j < j;\n",
      "    j = 1; j++) {\n",
      "      int i, j = 1;\n",
      "     int j, j = nc - nc - (i + 1);\n",
      "      int i, j = nc - (j + 1);\n",
      "       if (i <= 0) {\n",
      "         nc++;\n",
      "   }\n",
      "   \n",
      "    nc -= j;\n",
      "  \n",
      "   }\n",
      "   switch ((nc)->size() == (file_size)) {\n",
      "      case 0 :\n",
      "   *(nc + (ncc->size() << 1)) <<= sizeof(file));\n",
      "     }\n",
      "  \n",
      "    switch ((lvalue)->size() - 4) {\n",
      "      if (lvalue) {\n",
      "        return write_strlen ((pdata->pdata_size, ncc->size() - 4));\n",
      "    } else {\n",
      "          write_strlen ((pdata->pdata_size, ncc->size() - 4));\n",
      "           b;\n",
      "         if (moves->size() > 2)\n",
      "           return write_strlen ((pdata->pdata_size, ncc->size() - 2));\n",
      "           m = move_move_move_move (move_move_move_p (m, j, b));\n",
      "        }\n",
      "        break;\n",
      "   \n",
      "    }\n",
      "   else {\n",
      "      int i, j = int i;\n",
      "   \n",
      "     if (k & 0x80);\n",
      "     \n",
      "    int j = int i;\n",
      "      if ((i > 8) && (!(j[j])))) {\n",
      "         return write_strlen ((pdata->pdata_size, ncc->size() - 8));\n",
      "     } else {\n",
      "  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 45.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5100 | 1130.04] validation loss = 2.19\n",
      "[5100 | 1130.10] loss=2.75 avg=2.06\n",
      "[5101 | 1130.16] loss=2.24 avg=2.06\n",
      "[5102 | 1130.21] loss=2.79 avg=2.07\n",
      "[5103 | 1130.27] loss=1.11 avg=2.06\n",
      "[5104 | 1130.32] loss=2.79 avg=2.06\n",
      "[5105 | 1130.38] loss=2.33 avg=2.07\n",
      "[5106 | 1130.43] loss=1.94 avg=2.07\n",
      "[5107 | 1130.48] loss=1.61 avg=2.06\n",
      "[5108 | 1130.54] loss=1.60 avg=2.06\n",
      "[5109 | 1130.60] loss=2.04 avg=2.06\n",
      "[5110 | 1130.66] loss=1.22 avg=2.05\n",
      "[5111 | 1130.71] loss=3.16 avg=2.06\n",
      "[5112 | 1130.77] loss=2.44 avg=2.06\n",
      "[5113 | 1130.82] loss=2.20 avg=2.06\n",
      "[5114 | 1130.88] loss=2.61 avg=2.07\n",
      "[5115 | 1130.94] loss=1.88 avg=2.07\n",
      "[5116 | 1130.99] loss=2.77 avg=2.07\n",
      "[5117 | 1131.04] loss=1.38 avg=2.07\n",
      "[5118 | 1131.10] loss=3.18 avg=2.08\n",
      "[5119 | 1131.15] loss=1.58 avg=2.07\n",
      "[5120 | 1131.21] loss=1.92 avg=2.07\n",
      "[5121 | 1131.27] loss=2.19 avg=2.07\n",
      "[5122 | 1131.33] loss=2.30 avg=2.08\n",
      "[5123 | 1131.39] loss=1.89 avg=2.07\n",
      "[5124 | 1131.44] loss=2.34 avg=2.08\n",
      "[5125 | 1131.50] loss=2.85 avg=2.08\n",
      "[5126 | 1131.55] loss=3.19 avg=2.10\n",
      "[5127 | 1131.61] loss=2.39 avg=2.10\n",
      "[5128 | 1131.67] loss=3.29 avg=2.11\n",
      "[5129 | 1131.73] loss=1.61 avg=2.11\n",
      "[5130 | 1131.79] loss=2.33 avg=2.11\n",
      "[5131 | 1131.84] loss=2.41 avg=2.11\n",
      "[5132 | 1131.89] loss=2.46 avg=2.11\n",
      "[5133 | 1131.95] loss=2.99 avg=2.12\n",
      "[5134 | 1132.01] loss=1.27 avg=2.11\n",
      "[5135 | 1132.07] loss=1.81 avg=2.11\n",
      "[5136 | 1132.12] loss=2.33 avg=2.11\n",
      "[5137 | 1132.18] loss=1.54 avg=2.11\n",
      "[5138 | 1132.23] loss=2.55 avg=2.11\n",
      "[5139 | 1132.29] loss=1.25 avg=2.10\n",
      "[5140 | 1132.34] loss=2.75 avg=2.11\n",
      "[5141 | 1132.40] loss=3.75 avg=2.13\n",
      "[5142 | 1132.45] loss=2.51 avg=2.13\n",
      "[5143 | 1132.51] loss=1.95 avg=2.13\n",
      "[5144 | 1132.57] loss=2.20 avg=2.13\n",
      "[5145 | 1132.62] loss=0.86 avg=2.12\n",
      "[5146 | 1132.68] loss=1.42 avg=2.11\n",
      "[5147 | 1132.73] loss=1.89 avg=2.11\n",
      "[5148 | 1132.79] loss=2.72 avg=2.11\n",
      "[5149 | 1132.85] loss=1.44 avg=2.11\n",
      "[5150 | 1132.91] loss=1.84 avg=2.10\n",
      "[5151 | 1132.97] loss=1.71 avg=2.10\n",
      "[5152 | 1133.02] loss=1.12 avg=2.09\n",
      "[5153 | 1133.07] loss=2.85 avg=2.10\n",
      "[5154 | 1133.12] loss=1.68 avg=2.09\n",
      "[5155 | 1133.18] loss=1.32 avg=2.09\n",
      "[5156 | 1133.24] loss=1.57 avg=2.08\n",
      "[5157 | 1133.29] loss=2.10 avg=2.08\n",
      "[5158 | 1133.34] loss=2.34 avg=2.08\n",
      "[5159 | 1133.40] loss=1.95 avg=2.08\n",
      "[5160 | 1133.45] loss=3.34 avg=2.09\n",
      "[5161 | 1133.51] loss=1.59 avg=2.09\n",
      "[5162 | 1133.56] loss=2.48 avg=2.09\n",
      "[5163 | 1133.62] loss=1.38 avg=2.09\n",
      "[5164 | 1133.67] loss=2.29 avg=2.09\n",
      "[5165 | 1133.73] loss=1.09 avg=2.08\n",
      "[5166 | 1133.78] loss=2.22 avg=2.08\n",
      "[5167 | 1133.84] loss=1.63 avg=2.08\n",
      "[5168 | 1133.89] loss=1.82 avg=2.07\n",
      "[5169 | 1133.95] loss=1.62 avg=2.07\n",
      "[5170 | 1134.00] loss=2.14 avg=2.07\n",
      "[5171 | 1134.06] loss=1.28 avg=2.06\n",
      "[5172 | 1134.12] loss=1.13 avg=2.05\n",
      "[5173 | 1134.18] loss=3.19 avg=2.06\n",
      "[5174 | 1134.23] loss=1.80 avg=2.06\n",
      "[5175 | 1134.29] loss=1.88 avg=2.06\n",
      "[5176 | 1134.34] loss=3.47 avg=2.07\n",
      "[5177 | 1134.40] loss=2.12 avg=2.07\n",
      "[5178 | 1134.45] loss=1.69 avg=2.07\n",
      "[5179 | 1134.51] loss=2.73 avg=2.08\n",
      "[5180 | 1134.57] loss=1.62 avg=2.07\n",
      "[5181 | 1134.62] loss=2.10 avg=2.07\n",
      "[5182 | 1134.68] loss=1.36 avg=2.06\n",
      "[5183 | 1134.73] loss=2.09 avg=2.06\n",
      "[5184 | 1134.79] loss=2.08 avg=2.07\n",
      "[5185 | 1134.85] loss=2.35 avg=2.07\n",
      "[5186 | 1134.90] loss=1.03 avg=2.06\n",
      "[5187 | 1134.96] loss=1.98 avg=2.06\n",
      "[5188 | 1135.01] loss=2.66 avg=2.06\n",
      "[5189 | 1135.07] loss=1.26 avg=2.05\n",
      "[5190 | 1135.13] loss=1.94 avg=2.05\n",
      "[5191 | 1135.18] loss=2.56 avg=2.06\n",
      "[5192 | 1135.24] loss=2.63 avg=2.06\n",
      "[5193 | 1135.29] loss=0.83 avg=2.05\n",
      "[5194 | 1135.35] loss=2.07 avg=2.05\n",
      "[5195 | 1135.41] loss=1.63 avg=2.05\n",
      "[5196 | 1135.46] loss=2.88 avg=2.06\n",
      "[5197 | 1135.52] loss=2.93 avg=2.07\n",
      "[5198 | 1135.58] loss=2.79 avg=2.07\n",
      "[5199 | 1135.63] loss=0.90 avg=2.06\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " EQU                                                  /*                                                                                                                                                       -\t -\t\n",
      "And, what do I have to say about it ?\n",
      "\n",
      "                                                                                                                                         -pci-}\n",
      "\n",
      "                          //PIO =   } else {\n",
      "\n",
      "                                                                                                  }\n",
      "   }\n",
      "\n",
      "\n",
      "   }\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "if (pci_read(lst_info_get_state_state_from_state_state,\n",
      "                                                                           default  =                                      }\n",
      "   \n",
      "\n",
      "                                                 \n",
      "     )\n",
      "\n",
      "                                   \n",
      "                                    \"\t\";                              \n",
      "                                       \"RI\";                                     }\n",
      "\n",
      "     \n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "                             \n",
      "   \n",
      "   \n",
      "\n",
      "                           \n",
      "   \n",
      "   \n",
      "\n",
      "                                \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5200 | 1151.68] validation loss = 2.19\n",
      "[5200 | 1151.74] loss=2.60 avg=2.07\n",
      "[5201 | 1151.80] loss=2.01 avg=2.07\n",
      "[5202 | 1151.85] loss=1.28 avg=2.06\n",
      "[5203 | 1151.91] loss=1.79 avg=2.05\n",
      "[5204 | 1151.96] loss=2.70 avg=2.06\n",
      "[5205 | 1152.02] loss=1.85 avg=2.06\n",
      "[5206 | 1152.07] loss=2.11 avg=2.06\n",
      "[5207 | 1152.13] loss=1.56 avg=2.05\n",
      "[5208 | 1152.19] loss=3.66 avg=2.07\n",
      "[5209 | 1152.24] loss=3.26 avg=2.08\n",
      "[5210 | 1152.30] loss=2.01 avg=2.08\n",
      "[5211 | 1152.36] loss=0.99 avg=2.07\n",
      "[5212 | 1152.41] loss=2.16 avg=2.07\n",
      "[5213 | 1152.47] loss=1.65 avg=2.07\n",
      "[5214 | 1152.53] loss=2.37 avg=2.07\n",
      "[5215 | 1152.58] loss=2.19 avg=2.07\n",
      "[5216 | 1152.64] loss=1.36 avg=2.06\n",
      "[5217 | 1152.69] loss=2.24 avg=2.07\n",
      "[5218 | 1152.75] loss=1.71 avg=2.06\n",
      "[5219 | 1152.81] loss=2.76 avg=2.07\n",
      "[5220 | 1152.86] loss=1.29 avg=2.06\n",
      "[5221 | 1152.92] loss=1.53 avg=2.06\n",
      "[5222 | 1152.97] loss=2.05 avg=2.06\n",
      "[5223 | 1153.03] loss=3.17 avg=2.07\n",
      "[5224 | 1153.09] loss=2.46 avg=2.07\n",
      "[5225 | 1153.15] loss=3.15 avg=2.08\n",
      "[5226 | 1153.20] loss=1.24 avg=2.07\n",
      "[5227 | 1153.26] loss=2.87 avg=2.08\n",
      "[5228 | 1153.32] loss=2.84 avg=2.09\n",
      "[5229 | 1153.38] loss=2.25 avg=2.09\n",
      "[5230 | 1153.43] loss=1.73 avg=2.09\n",
      "[5231 | 1153.49] loss=3.01 avg=2.10\n",
      "[5232 | 1153.55] loss=2.95 avg=2.11\n",
      "[5233 | 1153.60] loss=2.79 avg=2.11\n",
      "[5234 | 1153.66] loss=3.00 avg=2.12\n",
      "[5235 | 1153.71] loss=2.74 avg=2.13\n",
      "[5236 | 1153.77] loss=2.27 avg=2.13\n",
      "[5237 | 1153.82] loss=3.05 avg=2.14\n",
      "[5238 | 1153.88] loss=2.22 avg=2.14\n",
      "[5239 | 1153.93] loss=1.32 avg=2.13\n",
      "[5240 | 1153.98] loss=2.64 avg=2.14\n",
      "[5241 | 1154.03] loss=1.01 avg=2.12\n",
      "[5242 | 1154.09] loss=1.46 avg=2.12\n",
      "[5243 | 1154.14] loss=1.47 avg=2.11\n",
      "[5244 | 1154.20] loss=1.98 avg=2.11\n",
      "[5245 | 1154.26] loss=1.40 avg=2.10\n",
      "[5246 | 1154.31] loss=2.52 avg=2.11\n",
      "[5247 | 1154.37] loss=1.31 avg=2.10\n",
      "[5248 | 1154.43] loss=2.38 avg=2.10\n",
      "[5249 | 1154.49] loss=1.53 avg=2.10\n",
      "[5250 | 1154.54] loss=2.05 avg=2.10\n",
      "[5251 | 1154.59] loss=1.97 avg=2.09\n",
      "[5252 | 1154.65] loss=1.78 avg=2.09\n",
      "[5253 | 1154.71] loss=0.98 avg=2.08\n",
      "[5254 | 1154.77] loss=1.88 avg=2.08\n",
      "[5255 | 1154.82] loss=2.63 avg=2.08\n",
      "[5256 | 1154.88] loss=2.22 avg=2.09\n",
      "[5257 | 1154.94] loss=2.17 avg=2.09\n",
      "[5258 | 1154.99] loss=3.31 avg=2.10\n",
      "[5259 | 1155.05] loss=3.01 avg=2.11\n",
      "[5260 | 1155.10] loss=4.06 avg=2.13\n",
      "[5261 | 1155.16] loss=0.99 avg=2.12\n",
      "[5262 | 1155.22] loss=1.37 avg=2.11\n",
      "[5263 | 1155.27] loss=2.18 avg=2.11\n",
      "[5264 | 1155.33] loss=1.14 avg=2.10\n",
      "[5265 | 1155.38] loss=2.32 avg=2.10\n",
      "[5266 | 1155.44] loss=2.95 avg=2.11\n",
      "[5267 | 1155.49] loss=2.84 avg=2.12\n",
      "[5268 | 1155.55] loss=2.89 avg=2.12\n",
      "[5269 | 1155.61] loss=3.20 avg=2.14\n",
      "[5270 | 1155.66] loss=2.59 avg=2.14\n",
      "[5271 | 1155.72] loss=1.25 avg=2.13\n",
      "[5272 | 1155.78] loss=1.91 avg=2.13\n",
      "[5273 | 1155.83] loss=0.97 avg=2.12\n",
      "[5274 | 1155.89] loss=1.89 avg=2.12\n",
      "[5275 | 1155.94] loss=1.56 avg=2.11\n",
      "[5276 | 1156.00] loss=3.21 avg=2.12\n",
      "[5277 | 1156.05] loss=1.98 avg=2.12\n",
      "[5278 | 1156.11] loss=1.75 avg=2.12\n",
      "[5279 | 1156.17] loss=2.46 avg=2.12\n",
      "[5280 | 1156.22] loss=1.27 avg=2.11\n",
      "[5281 | 1156.28] loss=2.15 avg=2.11\n",
      "[5282 | 1156.34] loss=2.27 avg=2.11\n",
      "[5283 | 1156.40] loss=3.03 avg=2.12\n",
      "[5284 | 1156.45] loss=0.95 avg=2.11\n",
      "[5285 | 1156.51] loss=2.07 avg=2.11\n",
      "[5286 | 1156.57] loss=1.97 avg=2.11\n",
      "[5287 | 1156.62] loss=0.99 avg=2.10\n",
      "[5288 | 1156.68] loss=1.10 avg=2.09\n",
      "[5289 | 1156.74] loss=2.45 avg=2.09\n",
      "[5290 | 1156.80] loss=0.88 avg=2.08\n",
      "[5291 | 1156.86] loss=2.58 avg=2.08\n",
      "[5292 | 1156.91] loss=1.05 avg=2.07\n",
      "[5293 | 1156.96] loss=1.81 avg=2.07\n",
      "[5294 | 1157.02] loss=0.92 avg=2.06\n",
      "[5295 | 1157.08] loss=2.53 avg=2.06\n",
      "[5296 | 1157.13] loss=1.78 avg=2.06\n",
      "[5297 | 1157.19] loss=2.42 avg=2.06\n",
      "[5298 | 1157.25] loss=2.39 avg=2.07\n",
      "[5299 | 1157.31] loss=1.26 avg=2.06\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:00<00:00, 49.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "_id) {\n",
      "                                return\n",
      "                                                              \n",
      "                                            \n",
      "                                             \n",
      "     }\n",
      "    if (x == -\t)\n",
      "               else\n",
      "                                        \n",
      "              else\n",
      "               \n",
      "               else\n",
      "             \n",
      "             \n",
      "          \n",
      "         \n",
      "          if (x >= 8)\n",
      "          \n",
      "          if (x <= 18)\n",
      "         \n",
      "         \n",
      "          \n",
      "          \n",
      "         \n",
      "        \n",
      "      \n",
      "        //\t  \n",
      "      \n",
      "     \n",
      "      \n",
      "     \n",
      "     \n",
      "    \n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "\\\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "}\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "   \n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "}\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "     \n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5300 | 1173.33] validation loss = 2.18\n",
      "[5300 | 1173.39] loss=2.65 avg=2.07\n",
      "[5301 | 1173.45] loss=1.52 avg=2.06\n",
      "[5302 | 1173.51] loss=2.56 avg=2.07\n",
      "[5303 | 1173.56] loss=1.45 avg=2.06\n",
      "[5304 | 1173.62] loss=1.91 avg=2.06\n",
      "[5305 | 1173.67] loss=1.42 avg=2.05\n",
      "[5306 | 1173.73] loss=1.75 avg=2.05\n",
      "[5307 | 1173.79] loss=1.37 avg=2.04\n",
      "[5308 | 1173.84] loss=2.33 avg=2.04\n",
      "[5309 | 1173.90] loss=1.54 avg=2.04\n",
      "[5310 | 1173.95] loss=2.28 avg=2.04\n",
      "[5311 | 1174.01] loss=2.73 avg=2.05\n",
      "[5312 | 1174.07] loss=1.25 avg=2.04\n",
      "[5313 | 1174.13] loss=3.29 avg=2.05\n",
      "[5314 | 1174.18] loss=2.36 avg=2.06\n",
      "[5315 | 1174.24] loss=2.15 avg=2.06\n",
      "[5316 | 1174.30] loss=1.08 avg=2.05\n",
      "[5317 | 1174.35] loss=1.21 avg=2.04\n",
      "[5318 | 1174.41] loss=1.84 avg=2.04\n",
      "[5319 | 1174.47] loss=1.72 avg=2.03\n",
      "[5320 | 1174.52] loss=2.91 avg=2.04\n",
      "[5321 | 1174.58] loss=2.68 avg=2.05\n",
      "[5322 | 1174.64] loss=2.24 avg=2.05\n",
      "[5323 | 1174.70] loss=1.39 avg=2.04\n",
      "[5324 | 1174.75] loss=2.41 avg=2.05\n",
      "[5325 | 1174.81] loss=2.99 avg=2.06\n",
      "[5326 | 1174.86] loss=2.85 avg=2.07\n",
      "[5327 | 1174.92] loss=1.83 avg=2.06\n",
      "[5328 | 1174.97] loss=3.77 avg=2.08\n",
      "[5329 | 1175.03] loss=2.09 avg=2.08\n",
      "[5330 | 1175.09] loss=3.48 avg=2.09\n",
      "[5331 | 1175.14] loss=2.54 avg=2.10\n",
      "[5332 | 1175.20] loss=1.58 avg=2.09\n",
      "[5333 | 1175.25] loss=2.41 avg=2.10\n",
      "[5334 | 1175.31] loss=2.11 avg=2.10\n",
      "[5335 | 1175.37] loss=2.77 avg=2.10\n",
      "[5336 | 1175.42] loss=1.84 avg=2.10\n",
      "[5337 | 1175.48] loss=2.19 avg=2.10\n",
      "[5338 | 1175.53] loss=1.83 avg=2.10\n",
      "[5339 | 1175.58] loss=2.77 avg=2.11\n",
      "[5340 | 1175.64] loss=3.01 avg=2.11\n",
      "[5341 | 1175.70] loss=2.41 avg=2.12\n",
      "[5342 | 1175.76] loss=2.00 avg=2.12\n",
      "[5343 | 1175.81] loss=2.19 avg=2.12\n",
      "[5344 | 1175.86] loss=2.52 avg=2.12\n",
      "[5345 | 1175.92] loss=2.46 avg=2.12\n",
      "[5346 | 1175.98] loss=1.55 avg=2.12\n",
      "[5347 | 1176.03] loss=2.04 avg=2.12\n",
      "[5348 | 1176.09] loss=2.91 avg=2.13\n",
      "[5349 | 1176.14] loss=1.89 avg=2.12\n",
      "[5350 | 1176.20] loss=1.57 avg=2.12\n",
      "[5351 | 1176.25] loss=1.21 avg=2.11\n",
      "[5352 | 1176.31] loss=1.99 avg=2.11\n",
      "[5353 | 1176.36] loss=2.71 avg=2.11\n",
      "[5354 | 1176.42] loss=2.89 avg=2.12\n",
      "[5355 | 1176.47] loss=3.00 avg=2.13\n",
      "[5356 | 1176.53] loss=2.99 avg=2.14\n",
      "[5357 | 1176.58] loss=3.22 avg=2.15\n",
      "[5358 | 1176.64] loss=0.77 avg=2.14\n",
      "[5359 | 1176.69] loss=1.32 avg=2.13\n",
      "[5360 | 1176.75] loss=1.85 avg=2.13\n",
      "[5361 | 1176.80] loss=2.67 avg=2.13\n",
      "[5362 | 1176.86] loss=1.28 avg=2.12\n",
      "[5363 | 1176.92] loss=2.54 avg=2.13\n",
      "[5364 | 1176.98] loss=2.69 avg=2.13\n",
      "[5365 | 1177.03] loss=2.71 avg=2.14\n",
      "[5366 | 1177.09] loss=2.22 avg=2.14\n",
      "[5367 | 1177.14] loss=1.48 avg=2.13\n",
      "[5368 | 1177.20] loss=1.87 avg=2.13\n",
      "[5369 | 1177.26] loss=1.48 avg=2.12\n",
      "[5370 | 1177.31] loss=3.22 avg=2.13\n",
      "[5371 | 1177.37] loss=3.05 avg=2.14\n",
      "[5372 | 1177.43] loss=2.00 avg=2.14\n",
      "[5373 | 1177.48] loss=0.88 avg=2.13\n",
      "[5374 | 1177.54] loss=1.73 avg=2.12\n",
      "[5375 | 1177.59] loss=1.32 avg=2.12\n",
      "[5376 | 1177.65] loss=1.14 avg=2.11\n",
      "[5377 | 1177.70] loss=1.21 avg=2.10\n",
      "[5378 | 1177.76] loss=3.52 avg=2.11\n",
      "[5379 | 1177.82] loss=3.07 avg=2.12\n",
      "[5380 | 1177.87] loss=2.49 avg=2.13\n",
      "[5381 | 1177.93] loss=1.24 avg=2.12\n",
      "[5382 | 1177.98] loss=2.35 avg=2.12\n",
      "[5383 | 1178.04] loss=2.16 avg=2.12\n",
      "[5384 | 1178.09] loss=2.34 avg=2.12\n",
      "[5385 | 1178.15] loss=1.84 avg=2.12\n",
      "[5386 | 1178.20] loss=2.39 avg=2.12\n",
      "[5387 | 1178.26] loss=2.19 avg=2.12\n",
      "[5388 | 1178.32] loss=2.14 avg=2.12\n",
      "[5389 | 1178.37] loss=1.08 avg=2.11\n",
      "[5390 | 1178.42] loss=2.92 avg=2.12\n",
      "[5391 | 1178.48] loss=1.84 avg=2.12\n",
      "[5392 | 1178.53] loss=2.95 avg=2.13\n",
      "[5393 | 1178.59] loss=2.28 avg=2.13\n",
      "[5394 | 1178.64] loss=2.80 avg=2.13\n",
      "[5395 | 1178.70] loss=1.81 avg=2.13\n",
      "[5396 | 1178.75] loss=2.34 avg=2.13\n",
      "[5397 | 1178.81] loss=1.14 avg=2.12\n",
      "[5398 | 1178.87] loss=3.02 avg=2.13\n",
      "[5399 | 1178.92] loss=3.05 avg=2.14\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 47.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " f;\n",
      "}\n",
      "\n",
      "         \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      " *\n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "     \n",
      "\n",
      "  \n",
      "\n",
      "   \n",
      "\n",
      "     \n",
      "\n",
      "    \n",
      "\n",
      "     -\n",
      "\n",
      "       \n",
      "\n",
      "    \n",
      "\n",
      "   \n",
      "\n",
      "      \n",
      "\n",
      "   \n",
      "\n",
      "   \n",
      "\n",
      "      \n",
      "\n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "    \n",
      "  \n",
      "\n",
      "    \n",
      "\n",
      "     \n",
      "   \n",
      "\n",
      "    \n",
      "     \n",
      "                            \n",
      "           \n",
      "                                             \n",
      "\n",
      "         \n",
      "\n",
      "     \n",
      "                                             \n",
      "\n",
      "                                 \n",
      "\n",
      "                                \n",
      "\n",
      "         \n",
      "                                        \n",
      "\n",
      "        \n",
      "                                            }\n",
      "\n",
      "        \n",
      "                                            \n",
      "                                          \n",
      "\n",
      "         \n",
      "\n",
      "        \n",
      "          \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5400 | 1194.84] validation loss = 2.18\n",
      "[5400 | 1194.90] loss=2.73 avg=2.15\n",
      "[5401 | 1194.96] loss=1.92 avg=2.14\n",
      "[5402 | 1195.01] loss=1.64 avg=2.14\n",
      "[5403 | 1195.07] loss=1.91 avg=2.14\n",
      "[5404 | 1195.13] loss=1.15 avg=2.13\n",
      "[5405 | 1195.19] loss=1.08 avg=2.12\n",
      "[5406 | 1195.24] loss=1.83 avg=2.11\n",
      "[5407 | 1195.30] loss=1.62 avg=2.11\n",
      "[5408 | 1195.35] loss=2.86 avg=2.12\n",
      "[5409 | 1195.40] loss=2.36 avg=2.12\n",
      "[5410 | 1195.46] loss=1.47 avg=2.11\n",
      "[5411 | 1195.51] loss=3.14 avg=2.12\n",
      "[5412 | 1195.57] loss=1.69 avg=2.12\n",
      "[5413 | 1195.62] loss=2.07 avg=2.12\n",
      "[5414 | 1195.68] loss=2.86 avg=2.13\n",
      "[5415 | 1195.73] loss=2.96 avg=2.13\n",
      "[5416 | 1195.79] loss=1.55 avg=2.13\n",
      "[5417 | 1195.85] loss=2.41 avg=2.13\n",
      "[5418 | 1195.91] loss=2.35 avg=2.13\n",
      "[5419 | 1195.96] loss=1.97 avg=2.13\n",
      "[5420 | 1196.02] loss=1.60 avg=2.13\n",
      "[5421 | 1196.07] loss=1.60 avg=2.12\n",
      "[5422 | 1196.13] loss=2.09 avg=2.12\n",
      "[5423 | 1196.19] loss=2.47 avg=2.12\n",
      "[5424 | 1196.24] loss=2.59 avg=2.13\n",
      "[5425 | 1196.30] loss=2.47 avg=2.13\n",
      "[5426 | 1196.36] loss=1.87 avg=2.13\n",
      "[5427 | 1196.42] loss=1.61 avg=2.12\n",
      "[5428 | 1196.47] loss=1.90 avg=2.12\n",
      "[5429 | 1196.53] loss=0.91 avg=2.11\n",
      "[5430 | 1196.59] loss=2.78 avg=2.12\n",
      "[5431 | 1196.65] loss=2.05 avg=2.12\n",
      "[5432 | 1196.70] loss=2.17 avg=2.12\n",
      "[5433 | 1196.76] loss=1.69 avg=2.11\n",
      "[5434 | 1196.82] loss=1.40 avg=2.11\n",
      "[5435 | 1196.87] loss=0.94 avg=2.09\n",
      "[5436 | 1196.93] loss=2.01 avg=2.09\n",
      "[5437 | 1196.98] loss=2.56 avg=2.10\n",
      "[5438 | 1197.03] loss=1.80 avg=2.09\n",
      "[5439 | 1197.09] loss=1.93 avg=2.09\n",
      "[5440 | 1197.14] loss=1.83 avg=2.09\n",
      "[5441 | 1197.20] loss=1.42 avg=2.08\n",
      "[5442 | 1197.26] loss=3.16 avg=2.09\n",
      "[5443 | 1197.31] loss=2.89 avg=2.10\n",
      "[5444 | 1197.37] loss=1.47 avg=2.10\n",
      "[5445 | 1197.42] loss=1.75 avg=2.09\n",
      "[5446 | 1197.48] loss=1.53 avg=2.09\n",
      "[5447 | 1197.54] loss=2.81 avg=2.09\n",
      "[5448 | 1197.59] loss=3.12 avg=2.10\n",
      "[5449 | 1197.65] loss=2.19 avg=2.11\n",
      "[5450 | 1197.70] loss=1.77 avg=2.10\n",
      "[5451 | 1197.76] loss=2.72 avg=2.11\n",
      "[5452 | 1197.82] loss=2.79 avg=2.11\n",
      "[5453 | 1197.88] loss=2.41 avg=2.12\n",
      "[5454 | 1197.93] loss=2.45 avg=2.12\n",
      "[5455 | 1197.99] loss=2.08 avg=2.12\n",
      "[5456 | 1198.04] loss=1.78 avg=2.12\n",
      "[5457 | 1198.10] loss=1.77 avg=2.11\n",
      "[5458 | 1198.16] loss=1.02 avg=2.10\n",
      "[5459 | 1198.22] loss=2.36 avg=2.11\n",
      "[5460 | 1198.27] loss=2.01 avg=2.10\n",
      "[5461 | 1198.33] loss=2.52 avg=2.11\n",
      "[5462 | 1198.39] loss=2.16 avg=2.11\n",
      "[5463 | 1198.44] loss=2.69 avg=2.11\n",
      "[5464 | 1198.50] loss=2.02 avg=2.11\n",
      "[5465 | 1198.56] loss=1.42 avg=2.11\n",
      "[5466 | 1198.61] loss=1.45 avg=2.10\n",
      "[5467 | 1198.67] loss=2.12 avg=2.10\n",
      "[5468 | 1198.73] loss=3.20 avg=2.11\n",
      "[5469 | 1198.78] loss=2.32 avg=2.11\n",
      "[5470 | 1198.84] loss=2.53 avg=2.12\n",
      "[5471 | 1198.90] loss=1.66 avg=2.11\n",
      "[5472 | 1198.95] loss=2.36 avg=2.12\n",
      "[5473 | 1199.01] loss=2.46 avg=2.12\n",
      "[5474 | 1199.06] loss=2.62 avg=2.12\n",
      "[5475 | 1199.12] loss=1.93 avg=2.12\n",
      "[5476 | 1199.18] loss=1.92 avg=2.12\n",
      "[5477 | 1199.23] loss=2.06 avg=2.12\n",
      "[5478 | 1199.29] loss=1.72 avg=2.12\n",
      "[5479 | 1199.34] loss=1.64 avg=2.11\n",
      "[5480 | 1199.40] loss=2.22 avg=2.11\n",
      "[5481 | 1199.45] loss=1.07 avg=2.10\n",
      "[5482 | 1199.51] loss=3.41 avg=2.11\n",
      "[5483 | 1199.57] loss=2.52 avg=2.12\n",
      "[5484 | 1199.62] loss=2.99 avg=2.13\n",
      "[5485 | 1199.67] loss=3.73 avg=2.14\n",
      "[5486 | 1199.73] loss=2.35 avg=2.15\n",
      "[5487 | 1199.79] loss=1.27 avg=2.14\n",
      "[5488 | 1199.84] loss=1.40 avg=2.13\n",
      "[5489 | 1199.90] loss=3.11 avg=2.14\n",
      "[5490 | 1199.95] loss=1.51 avg=2.13\n",
      "[5491 | 1200.01] loss=2.39 avg=2.14\n",
      "[5492 | 1200.06] loss=1.41 avg=2.13\n",
      "[5493 | 1200.12] loss=2.54 avg=2.13\n",
      "[5494 | 1200.18] loss=2.20 avg=2.13\n",
      "[5495 | 1200.23] loss=2.84 avg=2.14\n",
      "[5496 | 1200.29] loss=2.50 avg=2.14\n",
      "[5497 | 1200.35] loss=2.44 avg=2.15\n",
      "[5498 | 1200.41] loss=2.91 avg=2.15\n",
      "[5499 | 1200.46] loss=2.02 avg=2.15\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 42.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "f\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t。\t\t\t�\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t�\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=[\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t=[\t\t\t\t\t\t\t\t\t\t\t='} } } } }\n",
      "                      }\n",
      "\n",
      "if (typeof\n",
      "  == 'class')\n",
      "   {\n",
      "                                   case 'object':\n",
      "                                                                case 'int' :\n",
      "                                        case 'intlong' :\n",
      "                                case 'intlong' :\n",
      "                             case 'x' :\n",
      "                               case 'size' = 'intlong'\n",
      "                            case 'byte' = 'struct'\n",
      "                           case 'size'> */\n",
      "                             case 'int':\n",
      "                          case 'intint' :\n",
      "                        } return {\n",
      "                          instanceof {\n",
      "           ||   } \n",
      "                     case 'float','int','str','intshort','int';\n",
      "} else\n",
      "         default :\n",
      "                        case 'string','int','string','intshort','int';\n",
      "\n",
      "                   case 'long','int');\n",
      "                       case 'size' = 'intlong'\n",
      "                  )\n",
      "\n",
      "               \n",
      "       \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5500 | 1216.52] validation loss = 2.18\n",
      "[5500 | 1216.58] loss=0.86 avg=2.14\n",
      "[5501 | 1216.63] loss=1.57 avg=2.13\n",
      "[5502 | 1216.69] loss=3.12 avg=2.14\n",
      "[5503 | 1216.74] loss=0.87 avg=2.13\n",
      "[5504 | 1216.80] loss=1.24 avg=2.12\n",
      "[5505 | 1216.86] loss=3.30 avg=2.13\n",
      "[5506 | 1216.91] loss=2.62 avg=2.14\n",
      "[5507 | 1216.97] loss=2.05 avg=2.14\n",
      "[5508 | 1217.02] loss=3.08 avg=2.15\n",
      "[5509 | 1217.08] loss=1.76 avg=2.14\n",
      "[5510 | 1217.14] loss=1.93 avg=2.14\n",
      "[5511 | 1217.19] loss=3.44 avg=2.15\n",
      "[5512 | 1217.25] loss=2.46 avg=2.16\n",
      "[5513 | 1217.30] loss=1.51 avg=2.15\n",
      "[5514 | 1217.36] loss=1.36 avg=2.14\n",
      "[5515 | 1217.41] loss=2.08 avg=2.14\n",
      "[5516 | 1217.47] loss=2.25 avg=2.14\n",
      "[5517 | 1217.53] loss=2.92 avg=2.15\n",
      "[5518 | 1217.59] loss=3.30 avg=2.16\n",
      "[5519 | 1217.64] loss=1.74 avg=2.16\n",
      "[5520 | 1217.70] loss=1.28 avg=2.15\n",
      "[5521 | 1217.76] loss=2.47 avg=2.15\n",
      "[5522 | 1217.82] loss=1.83 avg=2.15\n",
      "[5523 | 1217.88] loss=2.89 avg=2.16\n",
      "[5524 | 1217.93] loss=2.60 avg=2.16\n",
      "[5525 | 1217.99] loss=2.64 avg=2.17\n",
      "[5526 | 1218.05] loss=1.38 avg=2.16\n",
      "[5527 | 1218.10] loss=1.77 avg=2.15\n",
      "[5528 | 1218.16] loss=1.25 avg=2.15\n",
      "[5529 | 1218.22] loss=1.55 avg=2.14\n",
      "[5530 | 1218.28] loss=2.23 avg=2.14\n",
      "[5531 | 1218.33] loss=2.05 avg=2.14\n",
      "[5532 | 1218.39] loss=1.79 avg=2.14\n",
      "[5533 | 1218.45] loss=0.36 avg=2.12\n",
      "[5534 | 1218.51] loss=1.92 avg=2.12\n",
      "[5535 | 1218.56] loss=1.59 avg=2.11\n",
      "[5536 | 1218.62] loss=2.37 avg=2.11\n",
      "[5537 | 1218.67] loss=0.80 avg=2.10\n",
      "[5538 | 1218.73] loss=1.87 avg=2.10\n",
      "[5539 | 1218.78] loss=2.86 avg=2.11\n",
      "[5540 | 1218.84] loss=2.70 avg=2.11\n",
      "[5541 | 1218.89] loss=1.62 avg=2.11\n",
      "[5542 | 1218.95] loss=3.39 avg=2.12\n",
      "[5543 | 1219.00] loss=2.40 avg=2.12\n",
      "[5544 | 1219.06] loss=1.34 avg=2.11\n",
      "[5545 | 1219.12] loss=1.43 avg=2.11\n",
      "[5546 | 1219.18] loss=2.83 avg=2.12\n",
      "[5547 | 1219.24] loss=1.55 avg=2.11\n",
      "[5548 | 1219.30] loss=3.20 avg=2.12\n",
      "[5549 | 1219.35] loss=1.73 avg=2.12\n",
      "[5550 | 1219.41] loss=1.89 avg=2.11\n",
      "[5551 | 1219.46] loss=2.20 avg=2.11\n",
      "[5552 | 1219.52] loss=2.62 avg=2.12\n",
      "[5553 | 1219.58] loss=1.61 avg=2.11\n",
      "[5554 | 1219.64] loss=2.42 avg=2.12\n",
      "[5555 | 1219.69] loss=1.13 avg=2.11\n",
      "[5556 | 1219.75] loss=2.25 avg=2.11\n",
      "[5557 | 1219.81] loss=2.53 avg=2.11\n",
      "[5558 | 1219.86] loss=1.21 avg=2.10\n",
      "[5559 | 1219.92] loss=2.63 avg=2.11\n",
      "[5560 | 1219.97] loss=1.69 avg=2.11\n",
      "[5561 | 1220.03] loss=2.71 avg=2.11\n",
      "[5562 | 1220.09] loss=1.94 avg=2.11\n",
      "[5563 | 1220.15] loss=1.13 avg=2.10\n",
      "[5564 | 1220.20] loss=1.88 avg=2.10\n",
      "[5565 | 1220.26] loss=3.24 avg=2.11\n",
      "[5566 | 1220.31] loss=2.30 avg=2.11\n",
      "[5567 | 1220.37] loss=2.20 avg=2.11\n",
      "[5568 | 1220.43] loss=1.39 avg=2.11\n",
      "[5569 | 1220.49] loss=3.16 avg=2.12\n",
      "[5570 | 1220.54] loss=1.51 avg=2.11\n",
      "[5571 | 1220.60] loss=1.20 avg=2.10\n",
      "[5572 | 1220.65] loss=2.46 avg=2.10\n",
      "[5573 | 1220.71] loss=2.68 avg=2.11\n",
      "[5574 | 1220.76] loss=2.94 avg=2.12\n",
      "[5575 | 1220.82] loss=2.83 avg=2.13\n",
      "[5576 | 1220.88] loss=3.16 avg=2.14\n",
      "[5577 | 1220.93] loss=2.40 avg=2.14\n",
      "[5578 | 1220.99] loss=2.73 avg=2.14\n",
      "[5579 | 1221.05] loss=2.18 avg=2.14\n",
      "[5580 | 1221.10] loss=2.42 avg=2.15\n",
      "[5581 | 1221.16] loss=1.74 avg=2.14\n",
      "[5582 | 1221.22] loss=3.32 avg=2.15\n",
      "[5583 | 1221.27] loss=3.24 avg=2.17\n",
      "[5584 | 1221.33] loss=3.46 avg=2.18\n",
      "[5585 | 1221.38] loss=2.63 avg=2.18\n",
      "[5586 | 1221.44] loss=2.37 avg=2.19\n",
      "[5587 | 1221.49] loss=2.28 avg=2.19\n",
      "[5588 | 1221.55] loss=1.87 avg=2.18\n",
      "[5589 | 1221.60] loss=2.49 avg=2.19\n",
      "[5590 | 1221.66] loss=2.98 avg=2.19\n",
      "[5591 | 1221.72] loss=2.69 avg=2.20\n",
      "[5592 | 1221.77] loss=1.45 avg=2.19\n",
      "[5593 | 1221.83] loss=2.29 avg=2.19\n",
      "[5594 | 1221.89] loss=2.08 avg=2.19\n",
      "[5595 | 1221.94] loss=2.68 avg=2.20\n",
      "[5596 | 1222.00] loss=1.45 avg=2.19\n",
      "[5597 | 1222.06] loss=2.43 avg=2.19\n",
      "[5598 | 1222.11] loss=1.51 avg=2.18\n",
      "[5599 | 1222.17] loss=1.83 avg=2.18\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 46.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "r   )\n",
      "                                          else\n",
      "                                               \n",
      "                                           \n",
      "                                 \n",
      "                                    \n",
      "                                  \n",
      "                                   \n",
      "                               \n",
      "                           \n",
      "                          \n",
      "                          \n",
      "                          \n",
      "                           \n",
      "                              \n",
      "                          \n",
      "                           \n",
      "   // if (!(this.stm = this.stm.setOutput(this)) && (!(this.stm.setOutput(this))))) {\n",
      "                       \n",
      "                          \n",
      "                        \n",
      "    // if (!(this.stm = this.stm.getOutput(this)) && (!(this.stm.setOutput(this))))) {\n",
      "                      \n",
      "                      \n",
      "                       \n",
      "m                      \n",
      "                     \n",
      "                      \n",
      "             m                \n",
      "                           \n",
      "            \n",
      "                        \n",
      "            \n",
      "                    \n",
      "     ;\n",
      "                     \n",
      "             {\n",
      "                        \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5600 | 1238.15] validation loss = 2.18\n",
      "[5600 | 1238.21] loss=1.69 avg=2.18\n",
      "[5601 | 1238.27] loss=2.88 avg=2.18\n",
      "[5602 | 1238.32] loss=1.94 avg=2.18\n",
      "[5603 | 1238.37] loss=2.50 avg=2.18\n",
      "[5604 | 1238.43] loss=1.63 avg=2.18\n",
      "[5605 | 1238.48] loss=2.16 avg=2.18\n",
      "[5606 | 1238.54] loss=1.06 avg=2.17\n",
      "[5607 | 1238.60] loss=2.65 avg=2.17\n",
      "[5608 | 1238.65] loss=2.12 avg=2.17\n",
      "[5609 | 1238.71] loss=2.80 avg=2.18\n",
      "[5610 | 1238.77] loss=2.09 avg=2.18\n",
      "[5611 | 1238.83] loss=2.98 avg=2.18\n",
      "[5612 | 1238.88] loss=2.16 avg=2.18\n",
      "[5613 | 1238.94] loss=3.69 avg=2.20\n",
      "[5614 | 1238.99] loss=1.51 avg=2.19\n",
      "[5615 | 1239.04] loss=2.51 avg=2.20\n",
      "[5616 | 1239.10] loss=1.57 avg=2.19\n",
      "[5617 | 1239.16] loss=2.72 avg=2.19\n",
      "[5618 | 1239.21] loss=2.53 avg=2.20\n",
      "[5619 | 1239.27] loss=2.86 avg=2.20\n",
      "[5620 | 1239.33] loss=2.49 avg=2.21\n",
      "[5621 | 1239.38] loss=2.01 avg=2.21\n",
      "[5622 | 1239.44] loss=1.39 avg=2.20\n",
      "[5623 | 1239.50] loss=2.23 avg=2.20\n",
      "[5624 | 1239.55] loss=1.50 avg=2.19\n",
      "[5625 | 1239.61] loss=1.35 avg=2.18\n",
      "[5626 | 1239.67] loss=1.62 avg=2.18\n",
      "[5627 | 1239.73] loss=1.77 avg=2.17\n",
      "[5628 | 1239.78] loss=1.53 avg=2.17\n",
      "[5629 | 1239.84] loss=1.71 avg=2.16\n",
      "[5630 | 1239.89] loss=1.89 avg=2.16\n",
      "[5631 | 1239.95] loss=1.32 avg=2.15\n",
      "[5632 | 1240.01] loss=1.68 avg=2.15\n",
      "[5633 | 1240.07] loss=2.13 avg=2.15\n",
      "[5634 | 1240.12] loss=1.10 avg=2.14\n",
      "[5635 | 1240.18] loss=1.24 avg=2.13\n",
      "[5636 | 1240.24] loss=1.96 avg=2.12\n",
      "[5637 | 1240.29] loss=2.58 avg=2.13\n",
      "[5638 | 1240.35] loss=1.52 avg=2.12\n",
      "[5639 | 1240.41] loss=1.84 avg=2.12\n",
      "[5640 | 1240.46] loss=2.53 avg=2.12\n",
      "[5641 | 1240.52] loss=1.50 avg=2.12\n",
      "[5642 | 1240.58] loss=2.76 avg=2.12\n",
      "[5643 | 1240.63] loss=1.16 avg=2.11\n",
      "[5644 | 1240.69] loss=2.50 avg=2.12\n",
      "[5645 | 1240.75] loss=1.87 avg=2.12\n",
      "[5646 | 1240.80] loss=2.11 avg=2.12\n",
      "[5647 | 1240.86] loss=2.48 avg=2.12\n",
      "[5648 | 1240.91] loss=1.19 avg=2.11\n",
      "[5649 | 1240.97] loss=1.47 avg=2.10\n",
      "[5650 | 1241.02] loss=1.33 avg=2.10\n",
      "[5651 | 1241.08] loss=2.59 avg=2.10\n",
      "[5652 | 1241.14] loss=2.54 avg=2.11\n",
      "[5653 | 1241.19] loss=1.77 avg=2.10\n",
      "[5654 | 1241.25] loss=2.00 avg=2.10\n",
      "[5655 | 1241.31] loss=2.09 avg=2.10\n",
      "[5656 | 1241.36] loss=1.35 avg=2.09\n",
      "[5657 | 1241.41] loss=2.24 avg=2.10\n",
      "[5658 | 1241.47] loss=2.11 avg=2.10\n",
      "[5659 | 1241.52] loss=3.19 avg=2.11\n",
      "[5660 | 1241.58] loss=3.34 avg=2.12\n",
      "[5661 | 1241.64] loss=2.00 avg=2.12\n",
      "[5662 | 1241.70] loss=1.43 avg=2.11\n",
      "[5663 | 1241.75] loss=2.69 avg=2.12\n",
      "[5664 | 1241.81] loss=1.34 avg=2.11\n",
      "[5665 | 1241.86] loss=3.48 avg=2.12\n",
      "[5666 | 1241.92] loss=1.94 avg=2.12\n",
      "[5667 | 1241.98] loss=2.47 avg=2.12\n",
      "[5668 | 1242.04] loss=2.02 avg=2.12\n",
      "[5669 | 1242.09] loss=1.37 avg=2.12\n",
      "[5670 | 1242.14] loss=1.28 avg=2.11\n",
      "[5671 | 1242.20] loss=1.58 avg=2.10\n",
      "[5672 | 1242.26] loss=2.20 avg=2.10\n",
      "[5673 | 1242.31] loss=2.31 avg=2.10\n",
      "[5674 | 1242.37] loss=1.43 avg=2.10\n",
      "[5675 | 1242.42] loss=2.02 avg=2.10\n",
      "[5676 | 1242.48] loss=2.40 avg=2.10\n",
      "[5677 | 1242.54] loss=1.89 avg=2.10\n",
      "[5678 | 1242.60] loss=2.46 avg=2.10\n",
      "[5679 | 1242.65] loss=2.04 avg=2.10\n",
      "[5680 | 1242.71] loss=2.84 avg=2.11\n",
      "[5681 | 1242.77] loss=2.38 avg=2.11\n",
      "[5682 | 1242.82] loss=2.95 avg=2.12\n",
      "[5683 | 1242.88] loss=1.52 avg=2.11\n",
      "[5684 | 1242.93] loss=1.64 avg=2.11\n",
      "[5685 | 1242.98] loss=3.06 avg=2.12\n",
      "[5686 | 1243.04] loss=3.26 avg=2.13\n",
      "[5687 | 1243.10] loss=3.38 avg=2.14\n",
      "[5688 | 1243.15] loss=2.42 avg=2.15\n",
      "[5689 | 1243.21] loss=3.15 avg=2.16\n",
      "[5690 | 1243.27] loss=2.96 avg=2.16\n",
      "[5691 | 1243.32] loss=1.44 avg=2.16\n",
      "[5692 | 1243.38] loss=1.82 avg=2.15\n",
      "[5693 | 1243.44] loss=1.30 avg=2.14\n",
      "[5694 | 1243.50] loss=2.21 avg=2.14\n",
      "[5695 | 1243.55] loss=1.94 avg=2.14\n",
      "[5696 | 1243.61] loss=1.08 avg=2.13\n",
      "[5697 | 1243.66] loss=2.89 avg=2.14\n",
      "[5698 | 1243.72] loss=2.17 avg=2.14\n",
      "[5699 | 1243.78] loss=2.26 avg=2.14\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 48.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " (!                                                                                                                                                                                                                                                                              * *                                       \n",
      "                                                                    \n",
      "                       }\n",
      "                                                                                           *        }\n",
      "        *                                \n",
      "                  }\n",
      "           }\n",
      "                                            } }\n",
      "                                                    \n",
      "                                                   }\n",
      "\n",
      "      }\n",
      "\n",
      "                                             if (err != \"stored out_message \")                                       if (err == \"stored out_message )\n",
      "                                                                                                      else                             \n",
      "                                                  else\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 46.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5700 | 1259.69] validation loss = 2.17\n",
      "[5700 | 1259.76] loss=2.92 avg=2.15\n",
      "[5701 | 1259.82] loss=2.73 avg=2.15\n",
      "[5702 | 1259.87] loss=2.90 avg=2.16\n",
      "[5703 | 1259.93] loss=1.79 avg=2.16\n",
      "[5704 | 1259.99] loss=1.07 avg=2.15\n",
      "[5705 | 1260.04] loss=3.39 avg=2.16\n",
      "[5706 | 1260.09] loss=0.98 avg=2.15\n",
      "[5707 | 1260.15] loss=2.81 avg=2.15\n",
      "[5708 | 1260.21] loss=1.44 avg=2.15\n",
      "[5709 | 1260.26] loss=2.20 avg=2.15\n",
      "[5710 | 1260.32] loss=2.39 avg=2.15\n",
      "[5711 | 1260.38] loss=1.45 avg=2.14\n",
      "[5712 | 1260.44] loss=2.38 avg=2.15\n",
      "[5713 | 1260.49] loss=0.57 avg=2.13\n",
      "[5714 | 1260.55] loss=3.14 avg=2.14\n",
      "[5715 | 1260.61] loss=2.00 avg=2.14\n",
      "[5716 | 1260.66] loss=1.49 avg=2.13\n",
      "[5717 | 1260.72] loss=2.44 avg=2.14\n",
      "[5718 | 1260.77] loss=2.74 avg=2.14\n",
      "[5719 | 1260.83] loss=0.82 avg=2.13\n",
      "[5720 | 1260.89] loss=2.93 avg=2.14\n",
      "[5721 | 1260.94] loss=2.66 avg=2.14\n",
      "[5722 | 1261.00] loss=2.80 avg=2.15\n",
      "[5723 | 1261.06] loss=1.14 avg=2.14\n",
      "[5724 | 1261.12] loss=1.29 avg=2.13\n",
      "[5725 | 1261.17] loss=1.97 avg=2.13\n",
      "[5726 | 1261.22] loss=3.27 avg=2.14\n",
      "[5727 | 1261.28] loss=1.19 avg=2.13\n",
      "[5728 | 1261.34] loss=2.15 avg=2.13\n",
      "[5729 | 1261.39] loss=2.52 avg=2.13\n",
      "[5730 | 1261.45] loss=2.92 avg=2.14\n",
      "[5731 | 1261.51] loss=1.71 avg=2.14\n",
      "[5732 | 1261.56] loss=2.28 avg=2.14\n",
      "[5733 | 1261.62] loss=1.86 avg=2.14\n",
      "[5734 | 1261.68] loss=1.48 avg=2.13\n",
      "[5735 | 1261.74] loss=2.05 avg=2.13\n",
      "[5736 | 1261.79] loss=2.82 avg=2.14\n",
      "[5737 | 1261.85] loss=1.72 avg=2.13\n",
      "[5738 | 1261.90] loss=2.27 avg=2.13\n",
      "[5739 | 1261.96] loss=2.63 avg=2.14\n",
      "[5740 | 1262.01] loss=1.92 avg=2.14\n",
      "[5741 | 1262.07] loss=2.93 avg=2.14\n",
      "[5742 | 1262.13] loss=2.44 avg=2.15\n",
      "[5743 | 1262.18] loss=1.78 avg=2.14\n",
      "[5744 | 1262.24] loss=2.79 avg=2.15\n",
      "[5745 | 1262.30] loss=1.91 avg=2.15\n",
      "[5746 | 1262.35] loss=2.53 avg=2.15\n",
      "[5747 | 1262.41] loss=2.65 avg=2.16\n",
      "[5748 | 1262.47] loss=1.45 avg=2.15\n",
      "[5749 | 1262.52] loss=2.29 avg=2.15\n",
      "[5750 | 1262.58] loss=2.46 avg=2.15\n",
      "[5751 | 1262.63] loss=1.88 avg=2.15\n",
      "[5752 | 1262.69] loss=2.69 avg=2.16\n",
      "[5753 | 1262.74] loss=2.00 avg=2.15\n",
      "[5754 | 1262.79] loss=3.23 avg=2.17\n",
      "[5755 | 1262.85] loss=3.64 avg=2.18\n",
      "[5756 | 1262.91] loss=1.31 avg=2.17\n",
      "[5757 | 1262.96] loss=1.95 avg=2.17\n",
      "[5758 | 1263.02] loss=2.31 avg=2.17\n",
      "[5759 | 1263.07] loss=1.50 avg=2.16\n",
      "[5760 | 1263.13] loss=1.33 avg=2.16\n",
      "[5761 | 1263.18] loss=1.66 avg=2.15\n",
      "[5762 | 1263.24] loss=2.02 avg=2.15\n",
      "[5763 | 1263.30] loss=2.53 avg=2.15\n",
      "[5764 | 1263.35] loss=2.46 avg=2.16\n",
      "[5765 | 1263.41] loss=0.94 avg=2.14\n",
      "[5766 | 1263.46] loss=1.59 avg=2.14\n",
      "[5767 | 1263.52] loss=1.22 avg=2.13\n",
      "[5768 | 1263.58] loss=2.63 avg=2.13\n",
      "[5769 | 1263.64] loss=1.25 avg=2.13\n",
      "[5770 | 1263.69] loss=1.65 avg=2.12\n",
      "[5771 | 1263.75] loss=2.51 avg=2.12\n",
      "[5772 | 1263.80] loss=1.83 avg=2.12\n",
      "[5773 | 1263.86] loss=2.13 avg=2.12\n",
      "[5774 | 1263.92] loss=2.02 avg=2.12\n",
      "[5775 | 1263.97] loss=1.67 avg=2.12\n",
      "[5776 | 1264.02] loss=2.96 avg=2.12\n",
      "[5777 | 1264.08] loss=2.75 avg=2.13\n",
      "[5778 | 1264.14] loss=1.36 avg=2.12\n",
      "[5779 | 1264.19] loss=1.63 avg=2.12\n",
      "[5780 | 1264.25] loss=1.53 avg=2.11\n",
      "[5781 | 1264.31] loss=2.08 avg=2.11\n",
      "[5782 | 1264.36] loss=3.30 avg=2.12\n",
      "[5783 | 1264.41] loss=1.51 avg=2.12\n",
      "[5784 | 1264.47] loss=2.49 avg=2.12\n",
      "[5785 | 1264.53] loss=2.46 avg=2.13\n",
      "[5786 | 1264.58] loss=1.63 avg=2.12\n",
      "[5787 | 1264.64] loss=2.09 avg=2.12\n",
      "[5788 | 1264.70] loss=1.92 avg=2.12\n",
      "[5789 | 1264.76] loss=1.34 avg=2.11\n",
      "[5790 | 1264.82] loss=1.66 avg=2.11\n",
      "[5791 | 1264.88] loss=1.04 avg=2.09\n",
      "[5792 | 1264.93] loss=2.09 avg=2.09\n",
      "[5793 | 1264.99] loss=2.82 avg=2.10\n",
      "[5794 | 1265.05] loss=1.57 avg=2.10\n",
      "[5795 | 1265.10] loss=2.10 avg=2.10\n",
      "[5796 | 1265.16] loss=1.91 avg=2.09\n",
      "[5797 | 1265.21] loss=1.30 avg=2.09\n",
      "[5798 | 1265.27] loss=3.15 avg=2.10\n",
      "[5799 | 1265.32] loss=1.36 avg=2.09\n",
      "Generating samples...\n"
     ]
    }
   ],
   "source": [
    "def maketree(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def randomize(context, hparams, p):\n",
    "    if p > 0:\n",
    "        mask = tf.random.uniform(shape=tf.shape(input=context)) < p\n",
    "        noise = tf.random.uniform(shape=tf.shape(input=context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)\n",
    "        return tf.compat.v1.where(mask, noise, context)\n",
    "    else:\n",
    "        return context\n",
    "\n",
    "\n",
    "def main():\n",
    "    enc = get_encoder(args.model_name, \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if args.sample_length > hparams.n_ctx:\n",
    "        raise ValueError(\n",
    "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [args.batch_size, None])\n",
    "        context_in = randomize(context, hparams, args.noise)\n",
    "        output = model(hparams=hparams, X=context_in)\n",
    "        \n",
    "        val_context = tf.compat.v1.placeholder(tf.int32, [args.val_batch_size, None])\n",
    "        val_output = model(hparams=hparams, X=val_context)\n",
    "        \n",
    "\n",
    "        tf_sample = sample_sequence(\n",
    "            hparams=hparams,\n",
    "            length=args.sample_length,\n",
    "            context=context,\n",
    "            batch_size=args.batch_size,\n",
    "            temperature=1.0,\n",
    "            top_k=args.top_k)\n",
    "\n",
    "        all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
    "        train_vars = all_vars\n",
    "\n",
    "        if args.optimizer == 'adam':\n",
    "            opt = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
    "        elif args.optimizer == 'sgd':\n",
    "            opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=args.learning_rate)\n",
    "        else:\n",
    "            exit('Bad optimizer:', args.optimizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Collect Metrics for Tensorboard\n",
    "        with tf.compat.v1.name_scope('metrics'):\n",
    "            with tf.compat.v1.name_scope('train'):\n",
    "                trn_loss        = tf.reduce_mean(\n",
    "                                    input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                        labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
    "                trn_loss_summ   = tf.compat.v1.summary.scalar('loss', trn_loss)\n",
    "                \n",
    "                trn_med_ph      = tf.compat.v1.placeholder(tf.float32,shape=None,name='median')\n",
    "                trn_med_summ    = tf.compat.v1.summary.scalar('median', trn_med_ph)\n",
    "                \n",
    "                trn_mean_ph     = tf.compat.v1.placeholder(tf.float32,shape=None,name='mean')\n",
    "                trn_mean_summ   = tf.compat.v1.summary.scalar('mean', trn_mean_ph)\n",
    "            \n",
    "            with tf.compat.v1.name_scope('valid'):\n",
    "                val_loss        = tf.reduce_mean(\n",
    "                                    input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                        labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))\n",
    "                val_loss_summ   = tf.compat.v1.summary.scalar('loss', val_loss)\n",
    "\n",
    "\n",
    "\n",
    "                val_med_ph      = tf.compat.v1.placeholder(tf.float32,shape=None,name='median')\n",
    "                val_med_summ    = tf.compat.v1.summary.scalar('median', val_med_ph)\n",
    "            \n",
    "            \n",
    "            \n",
    "        trn_summaries = tf.compat.v1.summary.merge([trn_loss_summ, trn_med_summ, trn_mean_summ])\n",
    "        val_summaries = tf.compat.v1.summary.merge([val_loss_summ, val_med_summ])\n",
    "\n",
    "        opt_grads = tf.gradients(ys=trn_loss, xs=train_vars)\n",
    "        opt_grads = list(zip(opt_grads, train_vars))\n",
    "        opt_apply = opt.apply_gradients(opt_grads)\n",
    "\n",
    "        trn_summ_log = tf.compat.v1.summary.FileWriter(os.path.join(CHECKPOINT_DIR, args.run_name, 'train'))\n",
    "        val_summ_log = tf.compat.v1.summary.FileWriter(os.path.join(CHECKPOINT_DIR, args.run_name, 'valid'))\n",
    "        \n",
    "        saver = tf.compat.v1.train.Saver(\n",
    "            var_list=all_vars,\n",
    "            max_to_keep=5,\n",
    "            keep_checkpoint_every_n_hours=2)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        ckpt = tf.train.latest_checkpoint(\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "        if ckpt is None:\n",
    "            # Get fresh GPT weights if new run.\n",
    "            ckpt = tf.train.latest_checkpoint(\n",
    "                os.path.join('models', args.model_name))\n",
    "        ################ FOR FINETUNING #########################\n",
    "        ckpt = tf.train.latest_checkpoint(\n",
    "            os.path.join(CHECKPOINT_DIR, \"m1\"))\n",
    "        ################ FOR FINETUNING #########################\n",
    "\n",
    "        if args.pretrained == True:\n",
    "            print('Loading checkpoint', ckpt)\n",
    "            saver.restore(sess, ckpt)\n",
    "\n",
    "        print('Loading dataset...')\n",
    "        data_sampler = Sampler(trn_set)\n",
    "        if args.val_every > 0:\n",
    "            val_chunks = val_set\n",
    "        print('dataset has', data_sampler.total_size, 'tokens')\n",
    "        print('Training...')\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            # Sample from validation set once with fixed seed to make\n",
    "            # it deterministic during training as well as across runs.\n",
    "            val_data_sampler = Sampler(val_chunks, seed=1)\n",
    "            val_batches = [[val_data_sampler.sample(256) for _ in range(args.val_batch_size)]\n",
    "                           for _ in range(args.val_batch_count)]\n",
    "\n",
    "        counter = 1\n",
    "        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
    "        if os.path.exists(counter_path):\n",
    "            # Load the step number if we're resuming a run\n",
    "            # Add 1 so we don't immediately try to save again\n",
    "            with open(counter_path, 'r') as fp:\n",
    "                counter = int(fp.read()) + 1\n",
    "\n",
    "        def save():\n",
    "            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "            print(\n",
    "                'Saving',\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name,\n",
    "                             'model-{}').format(counter))\n",
    "            saver.save(\n",
    "                sess,\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
    "                global_step=counter)\n",
    "            with open(counter_path, 'w') as fp:\n",
    "                fp.write(str(counter) + '\\n')\n",
    "                \n",
    "            # Save metrics such as losses\n",
    "            metrics = {\n",
    "                \"trn_losses\": trn_losses,\n",
    "                \"avg_trn_losses\": trn_avgs,\n",
    "                \"val_losses\": val_losses\n",
    "            }\n",
    "\n",
    "            with open(os.path.join(CHECKPOINT_DIR, args.run_name, 'metrics.pickle'), 'wb') as f:\n",
    "                pickle.dump(metrics, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        def generate_samples():\n",
    "            print('Generating samples...')\n",
    "            context_tokens = data_sampler.sample(1)\n",
    "            all_text = []\n",
    "            index = 0\n",
    "            while index < args.sample_num:\n",
    "                out = sess.run(\n",
    "                    tf_sample,\n",
    "                    feed_dict={context: args.batch_size * [context_tokens]})\n",
    "                for i in range(min(args.sample_num - index, args.batch_size)):\n",
    "                    text = enc.decode(out[i])\n",
    "                    text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
    "                        index + 1, text)\n",
    "                    all_text.append(text)\n",
    "                    index += 1\n",
    "            print(text)\n",
    "            maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
    "            with open(\n",
    "                    os.path.join(SAMPLE_DIR, args.run_name,\n",
    "                                 'samples-{}').format(counter), 'w') as fp:\n",
    "                fp.write('\\n'.join(all_text))\n",
    "                \n",
    "        def validation():\n",
    "            print('Calculating validation loss...')\n",
    "            losses = []\n",
    "            for batch in tqdm.tqdm(val_batches):\n",
    "                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
    "            v_val_loss = np.mean(losses)\n",
    "            val_losses.append(v_val_loss)\n",
    "            v_summary = sess.run(val_summaries, feed_dict={val_loss: v_val_loss, val_med_ph: median(losses)})\n",
    "            val_summ_log.add_summary(v_summary, counter)\n",
    "            val_summ_log.flush()\n",
    "            print(\n",
    "                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
    "                .format(\n",
    "                    counter=counter,\n",
    "                    time=time.time() - start_time,\n",
    "                    loss=v_val_loss))\n",
    "\n",
    "        def sample_batch():\n",
    "            return [data_sampler.sample(256) for _ in range(args.batch_size)]\n",
    "\n",
    "\n",
    "        avg_trn_loss = (0.0, 0.1)\n",
    "#         trn_losses = [0.0]\n",
    "#         val_losses = []\n",
    "        start_time = time.time()\n",
    "#         trn_avgs = []\n",
    "\n",
    "        try:\n",
    "            for _ in range(args.iterations):\n",
    "                if counter % args.save_every == 0:\n",
    "                    save()\n",
    "                if counter % args.sample_every == 0:\n",
    "                    generate_samples()\n",
    "                if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
    "                    validation()\n",
    "                    \n",
    "                if _ == 0:\n",
    "                    avg = 0\n",
    "                else: avg = avg_trn_loss[0] / avg_trn_loss[1]\n",
    "\n",
    "                (_, v_loss, v_summary) = sess.run(\n",
    "                    (opt_apply, trn_loss, trn_summaries),\n",
    "                    feed_dict={context: sample_batch(), trn_med_ph: median(trn_losses), trn_mean_ph: avg})\n",
    "                trn_losses.append(v_loss)\n",
    "                \n",
    "                trn_summ_log.add_summary(v_summary, counter)\n",
    "\n",
    "                avg_trn_loss = (avg_trn_loss[0] * 0.99 + v_loss,\n",
    "                            avg_trn_loss[1] * 0.99 + 1.0)\n",
    "\n",
    "                trn_avgs.append(avg)\n",
    "                print(\n",
    "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
    "                    .format(\n",
    "                        counter=counter,\n",
    "                        time=time.time() - start_time,\n",
    "                        loss=v_loss,\n",
    "                        avg=avg_trn_loss[0] / avg_trn_loss[1]))\n",
    "\n",
    "                counter += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted')\n",
    "            save()\n",
    "        \n",
    "        save()\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./checkpoint/unconditional_experiment/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: model 1 finetuned on vulnerability finished training\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "with open(os.path.join(CHECKPOINT_DIR, args.run_name, 'metrics.pickle'), 'rb') as f:\n",
    "    loss_dict = pickle.load(f)\n",
    "    \n",
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
