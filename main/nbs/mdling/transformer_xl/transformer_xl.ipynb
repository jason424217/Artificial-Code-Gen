{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing fastai storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"/tf/data/models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model Pretraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_test.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_valid.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/train.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/data_clas.pkl'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_train.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/data_lm.pkl'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/tmp'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_test'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_train'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/train'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/models'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/tmp.sh'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/valid'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/sm_valid'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/.ipynb_checkpoints'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/rename.sh'),\n",
       " PosixPath('/tf/data/datasets/raw/raw_java/data00m_god-r/test')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"/tf/data/datasets/raw/raw_java/data00m_god-r\")\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path, extensions={\".java\"},\n",
    "                                processor = [OpenFileProcessor(),\n",
    "                                             SPProcessor(lang=\"en\", pre_rules=[], post_rules=[])])\n",
    "           #Inputs: all the text files in path\n",
    "            .filter_by_folder(include=['sm_train', 'sm_valid', 'sm_test']) \n",
    "#            #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .split_by_folder(valid='sm_valid', train='sm_train')\n",
    "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch(), len(data_lm.train_ds), len(data_lm.valid_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, TransformerXL, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "max_lr = 5e-2\n",
    "moms = (0.5, 0.75)\n",
    "pct_strt = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, max_lr, moms=moms, pct_start = pct_strt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, max_lr, moms=moms, pct_start = pct_strt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load(model_path, 'fit_head');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn.fit_one_cycle(10, 5e-4, moms=(0.8,0.7), pct_start = 0.02)\n",
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save(model_path, 'fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished training TransformerXL\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"public String get\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab, extensions={\".java\"},\n",
    "                                processor = [OpenFileProcessor(),\n",
    "                                             SPProcessor(lang=\"en\")])\n",
    "             .filter_by_folder(include=['sm_train', 'sm_valid'])\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(train='sm_train', valid='sm_valid')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['before', 'after'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/tf/data/datasets/raw/security_c++/security-training.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/models'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/VDISC_validate.hdf5'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/VDISC_train.hdf5'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/security-test.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/security-validation.csv'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/VDISC_test.hdf5'),\n",
       " PosixPath('/tf/data/datasets/raw/security_c++/data_clas.pkl')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = Path(\"/tf/data/datasets/raw/security_c++\")\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ItemList??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='64' class='' max='82', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      78.05% [64/82 09:39<02:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas = TextClasDataBunch.from_csv(path, 'security-training.csv', vocab=data_lm.vocab,\n",
    "                                       processor = [OpenFileProcessor(),\n",
    "                                             SPProcessor(lang=\"en\")],\n",
    "                                       text_cols = 'code', label_cols = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_clas.train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (815576 items)\n",
       "x: TextList\n",
       "▁xxunk ▁xxunk * sb , const struct stat * st , ▁xxunk const char * caption , const char * ▁xxunk ) ▁xxunk { ▁xxunk switch ( ▁xxunk & ▁xxunk ▁xxunk ) ▁xxunk { ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk { ▁xxunk char ▁xxunk + 1 ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk if ( ▁xxunk , st ) > = 0 ) ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where a ▁xxunk * file system does not support a particular system ▁xxunk * call . ▁xxunk * ▁xxunk * % ▁xxunk = > the mount point of the file system , ▁xxunk * in ▁xxunk ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk file system % s does not support the % s system \" ▁xxunk \" call \" ) , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk else ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where a ▁xxunk * file system does not support a particular system ▁xxunk * call . ▁xxunk * ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk file system does not support the % s system call \" ) , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk } ▁xxunk break ; ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk { ▁xxunk struct stat ▁xxunk ; ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk ▁xxunk , st ) ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk if ▁xxunk ( ▁xxunk ▁xxunk , ▁xxunk , & ▁xxunk ) ▁xxunk > = ▁xxunk 0 ▁xxunk ) ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where ▁xxunk * a system call is not supported for a particular ▁xxunk * device ( or ▁xxunk not supported by the device ▁xxunk * driver ) . ▁xxunk * ▁xxunk * % ▁xxunk = > the file system path of the device special file ▁xxunk * % ▁xxunk = > the type of the special file ( already translated ) ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk % s % s does not support the % s system call \" ) , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk else ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk ▁xxunk , ▁xxunk caption , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk } ▁xxunk break ; ▁xxunk default : ▁xxunk { ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk ▁xxunk , st ) ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk ▁xxunk , ▁xxunk caption , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk break ; ▁xxunk } ▁xxunk },▁xxunk ▁xxunk ( ▁xxunk range ratio _ , ▁xxunk range temperature _ , int ▁xxunk _ , bool ▁xxunk _ , int ▁xxunk _ , double ▁xxunk _ , double ▁xxunk _ ) ▁xxunk { ▁xxunk if ( ( ▁xxunk ( ) < 0 ) || ( ▁xxunk ( ) < = 0 ) ) ▁xxunk ▁xxunk = ▁xxunk ▁xxunk ) ; ▁xxunk else ▁xxunk ▁xxunk = ratio _ ; ▁xxunk if ( ( ▁xxunk ( ) < = 0 ) || ( ▁xxunk ( ) < = 0 ) ) ▁xxunk temperature = ▁xxunk range ( ▁xxunk ) ; ▁xxunk else ▁xxunk temperature = temperature _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk if ( pairs ) delete [ ] pairs ; ▁xxunk pairs = new ▁xxunk pair [ ▁xxunk ] ; ▁xxunk },▁xxunk ▁xxunk str ) ▁xxunk { ▁xxunk return ▁xxunk , ▁xxunk \" ) , 1 , str ) ; ▁xxunk },▁xxunk ▁xxunk char * var , const char * value , void * cb ) ▁xxunk { ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) { ▁xxunk ▁xxunk = ▁xxunk , value ) ; ▁xxunk if ( ▁xxunk < 0 ) ▁xxunk ▁xxunk = 0 ; / * maybe warn ? * / ▁xxunk return 0 ; ▁xxunk } ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) { ▁xxunk ▁xxunk = ▁xxunk , value ) ; ▁xxunk return 0 ; ▁xxunk } ▁xxunk if ( ! ▁xxunk , \" ▁xxunk . \" ) ) ▁xxunk return ▁xxunk , 15 , value ) ; ▁xxunk return ▁xxunk , value , cb ) ; ▁xxunk },▁xxunk ▁xxunk ▁xxunk * seq , void * v ) ▁xxunk { ▁xxunk struct ▁xxunk * dev = ▁xxunk ; ▁xxunk struct ▁xxunk ▁xxunk = { 0 , 0 } ; ▁xxunk struct ▁xxunk * ▁xxunk ; ▁xxunk int ▁xxunk = 0 ; ▁xxunk int idx , end ; ▁xxunk # define ▁xxunk ) ▁xxunk ) * ▁xxunk , 1000 ) ▁xxunk idx = ▁xxunk ) & ▁xxunk ( ▁xxunk - 1 ) ; ▁xxunk end = idx - 1 ; ▁xxunk if ( end < 0 ) ▁xxunk end = ▁xxunk - 1 ; ▁xxunk ▁xxunk = & ▁xxunk ] ; ▁xxunk while ( idx ! = end ) { ▁xxunk if ( ▁xxunk ) { ▁xxunk if ( ! ▁xxunk ) { ▁xxunk ▁xxunk = 1 ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk , \" % ▁xxunk : sec % lu ▁xxunk % lu ▁xxunk % u opcode \" ▁xxunk \" % u % s ▁xxunk ▁xxunk sec % lu ▁xxunk % lu \" ▁xxunk \" ▁xxunk ▁xxunk ▁xxunk ▁xxunk \" ▁xxunk \" ▁xxunk ▁xxunk ▁xxunk % ▁xxunk \" ▁xxunk \" ▁xxunk % ▁xxunk ▁xxunk \" , ▁xxunk idx , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk , ▁xxunk ▁xxunk = = ▁xxunk ▁xxunk ? ▁xxunk \" ▁xxunk \" : \" ▁xxunk \" , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk - ▁xxunk ) , ▁xxunk ▁xxunk - ▁xxunk ) ) ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk ; ▁xxunk if ( idx > ( ▁xxunk - 1 ) ) ▁xxunk idx = 0 ; ▁xxunk ▁xxunk = & ▁xxunk ] ; ▁xxunk } ▁xxunk # ▁xxunk ▁xxunk ▁xxunk return 0 ; ▁xxunk }\n",
       "y: CategoryList\n",
       "1,0,0,0,0\n",
       "Path: /tf/data/datasets/raw/security_c++;\n",
       "\n",
       "Valid: LabelList (203895 items)\n",
       "x: TextList\n",
       "▁xxunk ▁xxunk ▁xxunk vector < string > & args ) { ▁xxunk ▁xxunk , 2 ) ) return ( \" \") ; ▁xxunk ▁xxunk target = ▁xxunk ] ) ; ▁xxunk ▁xxunk ( ) , ▁xxunk ( ) ) ; ▁xxunk return ▁xxunk ) ; ▁xxunk },▁xxunk ▁xxunk ( int ▁xxunk ) { ▁xxunk const char * p = ▁xxunk null ; ▁xxunk ▁xxunk , ▁xxunk ? \" _ ▁xxunk prompt \" : \" _ ▁xxunk ▁xxunk \" ) ; ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ; ▁xxunk p = ▁xxunk , -1 ) ; ▁xxunk if ( p = = ▁xxunk null ) p = ( ▁xxunk ? ▁xxunk prompt : ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , 1 ) ; / * remove global * / ▁xxunk return p ; ▁xxunk },▁xxunk ▁xxunk ( ▁xxunk * painter , ▁xxunk const ▁xxunk & palette , ▁xxunk ▁xxunk , ▁xxunk double length ) ▁xxunk { ▁xxunk const double width = ▁xxunk ( length / 3.0 ) ; ▁xxunk ▁xxunk ▁xxunk ] ; ▁xxunk ▁xxunk ( length , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , width / 2 ) ; ▁xxunk ▁xxunk ( length , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , ▁xxunk / 2 ) ; ▁xxunk ▁xxunk ( ▁xxunk , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , width / 2 ) ; ▁xxunk ▁xxunk ( ▁xxunk , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , ▁xxunk / 2 ) ; ▁xxunk const int ▁xxunk = 10 ; ▁xxunk const ▁xxunk ▁xxunk = ▁xxunk ( ▁xxunk , ▁xxunk ) ; ▁xxunk const ▁xxunk ▁xxunk = ▁xxunk ( ▁xxunk , ▁xxunk ) ; ▁xxunk ▁xxunk ▁xxunk ] ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ( ▁xxunk ) ; ▁xxunk for ( int i = 0 ; i < 4 ; ▁xxunk ) ▁xxunk { ▁xxunk ▁xxunk ( ▁xxunk ] ) ; ▁xxunk ▁xxunk ( ▁xxunk ] ) ; ▁xxunk } ▁xxunk },▁xxunk ▁xxunk ( ) const { ▁xxunk if ( ! ▁xxunk ( ) ) { ▁xxunk ▁xxunk < ▁xxunk > ▁xxunk size = getsize ( ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk return ▁xxunk ( ) + ▁xxunk , ▁xxunk ▁xxunk ( ) ) ; ▁xxunk } ▁xxunk ▁xxunk < ▁xxunk > ▁xxunk name = getname ( ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk ▁xxunk > fullname = ▁xxunk ( ▁xxunk ▁xxunk ( ) ) ; ▁xxunk ▁xxunk , * ▁xxunk name ) ; ▁xxunk ▁xxunk < ▁xxunk < ▁xxunk > > ▁xxunk buf = ▁xxunk ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk ▁xxunk ) ) ; ▁xxunk return ▁xxunk ( ) ; ▁xxunk },▁xxunk ▁xxunk * list ) ▁xxunk { ▁xxunk ▁xxunk * ▁xxunk ; ▁xxunk ▁xxunk * given , * final ; ▁xxunk ▁xxunk item , * current , found ; ▁xxunk char * plugin , * plugins = ▁xxunk null ; ▁xxunk int i = 0 , ▁xxunk ; ▁xxunk if ( ! ▁xxunk , \" % ▁xxunk \" , ▁xxunk false , ▁xxunk ▁xxunk ) ) ▁xxunk { ▁xxunk return list ; ▁xxunk } ▁xxunk given = ▁xxunk ) , 0 ) ; ▁xxunk final = ▁xxunk ) , 0 ) ; ▁xxunk ▁xxunk = ▁xxunk , \" \" , \" \") ; ▁xxunk while ( ▁xxunk , & plugin ) ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk ) ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , & item ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk / * the maximum priority used for plugins not found in this list * / ▁xxunk ▁xxunk = i + 1 ; ▁xxunk ▁xxunk = ▁xxunk , ▁xxunk \" % ▁xxunk \" , ▁xxunk ) ; ▁xxunk while ( ▁xxunk , & plugin ) ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk , ▁xxunk \" % ▁xxunk \" , 0 , ▁xxunk , plugin ) ; ▁xxunk if ( ! ▁xxunk ) ▁xxunk { ▁xxunk if ( ! ▁xxunk , ▁xxunk \" % ▁xxunk \" , ▁xxunk false , ▁xxunk , plugin ) ) ▁xxunk { ▁xxunk continue ; ▁xxunk } ▁xxunk ▁xxunk = 1 ; ▁xxunk } ▁xxunk ▁xxunk = plugin ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk if ( ▁xxunk , & item , ( ▁xxunk , ▁xxunk & found ) ! = -1 ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk - ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , & item ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk plugins = ▁xxunk ( \" \" ) ; ▁xxunk ▁xxunk = ▁xxunk ) ; ▁xxunk while ( ▁xxunk , ▁xxunk ) ) ▁xxunk { ▁xxunk char * prev = plugins ; ▁xxunk if ( ▁xxunk , \" % s % s \" , plugins ? : \" \" , ▁xxunk ) < 0 ) ▁xxunk { ▁xxunk plugins = prev ; ▁xxunk break ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk ) ; ▁xxunk return plugins ; ▁xxunk }\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: /tf/data/datasets/raw/security_c++;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(30000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f545f48dbf8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/raw/security_c++'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (815576 items)\n",
       "x: TextList\n",
       "▁xxunk ▁xxunk * sb , const struct stat * st , ▁xxunk const char * caption , const char * ▁xxunk ) ▁xxunk { ▁xxunk switch ( ▁xxunk & ▁xxunk ▁xxunk ) ▁xxunk { ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk { ▁xxunk char ▁xxunk + 1 ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk if ( ▁xxunk , st ) > = 0 ) ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where a ▁xxunk * file system does not support a particular system ▁xxunk * call . ▁xxunk * ▁xxunk * % ▁xxunk = > the mount point of the file system , ▁xxunk * in ▁xxunk ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk file system % s does not support the % s system \" ▁xxunk \" call \" ) , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk else ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where a ▁xxunk * file system does not support a particular system ▁xxunk * call . ▁xxunk * ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk file system does not support the % s system call \" ) , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk } ▁xxunk break ; ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk case ▁xxunk ▁xxunk : ▁xxunk { ▁xxunk struct stat ▁xxunk ; ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk ▁xxunk , st ) ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk if ▁xxunk ( ▁xxunk ▁xxunk , ▁xxunk , & ▁xxunk ) ▁xxunk > = ▁xxunk 0 ▁xxunk ) ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk / * ▁xxunk * ▁xxunk : this error message is ▁xxunk to explain ▁xxunk * an ▁xxunk ▁xxunk or ▁xxunk ▁xxunk error in the case where ▁xxunk * a system call is not supported for a particular ▁xxunk * device ( or ▁xxunk not supported by the device ▁xxunk * driver ) . ▁xxunk * ▁xxunk * % ▁xxunk = > the file system path of the device special file ▁xxunk * % ▁xxunk = > the type of the special file ( already translated ) ▁xxunk * % ▁xxunk = > the name of the ▁xxunk system call . ▁xxunk * / ▁xxunk ▁xxunk % s % s does not support the % s system call \" ) , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk else ▁xxunk { ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk ▁xxunk , ▁xxunk caption , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk } ▁xxunk break ; ▁xxunk default : ▁xxunk { ▁xxunk char ▁xxunk ] ; ▁xxunk ▁xxunk ▁xxunk ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk & ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ▁xxunk ) ; ▁xxunk ▁xxunk , st ) ; ▁xxunk ▁xxunk ▁xxunk ( ▁xxunk sb , ▁xxunk ▁xxunk , ▁xxunk caption , ▁xxunk ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk break ; ▁xxunk } ▁xxunk },▁xxunk ▁xxunk ( ▁xxunk range ratio _ , ▁xxunk range temperature _ , int ▁xxunk _ , bool ▁xxunk _ , int ▁xxunk _ , double ▁xxunk _ , double ▁xxunk _ ) ▁xxunk { ▁xxunk if ( ( ▁xxunk ( ) < 0 ) || ( ▁xxunk ( ) < = 0 ) ) ▁xxunk ▁xxunk = ▁xxunk ▁xxunk ) ; ▁xxunk else ▁xxunk ▁xxunk = ratio _ ; ▁xxunk if ( ( ▁xxunk ( ) < = 0 ) || ( ▁xxunk ( ) < = 0 ) ) ▁xxunk temperature = ▁xxunk range ( ▁xxunk ) ; ▁xxunk else ▁xxunk temperature = temperature _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk ▁xxunk = ▁xxunk _ ; ▁xxunk if ( pairs ) delete [ ] pairs ; ▁xxunk pairs = new ▁xxunk pair [ ▁xxunk ] ; ▁xxunk },▁xxunk ▁xxunk str ) ▁xxunk { ▁xxunk return ▁xxunk , ▁xxunk \" ) , 1 , str ) ; ▁xxunk },▁xxunk ▁xxunk char * var , const char * value , void * cb ) ▁xxunk { ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) ▁xxunk return ▁xxunk , var , value ) ; ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) { ▁xxunk ▁xxunk = ▁xxunk , value ) ; ▁xxunk if ( ▁xxunk < 0 ) ▁xxunk ▁xxunk = 0 ; / * maybe warn ? * / ▁xxunk return 0 ; ▁xxunk } ▁xxunk if ( ! ▁xxunk , \" ▁xxunk \" ) ) { ▁xxunk ▁xxunk = ▁xxunk , value ) ; ▁xxunk return 0 ; ▁xxunk } ▁xxunk if ( ! ▁xxunk , \" ▁xxunk . \" ) ) ▁xxunk return ▁xxunk , 15 , value ) ; ▁xxunk return ▁xxunk , value , cb ) ; ▁xxunk },▁xxunk ▁xxunk ▁xxunk * seq , void * v ) ▁xxunk { ▁xxunk struct ▁xxunk * dev = ▁xxunk ; ▁xxunk struct ▁xxunk ▁xxunk = { 0 , 0 } ; ▁xxunk struct ▁xxunk * ▁xxunk ; ▁xxunk int ▁xxunk = 0 ; ▁xxunk int idx , end ; ▁xxunk # define ▁xxunk ) ▁xxunk ) * ▁xxunk , 1000 ) ▁xxunk idx = ▁xxunk ) & ▁xxunk ( ▁xxunk - 1 ) ; ▁xxunk end = idx - 1 ; ▁xxunk if ( end < 0 ) ▁xxunk end = ▁xxunk - 1 ; ▁xxunk ▁xxunk = & ▁xxunk ] ; ▁xxunk while ( idx ! = end ) { ▁xxunk if ( ▁xxunk ) { ▁xxunk if ( ! ▁xxunk ) { ▁xxunk ▁xxunk = 1 ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk , \" % ▁xxunk : sec % lu ▁xxunk % lu ▁xxunk % u opcode \" ▁xxunk \" % u % s ▁xxunk ▁xxunk sec % lu ▁xxunk % lu \" ▁xxunk \" ▁xxunk ▁xxunk ▁xxunk ▁xxunk \" ▁xxunk \" ▁xxunk ▁xxunk ▁xxunk % ▁xxunk \" ▁xxunk \" ▁xxunk % ▁xxunk ▁xxunk \" , ▁xxunk idx , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk , ▁xxunk ▁xxunk = = ▁xxunk ▁xxunk ? ▁xxunk \" ▁xxunk \" : \" ▁xxunk \" , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk , ▁xxunk ▁xxunk , ▁xxunk ▁xxunk - ▁xxunk ) , ▁xxunk ▁xxunk - ▁xxunk ) ) ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk ; ▁xxunk if ( idx > ( ▁xxunk - 1 ) ) ▁xxunk idx = 0 ; ▁xxunk ▁xxunk = & ▁xxunk ] ; ▁xxunk } ▁xxunk # ▁xxunk ▁xxunk ▁xxunk return 0 ; ▁xxunk }\n",
       "y: CategoryList\n",
       "1,0,0,0,0\n",
       "Path: /tf/data/datasets/raw/security_c++;\n",
       "\n",
       "Valid: LabelList (203895 items)\n",
       "x: TextList\n",
       "▁xxunk ▁xxunk ▁xxunk vector < string > & args ) { ▁xxunk ▁xxunk , 2 ) ) return ( \" \") ; ▁xxunk ▁xxunk target = ▁xxunk ] ) ; ▁xxunk ▁xxunk ( ) , ▁xxunk ( ) ) ; ▁xxunk return ▁xxunk ) ; ▁xxunk },▁xxunk ▁xxunk ( int ▁xxunk ) { ▁xxunk const char * p = ▁xxunk null ; ▁xxunk ▁xxunk , ▁xxunk ? \" _ ▁xxunk prompt \" : \" _ ▁xxunk ▁xxunk \" ) ; ▁xxunk ▁xxunk , ▁xxunk ▁xxunk ) ; ▁xxunk p = ▁xxunk , -1 ) ; ▁xxunk if ( p = = ▁xxunk null ) p = ( ▁xxunk ? ▁xxunk prompt : ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , 1 ) ; / * remove global * / ▁xxunk return p ; ▁xxunk },▁xxunk ▁xxunk ( ▁xxunk * painter , ▁xxunk const ▁xxunk & palette , ▁xxunk ▁xxunk , ▁xxunk double length ) ▁xxunk { ▁xxunk const double width = ▁xxunk ( length / 3.0 ) ; ▁xxunk ▁xxunk ▁xxunk ] ; ▁xxunk ▁xxunk ( length , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , width / 2 ) ; ▁xxunk ▁xxunk ( length , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , ▁xxunk / 2 ) ; ▁xxunk ▁xxunk ( ▁xxunk , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , width / 2 ) ; ▁xxunk ▁xxunk ( ▁xxunk , 0.0 ) ; ▁xxunk ▁xxunk ( 0.0 , ▁xxunk / 2 ) ; ▁xxunk const int ▁xxunk = 10 ; ▁xxunk const ▁xxunk ▁xxunk = ▁xxunk ( ▁xxunk , ▁xxunk ) ; ▁xxunk const ▁xxunk ▁xxunk = ▁xxunk ( ▁xxunk , ▁xxunk ) ; ▁xxunk ▁xxunk ▁xxunk ] ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ] = ▁xxunk ( 100 + ▁xxunk ) ; ▁xxunk ▁xxunk ( ▁xxunk ) ; ▁xxunk for ( int i = 0 ; i < 4 ; ▁xxunk ) ▁xxunk { ▁xxunk ▁xxunk ( ▁xxunk ] ) ; ▁xxunk ▁xxunk ( ▁xxunk ] ) ; ▁xxunk } ▁xxunk },▁xxunk ▁xxunk ( ) const { ▁xxunk if ( ! ▁xxunk ( ) ) { ▁xxunk ▁xxunk < ▁xxunk > ▁xxunk size = getsize ( ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk return ▁xxunk ( ) + ▁xxunk , ▁xxunk ▁xxunk ( ) ) ; ▁xxunk } ▁xxunk ▁xxunk < ▁xxunk > ▁xxunk name = getname ( ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk ▁xxunk > fullname = ▁xxunk ( ▁xxunk ▁xxunk ( ) ) ; ▁xxunk ▁xxunk , * ▁xxunk name ) ; ▁xxunk ▁xxunk < ▁xxunk < ▁xxunk > > ▁xxunk buf = ▁xxunk ) ; ▁xxunk if ( ▁xxunk ▁xxunk ec = ▁xxunk ( ) ) ▁xxunk return ▁xxunk ec ; ▁xxunk ▁xxunk ) ) ; ▁xxunk return ▁xxunk ( ) ; ▁xxunk },▁xxunk ▁xxunk * list ) ▁xxunk { ▁xxunk ▁xxunk * ▁xxunk ; ▁xxunk ▁xxunk * given , * final ; ▁xxunk ▁xxunk item , * current , found ; ▁xxunk char * plugin , * plugins = ▁xxunk null ; ▁xxunk int i = 0 , ▁xxunk ; ▁xxunk if ( ! ▁xxunk , \" % ▁xxunk \" , ▁xxunk false , ▁xxunk ▁xxunk ) ) ▁xxunk { ▁xxunk return list ; ▁xxunk } ▁xxunk given = ▁xxunk ) , 0 ) ; ▁xxunk final = ▁xxunk ) , 0 ) ; ▁xxunk ▁xxunk = ▁xxunk , \" \" , \" \") ; ▁xxunk while ( ▁xxunk , & plugin ) ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk ) ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , & item ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk / * the maximum priority used for plugins not found in this list * / ▁xxunk ▁xxunk = i + 1 ; ▁xxunk ▁xxunk = ▁xxunk , ▁xxunk \" % ▁xxunk \" , ▁xxunk ) ; ▁xxunk while ( ▁xxunk , & plugin ) ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk , ▁xxunk \" % ▁xxunk \" , 0 , ▁xxunk , plugin ) ; ▁xxunk if ( ! ▁xxunk ) ▁xxunk { ▁xxunk if ( ! ▁xxunk , ▁xxunk \" % ▁xxunk \" , ▁xxunk false , ▁xxunk , plugin ) ) ▁xxunk { ▁xxunk continue ; ▁xxunk } ▁xxunk ▁xxunk = 1 ; ▁xxunk } ▁xxunk ▁xxunk = plugin ; ▁xxunk ▁xxunk = ▁xxunk ; ▁xxunk if ( ▁xxunk , & item , ( ▁xxunk , ▁xxunk & found ) ! = -1 ) ▁xxunk { ▁xxunk ▁xxunk = ▁xxunk - ▁xxunk ; ▁xxunk } ▁xxunk ▁xxunk , ▁xxunk ▁xxunk , & item ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk ▁xxunk , ( ▁xxunk , ▁xxunk null ) ; ▁xxunk plugins = ▁xxunk ( \" \" ) ; ▁xxunk ▁xxunk = ▁xxunk ) ; ▁xxunk while ( ▁xxunk , ▁xxunk ) ) ▁xxunk { ▁xxunk char * prev = plugins ; ▁xxunk if ( ▁xxunk , \" % s % s \" , plugins ? : \" \" , ▁xxunk ) < 0 ) ▁xxunk { ▁xxunk plugins = prev ; ▁xxunk break ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk } ▁xxunk ▁xxunk ) ; ▁xxunk ▁xxunk ) ; ▁xxunk return plugins ; ▁xxunk }\n",
       "y: CategoryList\n",
       "0,0,0,0,0\n",
       "Path: /tf/data/datasets/raw/security_c++;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(30000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f545f48dbf8>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/tf/data/datasets/raw/security_c++'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(30000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(30000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, TransformerXL, drop_mult=0.5)\n",
    "learn.load_encoder(Path(\"/tf/data/datasets/raw/raw_java/data00m_god-r/models/fine_tuned_enc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 2e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"I really loved that movie, it was awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
