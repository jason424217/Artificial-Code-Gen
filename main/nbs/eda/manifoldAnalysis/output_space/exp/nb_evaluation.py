
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/evaluation.ipynb

from exp.nb_clustering import *
from sklearn.neighbors.kde import KernelDensity
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# Generate Mixed Gaussian Model with k components
def generate_distributions(feature_vectors, k):
    gmm = GaussianMixture(k, covariance_type="diag")
    gmm.fit(feature_vectors)

    return gmm

# From https://stackoverflow.com/questions/26079881/kl-divergence-of-two-gmms
def gmm_kl(gmm_p, gmm_q, n_samples=10**5):
    X, _ = gmm_p.sample(n_samples) # Uses monte-carlo sampling
    log_p_X = gmm_p.score_samples(X)
    log_q_X = gmm_q.score_samples(X)
    return log_p_X.mean() - log_q_X.mean()