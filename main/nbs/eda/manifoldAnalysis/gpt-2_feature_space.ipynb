{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import model\n",
    "from encoder import get_encoder\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, past=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "            'hidden_state': lm_output['hidden_state']\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output, embedding):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1),\n",
    "            next_outputs['hidden_state']\n",
    "        ]\n",
    "\n",
    "    past, prev, output, h = body(past, context, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "    return output, past, h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Embedder:\n",
    "    \n",
    "    def __init__(self, chkpt_path, chunk_size):\n",
    "        tf.compat.v1.disable_eager_execution()\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self.context = tf.compat.v1.placeholder(tf.int32, [1, None])\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.g)\n",
    "    \n",
    "        self.MAX_CHUNK = chunk_size\n",
    "        self.enc = get_encoder(\"117M\", \"models\")\n",
    "        hparams = model.default_hparams()\n",
    "        with self.g.as_default():\n",
    "            self.output, self.past, self.hidden_state = sample_sequence(\n",
    "                hparams=hparams, length=None,\n",
    "                context=self.context,\n",
    "                past=None,\n",
    "                batch_size=1,\n",
    "                temperature=1, top_k=1\n",
    "            )\n",
    "        \n",
    "        if chkpt_path is not None:\n",
    "            self.restore(chkpt_path)\n",
    "            \n",
    "    def restore(self, chkpt_path):\n",
    "        with self.g.as_default():\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "            chkpt = tf.train.latest_checkpoint(chkpt_path)\n",
    "            saver.restore(self.sess, chkpt)\n",
    "        \n",
    "    def __call__(self, method):\n",
    "        with self.g.as_default():\n",
    "\n",
    "            p = None\n",
    "            for i in range(math.ceil(len(method) / self.MAX_CHUNK)):\n",
    "                chunk = method[i * self.MAX_CHUNK : (i + 1) * self.MAX_CHUNK]\n",
    "                context_tokens = self.enc.encode(chunk)\n",
    "\n",
    "                if p is None:\n",
    "                    out, p, h = self.sess.run([self.output, self.past, self.hidden_state], feed_dict={\n",
    "                        self.context: [context_tokens]\n",
    "                    }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "                else:\n",
    "                    out, p, h = self.sess.run([self.output, self.past, self.hidden_state], feed_dict={\n",
    "                        self.context: [context_tokens],\n",
    "                        self.past: p\n",
    "                    }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "\n",
    "            return np.squeeze(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features for all Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embd = Embedder(\"/tf/src/data/gpt-2/checkpoint/run3\", 1024)\n",
    "path = \"/tf/src/data/methods/DATA00M_[god-r]\"\n",
    "\n",
    "features = {}\n",
    "for i, fname in enumerate(tqdm.tqdm(os.listdir(path))):\n",
    "    if i => 10000: break\n",
    "    with open(os.path.join(path, fname)) as f:\n",
    "        method = f.read()\n",
    "        features[method] = np.sum(embd(method), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/tf/src/data/feature_space.pickle', 'wb') as f:\n",
    "    pickle.dump(features, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 1\\nstatus: model finished training\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in features:\n",
    "    print(key, features[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
