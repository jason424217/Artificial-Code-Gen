{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's download the dataset we are going to study. The [dataset](http://ai.stanford.edu/~amaas/data/sentiment/) has been curated by Andrew Maas et al. and contains a total of 100,000 reviews on IMDB. 25,000 of them are labelled as positive and negative for training, another 25,000 are labelled for testing (in both cases they are highly polarized). The remaning 50,000 is an additional unlabelled data (but we will find a use for it nonetheless).\n",
    "\n",
    "We'll begin with a sample we've prepared for you, so that things run quickly before going over the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It only contains one csv file, let's have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path/'texts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It contains one line per review, with the label ('negative' or 'positive'), the text and a flag to determine if it should be part of the validation set or the training set. If we ignore this flag, we can create a DataBunch containing this data in one line of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = TextDataBunch.from_csv(path, 'texts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By executing this line a process was launched that took a bit of time. Let's dig a bit into it. Images could be fed (almost) directly into a model because they're just a big array of pixel values that are floats between 0 and 1. A text is composed of words, and we can't apply mathematical functions to them directly. We first have to convert them to numbers. This is done in two differents steps: tokenization and numericalization. A `TextDataBunch` does all of that behind the scenes for you.\n",
    "\n",
    "Before we delve into the explanations, let's take the time to save the things that were calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next time we launch this notebook, we can skip the cell above that took a bit of time (and that will take a lot more when you get to the full dataset) and load those results like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_data(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step of processing we make the texts go through is to split the raw sentences into words, or more exactly tokens. The easiest way to do this would be to split the string on spaces, but we can be smarter:\n",
    "\n",
    "- we need to take care of punctuation\n",
    "- some words are contractions of two different words, like isn't or don't\n",
    "- we may need to clean some parts of our texts, if there's HTML code for instance\n",
    "\n",
    "To see what the tokenizer had done behind the scenes, let's have a look at a few texts in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextClasDataBunch.from_csv(path, 'texts.csv')\n",
    "data.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts are truncated at 100 tokens for more readability. We can see that it did more than just split on space and punctuation symbols: \n",
    "- the \"'s\" are grouped together in one token\n",
    "- the contractions are separated like this: \"did\", \"n't\"\n",
    "- content has been cleaned for any HTML symbol and lower cased\n",
    "- there are several special tokens (all those that begin by xx), to replace unknown tokens (see below) or to introduce different text fields (here we only have one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have extracted tokens from our texts, we convert to integers by creating a list of all the words used. We only keep the ones that appear at least twice with a maximum vocabulary size of 60,000 (by default) and replace the ones that don't make the cut by the unknown token `UNK`.\n",
    "\n",
    "The correspondance from ids to tokens is stored in the `vocab` attribute of our datasets, in a dictionary called `itos` (for int to string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.vocab.itos[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we look at what a what's in our datasets, we'll see the tokenized text as a representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the underlying data is all numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train_ds[0][0].data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the data block API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the data block API with NLP and have a lot more flexibility than what the default factory methods offer. In the previous example for instance, the data was randomly split between train and validation instead of reading the third column of the csv.\n",
    "\n",
    "With the data block API though, we have to manually call the tokenize and numericalize steps. This allows more flexibility, and if you're not using the defaults from fastai, the various arguments to pass will appear in the step they're revelant, so it'll be more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                .split_from_df(col=2)\n",
    "                .label_from_df(cols=0)\n",
    "                .databunch())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/imdb/README'),\n",
       " PosixPath('/root/.fastai/data/imdb/test'),\n",
       " PosixPath('/root/.fastai/data/imdb/train'),\n",
       " PosixPath('/root/.fastai/data/imdb/models'),\n",
       " PosixPath('/root/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/root/.fastai/data/imdb/data_lm.pkl'),\n",
       " PosixPath('/root/.fastai/data/imdb/data_clas.pkl'),\n",
       " PosixPath('/root/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/root/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/root/.fastai/data/imdb/unsup')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/root/.fastai/data/imdb/train/pos'),\n",
       " PosixPath('/root/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " PosixPath('/root/.fastai/data/imdb/train/unsupBow.feat'),\n",
       " PosixPath('/root/.fastai/data/imdb/train/neg')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.\n",
    "\n",
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = OpenFileProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPProcessor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_folder(path, processor=[OpenFileProcessor(), SPProcessor()])\n",
    "           #Inputs: all the text files in path\n",
    "            .filter_by_folder(include=['train', 'test', 'unsup']) \n",
    "           #We may have other temp folders that contain text files so we only keep what's in train and test\n",
    "            .split_by_rand_pct(0.1)\n",
    "           #We randomly split and keep 10% (10,000 reviews) for validation\n",
    "            .label_for_lm()           \n",
    "           #We want to do a language model so we label accordingly\n",
    "            .databunch(bs=bs))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>l ' en fan t ' ▁and ▁the ▁pointless ▁and ▁offensive ▁ ' f eux ▁xxmaj ▁rouge s ' ). ▁xxmaj ▁but ▁strangely ▁enough , ▁i ▁actually ▁think ▁that ▁ ' pola ▁xxup ▁x ' ▁is ▁an ▁amazing ▁film , ▁made ▁with ▁great ▁skill ▁and ▁passion ▁by ▁a ▁master ▁of ▁his ▁craft , ▁and ▁containing ▁remarkable ▁performances . ▁xxmaj ▁the ▁film ▁does ▁carry ▁melodrama ▁to ▁more ▁extreme ▁lengths ▁than ▁i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>▁involves ▁xxmaj ▁salman ▁xxmaj ▁khan . ▁a ▁very ▁funny ▁movie ▁from ▁start ▁to ▁finish . ▁xxmaj ▁all ▁the ▁characters ▁contribute ▁to ▁the ▁movie ▁and ▁believe ▁me , ▁there ▁are ▁a ▁lot ▁of ▁them . ▁xxmaj ▁aamir ▁xxmaj ▁khan , ▁xxmaj ▁salman ▁xxmaj ▁khan , ▁xxmaj ▁ raveena ▁xxmaj ▁tan don , ▁xxmaj ▁ karishma ▁xxmaj ▁kapoor , ▁xxmaj ▁paresh ▁xxmaj ▁rawal , ▁xxmaj ▁vi ju ▁xxmaj ▁kho te , ▁xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>▁that ▁you ▁sort ▁of ▁have ▁to ▁have ▁experienced ▁xxmaj ▁japan ▁to ▁get ▁it . ▁i ▁was ▁watching ▁this ▁with ▁a ▁well - travelled ▁friend ▁who ' s ▁never ▁been ▁to ▁xxmaj ▁japan , ▁and ▁he ▁noted ▁that ▁many ▁of ▁the ▁events ▁in ▁the ▁movie ▁were ▁so ▁ludicrous ▁that ▁they ▁destroyed ▁the ▁suspension ▁of ▁disbelief . ▁xxmaj ▁my ▁reply ▁was ▁that ▁those ▁events ▁were ▁the ▁absolute ▁un var nished ▁truth ▁about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>▁he ▁plans ▁to ▁scuttle ▁his ▁sub ▁and ▁surrender ▁when , ▁just ▁at ▁the ▁last ▁moment , ▁xxmaj ▁james ▁xxmaj ▁hobson ▁( eric ▁xxmaj ▁portman ), ▁a ▁seaman ▁who ▁had ▁been ▁sullen ▁and ▁a ▁loner ▁and ▁who ▁speaks ▁xxmaj ▁german , ▁says ▁there ▁is ▁a ▁small ▁xxmaj ▁danish ▁coastal ▁village ▁that ▁had ▁been ▁a ▁fuel ▁de pot . ▁xxmaj ▁he ▁thinks ▁it ▁might ▁still ▁be ▁for ▁the ▁xxmaj ▁germans . ▁xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>▁xxmaj ▁school ▁and ▁numerous ▁others ▁with ▁a ▁different ▁edge . ▁xxmaj ▁introduction ▁of ▁a ▁new ▁xxmaj ▁general ▁instead ▁of ▁xxmaj ▁captain ▁xxmaj ▁square ▁was ▁a ▁brilliant ▁move ▁- ▁especially ▁when ▁he ▁wouldn ' t ▁cash ▁the ▁cheque ▁( something ▁that ▁is ▁rarely ▁done ▁now ). ▁xxmaj ▁it ▁follows ▁through ▁the ▁early ▁years ▁of ▁getting ▁equipment ▁and ▁uniforms , ▁starting ▁up ▁and ▁training . ▁xxmaj ▁all ▁in ▁all , ▁its ▁a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm,\n",
    "                               TransformerXL, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.308750</td>\n",
       "      <td>4.300192</td>\n",
       "      <td>0.293311</td>\n",
       "      <td>28:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1, 5e-4, moms=(0.8,0.75), pct_start = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='10', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/10 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='progress-bar-interrupted' max='3020', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      Interrupted\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 23.65 GiB total capacity; 21.20 GiB already allocated; 715.25 MiB free; 751.85 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-bf7eb3c77115>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, final_div, wd, callbacks, tot_epochs, start_epoch)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks.append(OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor, pct_start=pct_start,\n\u001b[1;32m     21\u001b[0m                                        final_div=final_div, tot_epochs=tot_epochs, start_epoch=start_epoch))\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_div\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcb_fns_registered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_listy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_loss_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fastai/text/models/awd_lstm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1371\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 GiB (GPU 0; 23.65 GiB total capacity; 21.20 GiB already allocated; 715.25 MiB free; 751.85 MiB cached)"
     ]
    }
   ],
   "source": [
    "learn.fit_one_cycle(10, 5e-4, moms=(0.8,0.7), pct_start = 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 2\\nstatus: finished training TransformerXL\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"I liked this movie because\"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I liked this movie because ▁it ▁just ▁started ▁on ▁a ▁bad ▁script . ▁xxmaj ▁the ▁script ▁was ▁poor , ▁the ▁direction ▁was ▁bad ▁and ▁the ▁actors ▁were ▁so ▁bad . ▁xxmaj ▁the ▁story ▁line ▁was ▁not ▁that ▁bad , ▁but ▁the ▁music ▁was ▁pathetic .\n",
      "I liked this movie because ▁of ▁the ▁xxmaj ▁european ▁characters . ▁xxmaj ▁the ▁film ▁was ▁a ▁complete ▁piece ▁of ▁entertainment . ▁xxmaj ▁the ▁film ▁was ▁used ▁in ▁a ▁movie ▁almost ▁as ▁a ▁time ▁film . ▁i ▁am ▁a ▁huge ▁fan ▁of ▁xxmaj ▁carpenter ' s\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_folder(path, vocab=data_lm.vocab)\n",
    "             #grab all the text files in path\n",
    "             .split_by_folder(valid='test')\n",
    "             #split by train and valid folder (that only keeps 'train' and 'test' so no need to filter)\n",
    "             .label_from_folder(classes=['neg', 'pos'])\n",
    "             #label them all with their folders\n",
    "             .databunch(bs=bs))\n",
    "\n",
    "data_clas.save('data_clas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj some have praised xxunk xxmaj lost xxmaj xxunk as a xxmaj disney adventure for adults . i do n't think so -- at least not for thinking adults . \\n \\n  xxmaj this script suggests a beginning as a live - action movie , that struck someone as the type of crap you can not sell to adults anymore . xxmaj the \" crack staff \" of</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj warning : xxmaj does contain spoilers . \\n \\n  xxmaj open xxmaj your xxmaj eyes \\n \\n  xxmaj if you have not seen this film and plan on doing so , just stop reading here and take my word for it . xxmaj you have to see this film . i have seen it four times so far and i still have n't made up my</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj heavy - handed moralism . xxmaj writers using characters as mouthpieces to speak for themselves . xxmaj predictable , plodding plot points ( say that five times fast ) . a child 's imitation of xxmaj britney xxmaj spears . xxmaj this film has all the earmarks of a xxmaj lifetime xxmaj special reject . \\n \\n  i honestly believe that xxmaj jesus xxmaj xxunk and xxmaj</td>\n",
       "      <td>neg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>xxbos xxmaj in xxup nyc , seaman xxmaj michael o'hara ( xxmaj orson xxmaj welles ) rescues xxmaj elsa xxmaj bannister ( xxmaj rita xxmaj hayworth ) from a mugging &amp; rape as she takes a horse &amp; carriage through xxmaj central xxmaj park -and lives to regret it . xxmaj xxunk - haired xxmaj hayworth 's a platinum blonde in this one ; as dazzling as fresh - fallen</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie has an all star cast , xxmaj john xxmaj candy , xxmaj richard xxmaj lewis , xxmaj ornella xxmaj xxunk , xxmaj cybill xxmaj shepard , and xxmaj jim xxmaj belushi to name a few , run amuck in xxmaj monte xxmaj carlo , as well as some other beautiful xxmaj european locations , and is very funny . xxmaj the trouble that everyone gets in when they lie to protect themselves is great , and i highly recommend that you see this movie , it is well worth it ! xxmaj john xxmaj candy is in top form in xxmaj once xxmaj upon a xxmaj crime , as is everyone else ! xxmaj if you and your family are looking for a great family film , this is your ticket . xxmaj everyone gives stellar performances , great acting , great comedy , and great timing , which is rare in movies these days . xxmaj great plot , great mystery , ( which i love anyways ) and overall , well worth the money you spend on it . xxmaj so get the kids , grab some popcorn , juice , or tea , or sodas , and enjoy the show xxrep 4 !,xxbos xxmaj the image of movie studios being financially - driven instead of creatively is not without truth ( in fact , it 's more true than false ) . xxmaj this begs the question why xxmaj castle xxmaj rock xxmaj entertainment allowed xxmaj kenneth xxmaj branagh to create a full - length , uncut version of \" xxmaj hamlet \" with his complete creative control among other things . xxmaj of course , xxmaj branagh had to agree to some concessions ( a star - studded cast , and a 2.5 hour version for wider release ) , but why would the film studio allow xxmaj branagh to spend money on a 4 hour version that they knew few would see ? xxmaj could they have , at least in this case , had enough respect for the material and xxmaj branagh 's vision to create something for only a few people ? xxmaj that is not a question that i can answer . xxmaj whatever the reason , this is a glorious vision for those who are willing to spend four hours watching \" xxmaj hamlet . \" xxmaj everyone knows the story , so i will not spend much time on that . xxmaj however , unlike other productions of the play , stage included , this is a completely uncut production , which has never been done before . xxmaj according to some , xxmaj shakespeare never intended for the play to be produced uncut , leaving the decision of what to include to the director 's discretion . xxmaj that being said , i have no doubt that had he been able to see it , the xxmaj bard would have been overjoyed with xxmaj branagh 's production . \n",
       " \n",
       "  xxmaj the film is top - heavy with film stars , although most have mere bit parts . xxmaj all play their parts equally well . i would have thought xxmaj branagh too old to play the part of xxmaj hamlet , and while he still may be , his performance more than makes up for it . xxmaj hamlet is a complex part , displaying every emotion from grief to anger , happiness to madness , and everything in between . xxmaj branagh nailed it . xxmaj derek xxmaj jacobi is terrific as the wily xxmaj claudius , whose deception and treachery sets all these things in motion ; his unique voice is perfect for the role . xxmaj julie xxmaj christie is also very good as xxmaj gertrude , xxmaj hamlet 's caring mother who does n't realize what is going on until late in the game . \n",
       " \n",
       "  xxmaj the classical actors are cast in bit parts ( xxmaj judi xxmaj dench is on for all of 60 seconds and has no lines ) , but at least they 're in it . xxmaj surprisingly , no one takes this to heart ; everyone gives it their all , and it shows . xxmaj special mention has to go to xxmaj jack xxmaj lemmon and xxmaj billy xxmaj crystal , who are excellent . xxmaj robin xxmaj williams is a little too silly , but he 's not bad ( his part is pretty small anyway ) . \n",
       " \n",
       "  xxmaj yet , this is undeniably xxmaj branagh 's show . xxmaj he adapted one of the most famous plays in history , and in so doing , he took on a whale of a project ; it 's impressive that he got it done , but the fact that the film is this good is a monumental achievement . xxmaj what i really liked about this film is that you do n't have to be a xxmaj shakespeare scholar to enjoy it . xxmaj as most people know , xxmaj shakespeare is difficult to digest , but xxmaj branagh and his cast understand this . \" xxmaj hamlet \" is still immensely enjoyable to just sit and listen to the actors deliver the brilliant dialogue and excellent acting . \n",
       " \n",
       "  xxmaj this is a must see for anyone and everyone . xxmaj it may be four hours long , but it 's definitely worth it .,xxbos xxmaj first one has to take into account the time period this film was made in . 1995 . xxmaj rappers were in it , and that added to the flair of it . \n",
       " \n",
       "  xxmaj remy was a socially awkward teen trying to find his way and could n't , until he met and was befriended by xxmaj nazis . xxmaj they took him in . xxmaj nazi 's are n't all this awkward , but like most gangs , they fill a void that is missing be it economic , social , emotional , whatever . xxmaj michael xxmaj rappaport played the part perfectly . \n",
       " \n",
       "  xxmaj omar xxmaj epps was the hot shot track star , with a questionable work ethic and a chip on his shoulder . xxmaj he kept trying to feel sorry for himself and his plight , and had his girlfriend and professor to straighten him out on it . \n",
       " \n",
       "  xxmaj kristen was a young white girl trying to find herself and trying to fit in , until she was date raped . xxmaj she then found her self experiment with her sexuality , and getting involved politically . \n",
       " \n",
       "  xxmaj this film deals with racism and like most things that deal with racism , people 's own perspectives come into play . \n",
       " \n",
       "  i read so many comments about how there were no ' evil ' black characters but there were evil white one ( xxmaj nazis ) . xxmaj so what ? xxmaj remy was n't portrayed as evil at all , he was trying to find his way , and kept failing until some skinheads accepted him . xxmaj he was scared , it was sad to see him devolve how he did . xxmaj he even says right before he kills himself , i did n't mean it , i wanted to be an engineer . \n",
       " \n",
       "  xxmaj ice xxmaj cube and xxmaj busta xxmaj rhymes were angry black men , xxmaj ice xxmaj cube was somewhat of an intellectual , and xxmaj busta xxmaj rhymes was just portrayed as a dumb thug . xxmaj they both showed no consideration at all for their roommates , and generally appear to not like white people very much . xxmaj they were angry like the xxmaj nazis but not on the level of xxmaj nazis in terms of overall badness . xxmaj sorry if this makes it seem unfair , but are there really black groups like the xxmaj nazis ? xxmaj no . \n",
       " \n",
       "  xxmaj people say it shows xxunk black = good . xxmaj not true , the only bad white characters were the xxmaj nazis and the police , which is more or less true in real life . xxmaj kristen was a good girl , her boyfriend ( omar epps roommate ) was a good guy , and even xxmaj remy was a good guy , he was just misguided . \n",
       " \n",
       "  xxmaj omar xxmaj epps , xxmaj ice xxmaj cube and xxmaj busta were seriously flawed characters , angry and inconsiderate . xxmaj although their constant harassment by police seemed to justify some of their anger . xxmaj remy 's inability to fit in seemed to justify his anger as well . \n",
       " \n",
       "  xxmaj good movie , well done . xxmaj like all movies that deal with racism , its a great piece to get a discussion going . \n",
       " \n",
       "  i do n't think xxmaj cube and xxmaj busta coulda beat those xxmaj nazis though .,xxbos xxmaj walt xxmaj disney 's \" xxmaj the xxmaj rookie \" is based on the story of xxmaj jim xxmaj morris , a former minor league xxunk who made one of the most amazing comebacks in sports history , ending an almost 10 year retirement and making his xxmaj major xxmaj league debut in 1999 at the age of 35 . \n",
       " \n",
       "  xxmaj the film opens with a brief synopsis of xxmaj morris ' childhood , which included a series of re - locations - his father was a military man . xxmaj and even when his family settled for good in football crazed xxmaj texas , xxmaj morris ' passion for baseball remained strong . \n",
       " \n",
       "  xxmaj the childhood segment then jumps ahead about 23 years to the adult xxmaj morris ( played by xxmaj dennis xxmaj quaid ) who is now a baseball coach and chemistry teacher at xxmaj big xxmaj lake xxmaj high xxmaj school ( in real life it was xxmaj reagan xxmaj county xxmaj high xxmaj school in xxmaj big xxmaj lake , xxmaj texas ) . xxmaj it is mentioned that he attempted a career as a baseball player but that it did n't work out . \n",
       " \n",
       "  xxmaj morris 's team is struggling and he lectures them about giving up on their dreams . xxmaj they turn the table on him , telling him that he should try out for a xxmaj major xxmaj league team . xxmaj at several times when he pitches to them in practice , they express amazement at the speed with which he throws . xxmaj morris seems unconvinced but agrees to a deal with his players in which if they win district , he will try out for a xxmaj major xxmaj league team . \n",
       " \n",
       "  xxmaj big xxmaj lake does win district and , adhering to his end of the deal , xxmaj morris attends a xxmaj tampa xxmaj bay xxmaj devil xxmaj rays try out . xxmaj phenomenally , he throws 98 miles an hour - faster than he threw during his minor league career and an outstanding speed even for a xxmaj major xxmaj league pitcher . xxmaj after another try out with the team , xxmaj morris is offered a contract with the xxmaj devil xxmaj rays . \n",
       " \n",
       "  xxmaj this leaves him with a tough decision - stay in his comfortable life or once again pursue his xxmaj major xxmaj league dream by going through the minor league grind of making little money and spending months at a time away from home . xxmaj and the decision is even more agonizing than during his first minor league stint because he now has a wife and three children . \n",
       " \n",
       "  xxmaj morris signs with the xxmaj devil xxmaj rays , begins at the xxup aa level and moves up quickly to the xxup aaa level , one level below xxmaj major xxmaj league xxmaj baseball . xxmaj but as the season winds down , the chances of him getting \" called up \" grow increasingly slim . \n",
       " \n",
       "  xxmaj for the most part , i love this movie . xxmaj there are lots of great performances and likable characters and it 's easy to find yourself really pulling for xxmaj morris . xxmaj also , the movie does a great job portraying professional baseball at both the major and minor league levels . xxmaj and most of all , it teaches the timeless message of holding tight to your dreams even when they seem distant and almost impossible to achieve . \n",
       " \n",
       "  xxmaj still , the movie has some flaws . xxmaj while generally accurate , it exaggerates and even fabricates a few things . xxmaj check out http : / / xxunk / xxunk / s / closer / xxunk for some examples . xxmaj also , except for one scene in which he prays with his players , the movie completely ignores xxmaj morris ' xxmaj christian faith . xxmaj but considering xxmaj disney 's left wing zeal , that 's not surprising . \n",
       " \n",
       "  xxmaj presumably , a lot of the exaggerations / fabrications were done to make the story more dramatic . xxmaj yet the 20 minute documentary on xxmaj morris that is included on the xxup dvd features some information that makes his story more dramatic but is excluded from the movie . \n",
       " \n",
       "  xxmaj for example , from birth until his family settled in xxmaj texas for good when he was 12 , xxmaj morris re - located 14 times . xxmaj and his initial minor league career ended after four surgeries through which he lost half of the muscle in his left ( pitching ) shoulder , thus making his throwing 98 mph even more inexplicable . \n",
       " \n",
       "  xxmaj to fully appreciate and understand the story of xxmaj jim xxmaj morris , it 's good to not only watch \" xxmaj the xxmaj rookie \" but to watch the xxup dvd 's documentary , check out the aforementioned link to the movie 's inaccuracies and probably also to read xxmaj morris ' biography , also titled \" xxmaj the xxmaj rookie . \" i have n't read the book but i hope to one of these days . \n",
       " \n",
       "  xxmaj but overall , \" xxmaj the xxmaj rookie \" is a very good portrayal of a miraculous story and is a powerful testament to the power of dreams and the triumph of the common man . 8 / 10,xxbos i saw this movie when i was about 8-years - old and i liked it but it was n't until i watched it again at the age of 13 that i really understood it for what it is ; a cartoon about a criminal dog with a real heart of gold \" adopts \" a little girl in order to exploit her for her talents to talk to animals . xxmaj the dog star , xxmaj charlie xxup b. xxmaj barkin , is murdered by his formal business partner , xxmaj carface , ( who is absolutely diabolical by the way ) . xxmaj his soul then goes to where else but xxmaj heaven only to find a golden watch that is really his life 's time , which xxmaj charlie , being the sneaky but lovable cad that he is steals and rewinds , sending him back to xxmaj earth . xxmaj once back on xxmaj earth , xxmaj charlie goes about seeking revenge on the evil xxmaj carface . xxmaj this is how he comes upon young xxmaj anne - xxmaj marie , the lonely little orphan that can talk to animals whom xxmaj charlie plans to scam for her talents in order to get back at his enemy xxmaj carface . xxmaj but scoundrel xxmaj charlie actually comes to care for young xxmaj anne - xxmaj marie and his plans xxunk as he must make up his mind to do what is right after xxmaj anne - xxmaj marie discovers what her \" best friend \" xxmaj charlie has really been using her to make money for a new and better dog casino . xxmaj now he must rescue her from the dreaded xxmaj carface . i still love this movie even at the age of 22 . xxmaj the idea and plot really are quite different and original from that of many other animated films . i especially like the idea that a dog plays the role of the villain for once . xxmaj carface was even better than he was in the xxmaj all xxmaj dogs go to xxmaj heaven sequel . xxmaj in that picture he appeared quite dubious to his role of villain .\n",
       "y: CategoryList\n",
       "pos,pos,pos,pos,pos\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj no one would ever question that director xxmaj leos xxmaj carax is a genius , but what we wonder about is : is he an insane genius ? xxmaj so many people hated this film ! i am normally the first person to accuse many xxmaj french directors of making offensive , boring , disgusting and pretentious films ( such as the horrible recent film ' l'enfant ' and the pointless and offensive ' xxmaj feux xxmaj rouges ' ) . xxmaj but strangely enough , i actually think that ' xxmaj pola x ' is an amazing film , made with great skill and passion by a master of his craft , and containing remarkable performances . xxmaj the film does carry melodrama to more extreme lengths than i believe i have ever seen on screen before . xxmaj but then , xxmaj carax is extreme , that we know . xxmaj the film also contains what i consider way over - the - top xxmaj trotskyite or xxmaj anarchist fantasies and wet - dreams , what with a mysterious group of young men training to fire machine guns at the bourgeoisie in between playing xxmaj scott xxmaj walker 's rather fascinating music in a band which has its recording sessions in an abandoned warehouse filled with squatters and fires burning in old steel barrels . xxmaj guillaume xxmaj depardieu plays a rich young man in a château ( whose step - mother is xxmaj catherine xxmaj deneuve , and he wanders into her bathroom while she is naked in the bath , by the way ) . xxmaj but he suddenly ' snaps ' completely when he discovers that his deceased father , a famous diplomat , had fathered an illegitimate daughter who had been effectively disposed of by xxmaj deneuve as an inconvenience . xxmaj this is because the sister suddenly turns up as a kind of xxmaj romanian refugee with wild dishevelled hair , expressionless face , and little ability to speak xxmaj french coherently . xxmaj depardieu then transforms himself into a ' class hero ' of the far left and wants to kill or destroy his family for their hypocrisy and corruption , and lives in squalor and extreme poverty , while scorning a vast inheritance . xxmaj he then commences an incestuous sexual relationship with his half - sister , which is shown in an explicit sex scene which has offended many people , though i have no objection to it , as i think people are far too hysterical about sex , especially in xxmaj america , where apparently it never happens . xxmaj the intensity of the acting and the filming make this unlikely scenario come off as an experience of powerful , if depressing , hyper - melodrama . xxmaj the differences between xxmaj carax making an extreme film like this and the numerous extreme xxmaj french films which i think are pretentious and disgusting are ( 1 ) that xxmaj carax is an excellent filmmaker , and ( 2 ) he is seriously attempting to explore a meaningful , if harrowing , extreme emotional condition , whereby a human being disintegrates and turns against his background . xxmaj many would say that the extreme elements in this film were gratuitous , but i do n't agree . i believe xxmaj carax was genuine , and was not making an exploitation picture at all . xxmaj it is very difficult to defend a man who goes that far and who , for all i know , may be a complete madman , but i believe he deserves defending for this remarkable cinematic achievement .,xxbos a picture starring xxmaj danny xxmaj devito and xxmaj billy xxmaj crystal . xxmaj they are both famous actors , and in this charming comedy entails them trying to murder each other 's pet peeve . xxmaj billy xxmaj crystal 's pet peeve being his ex wife who stole his book and put her name on it , and xxmaj danny xxmaj devito 's pet peeve being his malignant mother . xxmaj billy xxmaj crystal is an author and a teacher apparently and xxmaj danny xxmaj devito is one of his students . xxmaj this comedy classic is very entertaining and for all ages . \n",
       " \n",
       "  xxmaj danny xxmaj devito seems to act like a child in this movie because of his evil mother who keeps putting him down all the time . xxmaj she says things like \" xxmaj you have no friends ! \" and when xxmaj billy xxmaj crystal fell down , she said \" xxmaj burry him before he stinks up the basement ! \" . xxmaj she puts down xxmaj danny also by calling him \" xxmaj lard ass ! \" and other things just to make him angry .,xxbos xxmaj funny , yes . a xxmaj freleng classic ! xxmaj to watch xxmaj sylvester turn green is always a treat , and it brings us back to the days when cartoon slapstick was brave and geared for the adult mind . \n",
       " \n",
       "  xxmaj loved it !,xxbos xxmaj there is a need for this kind of entertainment in our modern world . xxmaj you can watch \" xxmaj ma and xxmaj pa \" with adults , with your family ( kids any age or just by yourself like me . xxmaj they are gentle , but gentle is so refreshing in a society of kids killing kids , a horrible war , inappropriate prime time television and poverty . xxmaj we do n't even get a hint of where all of those children came from ! xxmaj give me modern plumbing and i 'll gladly become a xxmaj kettle . xxmaj humor does xxup not require offensive language . xxmaj it is hard to follow conversations in shows where every other word is bleeped . xxmaj relax , take your shoes off , and climb in your recliner with a good old - fashioned glass of lemonade , and just breathe easy watching xxmaj ma sweeping the chickens off the table at lunch time ! xxmaj pj,xxbos xxmaj chris xxmaj noth plays a maniac who wrote a children 's book as a young lad , posing as a doctor who menaces a community in order to push his sister 's son into the position as his school 's genius . xxmaj the only man that can stop him is a cop who plays by his own rules , and is forced to go undercover with the help of his mute father who communicates via xxunk board . xxmaj will xxmaj rick xxmaj springfield find out what s up ? xxmaj or will somebody sabotage his recording studio ! xxmaj does the mentally retarded boy who operates the book mobile have clues ? xxmaj watch to find out ! i 'm not making this up !\n",
       "y: CategoryList\n",
       "pos,pos,pos,pos,pos\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(60000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f1a9f317c80>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\n",
       "learn: RNNLearner(data=TextClasDataBunch;\n",
       "\n",
       "Train: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj this movie has an all star cast , xxmaj john xxmaj candy , xxmaj richard xxmaj lewis , xxmaj ornella xxmaj xxunk , xxmaj cybill xxmaj shepard , and xxmaj jim xxmaj belushi to name a few , run amuck in xxmaj monte xxmaj carlo , as well as some other beautiful xxmaj european locations , and is very funny . xxmaj the trouble that everyone gets in when they lie to protect themselves is great , and i highly recommend that you see this movie , it is well worth it ! xxmaj john xxmaj candy is in top form in xxmaj once xxmaj upon a xxmaj crime , as is everyone else ! xxmaj if you and your family are looking for a great family film , this is your ticket . xxmaj everyone gives stellar performances , great acting , great comedy , and great timing , which is rare in movies these days . xxmaj great plot , great mystery , ( which i love anyways ) and overall , well worth the money you spend on it . xxmaj so get the kids , grab some popcorn , juice , or tea , or sodas , and enjoy the show xxrep 4 !,xxbos xxmaj the image of movie studios being financially - driven instead of creatively is not without truth ( in fact , it 's more true than false ) . xxmaj this begs the question why xxmaj castle xxmaj rock xxmaj entertainment allowed xxmaj kenneth xxmaj branagh to create a full - length , uncut version of \" xxmaj hamlet \" with his complete creative control among other things . xxmaj of course , xxmaj branagh had to agree to some concessions ( a star - studded cast , and a 2.5 hour version for wider release ) , but why would the film studio allow xxmaj branagh to spend money on a 4 hour version that they knew few would see ? xxmaj could they have , at least in this case , had enough respect for the material and xxmaj branagh 's vision to create something for only a few people ? xxmaj that is not a question that i can answer . xxmaj whatever the reason , this is a glorious vision for those who are willing to spend four hours watching \" xxmaj hamlet . \" xxmaj everyone knows the story , so i will not spend much time on that . xxmaj however , unlike other productions of the play , stage included , this is a completely uncut production , which has never been done before . xxmaj according to some , xxmaj shakespeare never intended for the play to be produced uncut , leaving the decision of what to include to the director 's discretion . xxmaj that being said , i have no doubt that had he been able to see it , the xxmaj bard would have been overjoyed with xxmaj branagh 's production . \n",
       " \n",
       "  xxmaj the film is top - heavy with film stars , although most have mere bit parts . xxmaj all play their parts equally well . i would have thought xxmaj branagh too old to play the part of xxmaj hamlet , and while he still may be , his performance more than makes up for it . xxmaj hamlet is a complex part , displaying every emotion from grief to anger , happiness to madness , and everything in between . xxmaj branagh nailed it . xxmaj derek xxmaj jacobi is terrific as the wily xxmaj claudius , whose deception and treachery sets all these things in motion ; his unique voice is perfect for the role . xxmaj julie xxmaj christie is also very good as xxmaj gertrude , xxmaj hamlet 's caring mother who does n't realize what is going on until late in the game . \n",
       " \n",
       "  xxmaj the classical actors are cast in bit parts ( xxmaj judi xxmaj dench is on for all of 60 seconds and has no lines ) , but at least they 're in it . xxmaj surprisingly , no one takes this to heart ; everyone gives it their all , and it shows . xxmaj special mention has to go to xxmaj jack xxmaj lemmon and xxmaj billy xxmaj crystal , who are excellent . xxmaj robin xxmaj williams is a little too silly , but he 's not bad ( his part is pretty small anyway ) . \n",
       " \n",
       "  xxmaj yet , this is undeniably xxmaj branagh 's show . xxmaj he adapted one of the most famous plays in history , and in so doing , he took on a whale of a project ; it 's impressive that he got it done , but the fact that the film is this good is a monumental achievement . xxmaj what i really liked about this film is that you do n't have to be a xxmaj shakespeare scholar to enjoy it . xxmaj as most people know , xxmaj shakespeare is difficult to digest , but xxmaj branagh and his cast understand this . \" xxmaj hamlet \" is still immensely enjoyable to just sit and listen to the actors deliver the brilliant dialogue and excellent acting . \n",
       " \n",
       "  xxmaj this is a must see for anyone and everyone . xxmaj it may be four hours long , but it 's definitely worth it .,xxbos xxmaj first one has to take into account the time period this film was made in . 1995 . xxmaj rappers were in it , and that added to the flair of it . \n",
       " \n",
       "  xxmaj remy was a socially awkward teen trying to find his way and could n't , until he met and was befriended by xxmaj nazis . xxmaj they took him in . xxmaj nazi 's are n't all this awkward , but like most gangs , they fill a void that is missing be it economic , social , emotional , whatever . xxmaj michael xxmaj rappaport played the part perfectly . \n",
       " \n",
       "  xxmaj omar xxmaj epps was the hot shot track star , with a questionable work ethic and a chip on his shoulder . xxmaj he kept trying to feel sorry for himself and his plight , and had his girlfriend and professor to straighten him out on it . \n",
       " \n",
       "  xxmaj kristen was a young white girl trying to find herself and trying to fit in , until she was date raped . xxmaj she then found her self experiment with her sexuality , and getting involved politically . \n",
       " \n",
       "  xxmaj this film deals with racism and like most things that deal with racism , people 's own perspectives come into play . \n",
       " \n",
       "  i read so many comments about how there were no ' evil ' black characters but there were evil white one ( xxmaj nazis ) . xxmaj so what ? xxmaj remy was n't portrayed as evil at all , he was trying to find his way , and kept failing until some skinheads accepted him . xxmaj he was scared , it was sad to see him devolve how he did . xxmaj he even says right before he kills himself , i did n't mean it , i wanted to be an engineer . \n",
       " \n",
       "  xxmaj ice xxmaj cube and xxmaj busta xxmaj rhymes were angry black men , xxmaj ice xxmaj cube was somewhat of an intellectual , and xxmaj busta xxmaj rhymes was just portrayed as a dumb thug . xxmaj they both showed no consideration at all for their roommates , and generally appear to not like white people very much . xxmaj they were angry like the xxmaj nazis but not on the level of xxmaj nazis in terms of overall badness . xxmaj sorry if this makes it seem unfair , but are there really black groups like the xxmaj nazis ? xxmaj no . \n",
       " \n",
       "  xxmaj people say it shows xxunk black = good . xxmaj not true , the only bad white characters were the xxmaj nazis and the police , which is more or less true in real life . xxmaj kristen was a good girl , her boyfriend ( omar epps roommate ) was a good guy , and even xxmaj remy was a good guy , he was just misguided . \n",
       " \n",
       "  xxmaj omar xxmaj epps , xxmaj ice xxmaj cube and xxmaj busta were seriously flawed characters , angry and inconsiderate . xxmaj although their constant harassment by police seemed to justify some of their anger . xxmaj remy 's inability to fit in seemed to justify his anger as well . \n",
       " \n",
       "  xxmaj good movie , well done . xxmaj like all movies that deal with racism , its a great piece to get a discussion going . \n",
       " \n",
       "  i do n't think xxmaj cube and xxmaj busta coulda beat those xxmaj nazis though .,xxbos xxmaj walt xxmaj disney 's \" xxmaj the xxmaj rookie \" is based on the story of xxmaj jim xxmaj morris , a former minor league xxunk who made one of the most amazing comebacks in sports history , ending an almost 10 year retirement and making his xxmaj major xxmaj league debut in 1999 at the age of 35 . \n",
       " \n",
       "  xxmaj the film opens with a brief synopsis of xxmaj morris ' childhood , which included a series of re - locations - his father was a military man . xxmaj and even when his family settled for good in football crazed xxmaj texas , xxmaj morris ' passion for baseball remained strong . \n",
       " \n",
       "  xxmaj the childhood segment then jumps ahead about 23 years to the adult xxmaj morris ( played by xxmaj dennis xxmaj quaid ) who is now a baseball coach and chemistry teacher at xxmaj big xxmaj lake xxmaj high xxmaj school ( in real life it was xxmaj reagan xxmaj county xxmaj high xxmaj school in xxmaj big xxmaj lake , xxmaj texas ) . xxmaj it is mentioned that he attempted a career as a baseball player but that it did n't work out . \n",
       " \n",
       "  xxmaj morris 's team is struggling and he lectures them about giving up on their dreams . xxmaj they turn the table on him , telling him that he should try out for a xxmaj major xxmaj league team . xxmaj at several times when he pitches to them in practice , they express amazement at the speed with which he throws . xxmaj morris seems unconvinced but agrees to a deal with his players in which if they win district , he will try out for a xxmaj major xxmaj league team . \n",
       " \n",
       "  xxmaj big xxmaj lake does win district and , adhering to his end of the deal , xxmaj morris attends a xxmaj tampa xxmaj bay xxmaj devil xxmaj rays try out . xxmaj phenomenally , he throws 98 miles an hour - faster than he threw during his minor league career and an outstanding speed even for a xxmaj major xxmaj league pitcher . xxmaj after another try out with the team , xxmaj morris is offered a contract with the xxmaj devil xxmaj rays . \n",
       " \n",
       "  xxmaj this leaves him with a tough decision - stay in his comfortable life or once again pursue his xxmaj major xxmaj league dream by going through the minor league grind of making little money and spending months at a time away from home . xxmaj and the decision is even more agonizing than during his first minor league stint because he now has a wife and three children . \n",
       " \n",
       "  xxmaj morris signs with the xxmaj devil xxmaj rays , begins at the xxup aa level and moves up quickly to the xxup aaa level , one level below xxmaj major xxmaj league xxmaj baseball . xxmaj but as the season winds down , the chances of him getting \" called up \" grow increasingly slim . \n",
       " \n",
       "  xxmaj for the most part , i love this movie . xxmaj there are lots of great performances and likable characters and it 's easy to find yourself really pulling for xxmaj morris . xxmaj also , the movie does a great job portraying professional baseball at both the major and minor league levels . xxmaj and most of all , it teaches the timeless message of holding tight to your dreams even when they seem distant and almost impossible to achieve . \n",
       " \n",
       "  xxmaj still , the movie has some flaws . xxmaj while generally accurate , it exaggerates and even fabricates a few things . xxmaj check out http : / / xxunk / xxunk / s / closer / xxunk for some examples . xxmaj also , except for one scene in which he prays with his players , the movie completely ignores xxmaj morris ' xxmaj christian faith . xxmaj but considering xxmaj disney 's left wing zeal , that 's not surprising . \n",
       " \n",
       "  xxmaj presumably , a lot of the exaggerations / fabrications were done to make the story more dramatic . xxmaj yet the 20 minute documentary on xxmaj morris that is included on the xxup dvd features some information that makes his story more dramatic but is excluded from the movie . \n",
       " \n",
       "  xxmaj for example , from birth until his family settled in xxmaj texas for good when he was 12 , xxmaj morris re - located 14 times . xxmaj and his initial minor league career ended after four surgeries through which he lost half of the muscle in his left ( pitching ) shoulder , thus making his throwing 98 mph even more inexplicable . \n",
       " \n",
       "  xxmaj to fully appreciate and understand the story of xxmaj jim xxmaj morris , it 's good to not only watch \" xxmaj the xxmaj rookie \" but to watch the xxup dvd 's documentary , check out the aforementioned link to the movie 's inaccuracies and probably also to read xxmaj morris ' biography , also titled \" xxmaj the xxmaj rookie . \" i have n't read the book but i hope to one of these days . \n",
       " \n",
       "  xxmaj but overall , \" xxmaj the xxmaj rookie \" is a very good portrayal of a miraculous story and is a powerful testament to the power of dreams and the triumph of the common man . 8 / 10,xxbos i saw this movie when i was about 8-years - old and i liked it but it was n't until i watched it again at the age of 13 that i really understood it for what it is ; a cartoon about a criminal dog with a real heart of gold \" adopts \" a little girl in order to exploit her for her talents to talk to animals . xxmaj the dog star , xxmaj charlie xxup b. xxmaj barkin , is murdered by his formal business partner , xxmaj carface , ( who is absolutely diabolical by the way ) . xxmaj his soul then goes to where else but xxmaj heaven only to find a golden watch that is really his life 's time , which xxmaj charlie , being the sneaky but lovable cad that he is steals and rewinds , sending him back to xxmaj earth . xxmaj once back on xxmaj earth , xxmaj charlie goes about seeking revenge on the evil xxmaj carface . xxmaj this is how he comes upon young xxmaj anne - xxmaj marie , the lonely little orphan that can talk to animals whom xxmaj charlie plans to scam for her talents in order to get back at his enemy xxmaj carface . xxmaj but scoundrel xxmaj charlie actually comes to care for young xxmaj anne - xxmaj marie and his plans xxunk as he must make up his mind to do what is right after xxmaj anne - xxmaj marie discovers what her \" best friend \" xxmaj charlie has really been using her to make money for a new and better dog casino . xxmaj now he must rescue her from the dreaded xxmaj carface . i still love this movie even at the age of 22 . xxmaj the idea and plot really are quite different and original from that of many other animated films . i especially like the idea that a dog plays the role of the villain for once . xxmaj carface was even better than he was in the xxmaj all xxmaj dogs go to xxmaj heaven sequel . xxmaj in that picture he appeared quite dubious to his role of villain .\n",
       "y: CategoryList\n",
       "pos,pos,pos,pos,pos\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Valid: LabelList (25000 items)\n",
       "x: TextList\n",
       "xxbos xxmaj no one would ever question that director xxmaj leos xxmaj carax is a genius , but what we wonder about is : is he an insane genius ? xxmaj so many people hated this film ! i am normally the first person to accuse many xxmaj french directors of making offensive , boring , disgusting and pretentious films ( such as the horrible recent film ' l'enfant ' and the pointless and offensive ' xxmaj feux xxmaj rouges ' ) . xxmaj but strangely enough , i actually think that ' xxmaj pola x ' is an amazing film , made with great skill and passion by a master of his craft , and containing remarkable performances . xxmaj the film does carry melodrama to more extreme lengths than i believe i have ever seen on screen before . xxmaj but then , xxmaj carax is extreme , that we know . xxmaj the film also contains what i consider way over - the - top xxmaj trotskyite or xxmaj anarchist fantasies and wet - dreams , what with a mysterious group of young men training to fire machine guns at the bourgeoisie in between playing xxmaj scott xxmaj walker 's rather fascinating music in a band which has its recording sessions in an abandoned warehouse filled with squatters and fires burning in old steel barrels . xxmaj guillaume xxmaj depardieu plays a rich young man in a château ( whose step - mother is xxmaj catherine xxmaj deneuve , and he wanders into her bathroom while she is naked in the bath , by the way ) . xxmaj but he suddenly ' snaps ' completely when he discovers that his deceased father , a famous diplomat , had fathered an illegitimate daughter who had been effectively disposed of by xxmaj deneuve as an inconvenience . xxmaj this is because the sister suddenly turns up as a kind of xxmaj romanian refugee with wild dishevelled hair , expressionless face , and little ability to speak xxmaj french coherently . xxmaj depardieu then transforms himself into a ' class hero ' of the far left and wants to kill or destroy his family for their hypocrisy and corruption , and lives in squalor and extreme poverty , while scorning a vast inheritance . xxmaj he then commences an incestuous sexual relationship with his half - sister , which is shown in an explicit sex scene which has offended many people , though i have no objection to it , as i think people are far too hysterical about sex , especially in xxmaj america , where apparently it never happens . xxmaj the intensity of the acting and the filming make this unlikely scenario come off as an experience of powerful , if depressing , hyper - melodrama . xxmaj the differences between xxmaj carax making an extreme film like this and the numerous extreme xxmaj french films which i think are pretentious and disgusting are ( 1 ) that xxmaj carax is an excellent filmmaker , and ( 2 ) he is seriously attempting to explore a meaningful , if harrowing , extreme emotional condition , whereby a human being disintegrates and turns against his background . xxmaj many would say that the extreme elements in this film were gratuitous , but i do n't agree . i believe xxmaj carax was genuine , and was not making an exploitation picture at all . xxmaj it is very difficult to defend a man who goes that far and who , for all i know , may be a complete madman , but i believe he deserves defending for this remarkable cinematic achievement .,xxbos a picture starring xxmaj danny xxmaj devito and xxmaj billy xxmaj crystal . xxmaj they are both famous actors , and in this charming comedy entails them trying to murder each other 's pet peeve . xxmaj billy xxmaj crystal 's pet peeve being his ex wife who stole his book and put her name on it , and xxmaj danny xxmaj devito 's pet peeve being his malignant mother . xxmaj billy xxmaj crystal is an author and a teacher apparently and xxmaj danny xxmaj devito is one of his students . xxmaj this comedy classic is very entertaining and for all ages . \n",
       " \n",
       "  xxmaj danny xxmaj devito seems to act like a child in this movie because of his evil mother who keeps putting him down all the time . xxmaj she says things like \" xxmaj you have no friends ! \" and when xxmaj billy xxmaj crystal fell down , she said \" xxmaj burry him before he stinks up the basement ! \" . xxmaj she puts down xxmaj danny also by calling him \" xxmaj lard ass ! \" and other things just to make him angry .,xxbos xxmaj funny , yes . a xxmaj freleng classic ! xxmaj to watch xxmaj sylvester turn green is always a treat , and it brings us back to the days when cartoon slapstick was brave and geared for the adult mind . \n",
       " \n",
       "  xxmaj loved it !,xxbos xxmaj there is a need for this kind of entertainment in our modern world . xxmaj you can watch \" xxmaj ma and xxmaj pa \" with adults , with your family ( kids any age or just by yourself like me . xxmaj they are gentle , but gentle is so refreshing in a society of kids killing kids , a horrible war , inappropriate prime time television and poverty . xxmaj we do n't even get a hint of where all of those children came from ! xxmaj give me modern plumbing and i 'll gladly become a xxmaj kettle . xxmaj humor does xxup not require offensive language . xxmaj it is hard to follow conversations in shows where every other word is bleeped . xxmaj relax , take your shoes off , and climb in your recliner with a good old - fashioned glass of lemonade , and just breathe easy watching xxmaj ma sweeping the chickens off the table at lunch time ! xxmaj pj,xxbos xxmaj chris xxmaj noth plays a maniac who wrote a children 's book as a young lad , posing as a doctor who menaces a community in order to push his sister 's son into the position as his school 's genius . xxmaj the only man that can stop him is a cop who plays by his own rules , and is forced to go undercover with the help of his mute father who communicates via xxunk board . xxmaj will xxmaj rick xxmaj springfield find out what s up ? xxmaj or will somebody sabotage his recording studio ! xxmaj does the mentally retarded boy who operates the book mobile have clues ? xxmaj watch to find out ! i 'm not making this up !\n",
       "y: CategoryList\n",
       "pos,pos,pos,pos,pos\n",
       "Path: /root/.fastai/data/imdb;\n",
       "\n",
       "Test: None, model=SequentialRNN(\n",
       "  (0): MultiBatchEncoder(\n",
       "    (module): TransformerXL(\n",
       "      (encoder): Embedding(60000, 410)\n",
       "      (pos_enc): PositionalEncoding()\n",
       "      (drop_emb): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (4): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (5): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (6): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (7): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (8): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (9): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (10): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (11): DecoderLayer(\n",
       "          (mhra): MultiHeadRelativeAttention(\n",
       "            (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "            (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "            (drop_att): Dropout(p=0.05, inplace=False)\n",
       "            (drop_res): Dropout(p=0.05, inplace=False)\n",
       "            (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "          )\n",
       "          (ff): SequentialEx(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "              (1): ReLU(inplace=True)\n",
       "              (2): Dropout(p=0.05, inplace=False)\n",
       "              (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "              (4): Dropout(p=0.05, inplace=False)\n",
       "              (5): MergeLayer()\n",
       "              (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       "), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7f1a9f317c80>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/root/.fastai/data/imdb'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[...], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)\n",
       "alpha: 2.0\n",
       "beta: 1.0], layer_groups=[Sequential(\n",
       "  (0): Embedding(60000, 410)\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (4): ParameterModule()\n",
       "  (5): ParameterModule()\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (2): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (3): DecoderLayer(\n",
       "    (mhra): MultiHeadRelativeAttention(\n",
       "      (attention): Linear(in_features=410, out_features=1230, bias=False)\n",
       "      (out): Linear(in_features=410, out_features=410, bias=False)\n",
       "      (drop_att): Dropout(p=0.05, inplace=False)\n",
       "      (drop_res): Dropout(p=0.05, inplace=False)\n",
       "      (ln): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      (r_attn): Linear(in_features=410, out_features=410, bias=False)\n",
       "    )\n",
       "    (ff): SequentialEx(\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=410, out_features=2100, bias=True)\n",
       "        (1): ReLU(inplace=True)\n",
       "        (2): Dropout(p=0.05, inplace=False)\n",
       "        (3): Linear(in_features=2100, out_features=410, bias=True)\n",
       "        (4): Dropout(p=0.05, inplace=False)\n",
       "        (5): MergeLayer()\n",
       "        (6): LayerNorm((410,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "), Sequential(\n",
       "  (0): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): BatchNorm1d(1230, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.05, inplace=False)\n",
       "      (2): Linear(in_features=1230, out_features=50, bias=True)\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): Dropout(p=0.1, inplace=False)\n",
       "      (6): Linear(in_features=50, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")], add_time=True, silent=False, cb_fns_registered=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = text_classifier_learner(data_clas, TransformerXL, drop_mult=0.5)\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='4' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      80.00% [4/5 16:08<04:02]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.304980</td>\n",
       "      <td>0.296074</td>\n",
       "      <td>0.877120</td>\n",
       "      <td>04:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.307679</td>\n",
       "      <td>0.251200</td>\n",
       "      <td>0.901760</td>\n",
       "      <td>03:54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281878</td>\n",
       "      <td>0.252504</td>\n",
       "      <td>0.900480</td>\n",
       "      <td>04:08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.261485</td>\n",
       "      <td>0.235777</td>\n",
       "      <td>0.908760</td>\n",
       "      <td>03:57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='82' class='' max='391', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      20.97% [82/391 00:49<03:07]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(5, 1e-1, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.694001</td>\n",
       "      <td>38.782734</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.freeze_to(-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\"I really loved that movie, it was awesome!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
