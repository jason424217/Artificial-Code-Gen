{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2ulU52557swe"
   },
   "source": [
    "# Setup Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests tqdm regex tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 41269,
     "status": "ok",
     "timestamp": 1559840980626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "fD_Agx5I7vOa",
    "outputId": "322324f8-80d3-4737-a8e9-0021a0f63acb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACDkC-h3YqZc"
   },
   "source": [
    "# Download Weights and Encoding Files\n",
    "Copied from https://github.com/openai/gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24071,
     "status": "ok",
     "timestamp": 1559842113607,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "EcX3rSfu7xXP",
    "outputId": "f00971e3-88f3-47c0-8eda-646478403366"
   },
   "outputs": [],
   "source": [
    "subdir = os.path.join('models', \"345M\")\n",
    "if not os.path.exists(subdir):\n",
    "    os.makedirs(subdir)\n",
    "\n",
    "for filename in ['checkpoint','encoder.json', 'model.ckpt.data-00000-of-00001', 'model.ckpt.index', 'model.ckpt.meta', 'vocab.bpe']:\n",
    "\n",
    "    r = requests.get(\"https://storage.googleapis.com/gpt-2/\" + subdir + \"/\" + filename, stream=True)\n",
    "\n",
    "    with open(os.path.join(subdir, filename), 'wb') as f:\n",
    "        file_size = int(r.headers[\"content-length\"])\n",
    "        chunk_size = 1000\n",
    "        with tqdm(ncols=100, desc=\"Fetching \" + filename, total=file_size, unit_scale=True) as pbar:\n",
    "            # 1k for chunk_size, since Ethernet packet size is around 1500 bytes\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                f.write(chunk)\n",
    "                pbar.update(chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O ./data/training.txt https://raw.githubusercontent.com/micheletufano/NeuralCodeTranslator/master/dataset/bug-fixes/medium/train/fixed.txt # http://groups.inf.ed.ac.uk/cup/javaGithub/java_projects.tar.gz\n",
    "!ls ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Dataset from textfile where each example is on a separate line \n",
    "trn_exmpls = tf.data.TextLineDataset(os.path.join(\"data\", \"training.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'public static TYPE_1 init ( java.lang.String name , java.util.Date date ) { TYPE_1 VAR_1 = new TYPE_1 ( ) ; VAR_1 . METHOD_1 ( name ) ; java.util.Calendar VAR_2 = null ; if ( date != null ) { VAR_2 = java.util.Calendar.getInstance ( ) ; VAR_2 . METHOD_2 ( date ) ; } VAR_1 . METHOD_3 ( VAR_2 ) ; return VAR_1 ; }', shape=(), dtype=string)\n",
      "tf.Tensor(b'public TYPE_1 METHOD_1 ( java.lang.String name ) { if ( name . equals ( STRING_3 ) ) return new TYPE_3 ( STRING_4 , true ) ; if ( name . equals ( STRING_5 ) ) return new TYPE_4 ( ) ; return super . METHOD_1 ( name ) ; }', shape=(), dtype=string)\n",
      "tf.Tensor(b'private boolean METHOD_1 ( TYPE_1 VAR_1 ) { boolean VAR_2 = ( VAR_3 . compareTo ( VAR_1 . METHOD_2 ( ) ) ) < 0 ; VAR_2 = VAR_2 || ( ! ( VAR_1 . METHOD_3 ( ) . METHOD_4 ( ) . equals ( VAR_4 ) ) ) ; return VAR_2 ; }', shape=(), dtype=string)\n",
      "tf.Tensor(b'public void METHOD_1 ( TYPE_1 VAR_1 , boolean VAR_2 ) { if ( VAR_2 ) { VAR_3 . METHOD_2 ( 1 , CHAR_1 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ; } else { VAR_3 . METHOD_2 ( 1 , CHAR_2 ) ; VAR_4 . METHOD_3 ( VAR_3 . toString ( ) ) ; } }', shape=(), dtype=string)\n",
      "tf.Tensor(b'public boolean METHOD_1 ( ) { if ( ( VAR_1 ) >= ( ( VAR_2 . METHOD_2 ( VAR_3 ) ) - 1 ) ) { return false ; } if ( ( METHOD_3 ( ) . getValue ( ) ) <= ( METHOD_4 ( ( ( VAR_1 ) + 1 ) , VAR_3 ) . getValue ( ) ) ) { return false ; } ( VAR_1 ) ++ ; return true ; }', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Examine the first 5 method examples\n",
    "for method in trn_exmpls.take(5):\n",
    "    print(method)\n",
    "# trn_exmpls = trn_exmpls.batch(2)\n",
    "# trn_exmpls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Byte Pair Encoding tokenization on the dataset\n",
    "tokenizer_method = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    (method.numpy() for method in trn_exmpls), target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized string is [3014, 3034, 3035, 3045, 2962, 3035, 3045, 2962, 1846, 3041, 1669, 3048, 3041, 3029, 3027, 3028, 3052, 3056, 2962, 15, 2962, 3025, 2962, 7, 2976]\n",
      "The original string: This is out of vocabz~ if _ java.\n",
      "3014 ----> T\n",
      "3034 ----> h\n",
      "3035 ----> i\n",
      "3045 ----> s\n",
      "2962 ---->  \n",
      "3035 ----> i\n",
      "3045 ----> s\n",
      "2962 ---->  \n",
      "1846 ----> out \n",
      "3041 ----> o\n",
      "1669 ----> f \n",
      "3048 ----> v\n",
      "3041 ----> o\n",
      "3029 ----> c\n",
      "3027 ----> a\n",
      "3028 ----> b\n",
      "3052 ----> z\n",
      "3056 ----> ~\n",
      "2962 ---->  \n",
      "15 ----> if\n",
      "2962 ---->  \n",
      "3025 ----> _\n",
      "2962 ---->  \n",
      "7 ----> java\n",
      "2976 ----> .\n"
     ]
    }
   ],
   "source": [
    "# Examine the tokenization\n",
    "sample_string = 'This is out of vocabz~ if _ java.'\n",
    "\n",
    "tokenized_string = tokenizer_method.encode(sample_string)\n",
    "print ('Tokenized string is {}'.format(tokenized_string))\n",
    "\n",
    "original_string = tokenizer_method.decode(tokenized_string)\n",
    "print ('The original string: {}'.format(original_string))\n",
    "\n",
    "# Prints out the words as tokens. If word does not exist, it will break it down into individual tokens\n",
    "for ts in tokenized_string:\n",
    "  print ('{} ----> {}'.format(ts, tokenizer_method.decode([ts])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a method and generates the input into the model, except the last token,\n",
    "# and the target output of the model (input shifted to the left by one)\n",
    "def split_into_inpt_trgt(method):\n",
    "    input_method = tokenizer_method.encode(method.numpy())[:-1]\n",
    "    target_method = tokenizer_method.encode(method.numpy())[1:]\n",
    "    return input_method, target_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_split_into_inpt_trgt(method):\n",
    "    return tf.py_function(split_into_inpt_trgt, [method], [tf.int64, tf.int64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following: https://www.tensorflow.org/beta/tutorials/text/text_generation#top_of_page for how to get data into format for language models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate, shuffle, cache, and prefetch the training examples\n",
    "trn_ds = trn_exmpls.map(tf_split_into_inpt_trgt)#.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "trn_ds = trn_ds.cache()\n",
    "trn_ds = trn_ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "trn_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'public static TYPE\\\\&undsc1 init ( java.lang.String name , java.util.Date date ) { TYPE\\\\&undsc1 VAR\\\\&undsc1 = new TYPE\\\\&undsc1 ( ) ; VAR\\\\&undsc1 . METHOD\\\\&undsc1 ( name ) ; java.util.Calendar VAR\\\\&undsc2 = null ; if ( date != null ) { VAR\\\\&undsc2 = java.util.Calendar.getInstance ( ) ; VAR\\\\&undsc2 . METHOD\\\\&undsc2 ( date ) ; } VAR\\\\&undsc1 . METHOD\\\\&undsc3 ( VAR\\\\&undsc2 ) ; return VAR\\\\&undsc1'\n",
      "Target data: 'static TYPE\\\\&undsc1 init ( java.lang.String name , java.util.Date date ) { TYPE\\\\&undsc1 VAR\\\\&undsc1 = new TYPE\\\\&undsc1 ( ) ; VAR\\\\&undsc1 . METHOD\\\\&undsc1 ( name ) ; java.util.Calendar VAR\\\\&undsc2 = null ; if ( date != null ) { VAR\\\\&undsc2 = java.util.Calendar.getInstance ( ) ; VAR\\\\&undsc2 . METHOD\\\\&undsc2 ( date ) ; } VAR\\\\&undsc1 . METHOD\\\\&undsc3 ( VAR\\\\&undsc2 ) ; return VAR\\\\&undsc1 ; }'\n"
     ]
    }
   ],
   "source": [
    "# Examine the dataset\n",
    "for inpt_exmpl, outpt_exmpl in trn_ds.take(1):\n",
    "    print ('Input data: ', repr(''.join(tokenizer_method.decode(inpt_exmpl.numpy()))))\n",
    "    print ('Target data:', repr(''.join(tokenizer_method.decode(outpt_exmpl.numpy()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 17 (public )\n",
      "  expected output: 77 (static )\n",
      "Step    1\n",
      "  input: 77 (static )\n",
      "  expected output: 18 (TYPE\\&undsc1 )\n",
      "Step    2\n",
      "  input: 18 (TYPE\\&undsc1 )\n",
      "  expected output: 282 (init)\n",
      "Step    3\n",
      "  input: 282 (init)\n",
      "  expected output: 1 ( ( )\n",
      "Step    4\n",
      "  input: 1 ( ( )\n",
      "  expected output: 7 (java)\n"
     ]
    }
   ],
   "source": [
    "# Examine the dataset\n",
    "for i, (inpt_tkn, trgt_tkn) in enumerate(zip(inpt_exmpl[:5], outpt_exmpl[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(inpt_tkn, tokenizer_method.decode([inpt_tkn.numpy()])))\n",
    "    print(\"  expected output: {} ({:s})\".format(trgt_tkn, tokenizer_method.decode([trgt_tkn.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=793582, shape=(64, 91), dtype=int64, numpy=\n",
       " array([[ 17,  26,  11, ...,   0,   0,   0],\n",
       "        [ 17,  26,  11, ...,   0,   0,   0],\n",
       "        [ 17,  26,  11, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 50,  26, 282, ...,   0,   0,   0],\n",
       "        [ 17,  26,  11, ...,   0,   0,   0],\n",
       "        [ 17,  66,  11, ...,   0,   0,   0]])>,\n",
       " <tf.Tensor: id=793583, shape=(64, 91), dtype=int64, numpy=\n",
       " array([[ 26,  11,  42, ...,   0,   0,   0],\n",
       "        [ 26,  11,  42, ...,   0,   0,   0],\n",
       "        [ 26,  11,   1, ...,   0,   0,   0],\n",
       "        ...,\n",
       "        [ 26, 282,   1, ...,   0,   0,   0],\n",
       "        [ 26,  11,  42, ...,   0,   0,   0],\n",
       "        [ 66,  11,  42, ...,   0,   0,   0]])>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "trn_ds = trn_ds.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1]), drop_remainder=True)\n",
    "inpt_batch, trgt_batch = next(iter(trn_ds))\n",
    "inpt_batch, trgt_batch\n",
    "# trn_ds.output_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = tokenizer_method.vocab_size\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = vocab_size,\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 91, 3186) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "input_example_batch, target_example_batch = next(iter(trn_ds))\n",
    "example_batch_predictions = model(input_example_batch)\n",
    "print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (64, None, 256)           815616    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (64, None, 3186)          3265650   \n",
      "=================================================================\n",
      "Total params: 9,328,242\n",
      "Trainable params: 9,328,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'public void METHOD\\\\&undsc1 ( ) { if ( ( TYPE\\\\&undsc2 . METHOD\\\\&undsc2 ( ) ) == null ) return ; TYPE\\\\&undsc1 VAR\\\\&undsc1 = ( ( TYPE\\\\&undsc1 ) ( TYPE\\\\&undsc2 . METHOD\\\\&undsc2 ( ) . METHOD\\\\&undsc3 ( VAR\\\\&undsc2 ) ) ) ; if ( VAR\\\\&undsc1 == null ) return ; VAR\\\\&undsc1 . METHOD\\\\&undsc4 ( ) ; TYPE\\\\&undsc2 . METHOD\\\\&undsc2 ( ) . METHOD\\\\&undsc5 ( VAR\\\\&undsc2'\n",
      "\n",
      "Next Char Predictions: \n",
      " ' ) ) ) !=  ( ) ) & - boolean  ) >= ( ( ( fail ( ) ) /  ) , ( ) - >  ) ) ) && ( ( ! (  ( ) ) & (  ) ] ) ==  ) ) + ( ( ( - testng] ] ; } -- (  ) } ) ,  [ ] ) {  ( ) ) ; ) {  ) ) ; } } } ) ) < (  ) ) ) ) * (  ) ] ) {  ) ) > (  > > ( ) ;  ) < < (  ) ) ) ) , ( (  ? !  ++ ) ] ) ;  ) ++ ) ] = ( (  ) ++ ; } { } } } \\x1c ) ) ] ; } } ) ) ; } } } } }  : } } } > ) (  ; } } } } ) ) ) ) >= (  ] ) != ( ( (  } ) ; } } ) ;  ( ) ) ; } } } ) -- ; } } \" ; } } params ( ( ( !  ) )  ( ( ( ( \"\\\\ ) ] = -  = -  ] ) < < (  } ; 1 == ( ( ( throw INT\\\\&undsc15 ( ) ) != ( ( ( � ) ) & ( ! (  ( ) ] =  > ( ) ) ; } } } ? (  ] ) ) ) ) ; }  ) ) ? -  ] ] [ ( ( � ) ] ) +  ^  ( ) ) ) ) ) && ( ^ ( ) { } } VAR\\\\&undsc9 ) ] ) <= � ) <= ( (  ) ; } } ) ; }\" ) ) ) || ( � ( ) ) && ( ( ( ( switch ] ] [ ( (  ) { } } ) ; }  ( ( ( ( (  % ( TYPE\\\\&undsc5METHOD\\\\&undsc17 ( ) ) ) ) - ( ( \\x0b\" : (  ) , ( ) - > INT\\\\&undsc2 ] ] ;  >= ( - '\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(tokenizer_method.decode(input_example_batch[0].numpy()))))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(tokenizer_method.decode(sampled_indices))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 91, 3186)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       8.064807\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-fd6f8ec66091>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_select_training_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2714\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2715\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2716\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2717\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2718\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "history = model.fit(trn_ds, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (1, None, 256)            815616    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 3186)           3265650   \n",
      "=================================================================\n",
      "Total params: 9,328,242\n",
      "Trainable params: 9,328,242\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = tokenizer_method.encode(start_string)\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(tokenizer_method.decode([predicted_id]))\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public FLOAT\\&undsc2 ) ) ) ) ) > ( y < INT\\&undsc1 , 0 , start , length , INT\\&undsc2 } ; } }METHOD\\&undsc2 ( ) ) > ( number ) ) . equals ( event . length ) ; ( VAR\\&undsc1 ) ++ ; return end ) ; }b . METHOD\\&undsc4 ( ( ( VAR\\&undsc2 ) - count ) | size ) ) ; } } }n ) ) ; }VAR\\&undsc3 ; }this . result = METHOD\\&undsc6 ( ) ) ) ; }else ; }this . METHOD\\&undsc5 ( VAR\\&undsc3 , ( x ] = \"b ) ++ ; } } return false ; }VAR\\&undsc10 ; }this . METHOD\\&undsc6 ( y ) ; this . METHOD\\&undsc5 ( this . METHOD\\&undsc6 ( ) ) ; }this . position ) ++ ; this . METHOD\\&undsc1 ( ) ; }VAR\\&undsc1 . setEnabled ( this ) ; } catch ( java.lang.Exception VAR\\&undsc2 : this . VAR\\&undsc1 ) { this . METHOD\\&undsc1 ( ) . METHOD\\&undsc1 ( ) ; return ; } } } }this . VAR\\&undsc1 ) - 1 ; } }this . VAR\\&undsc2 += \"\\else { this . delete ( ) ; } finally { this . METHOD\\&undsc2 ( ) ; this . METHOD\\&undsc7 ( ) . METHOD\\&undsc8 ( true ) ; } this . METHOD\\&undsc6 ( ( ( VAR\\&undsc2 . METHOD\\&undsc1 ( ) ) + 1 ) ) ) ) ) ; } } }this . VAR\\&undsc1 ) ) ) ; }this . METHOD\\&undsc3 ( this ) ) ; }this . VAR\\&undsc2 ++ ) ] ) ; } this . VAR\\&undsc9 = this ; }METHOD\\&undsc2 ( ) ; }c ; for ( TYPE\\&undsc1 VAR\\&undsc3 ) { VAR\\&undsc5 = false ; } }VAR\\&undsc5 . METHOD\\&undsc1 ( VAR\\&undsc3 , ( ( VAR\\&undsc6 ) + ( VAR\\&undsc5 ) ) ; } }INT\\&undsc3 - ( ( this . VAR\\&undsc4 . y ) ) * 1 ) ) ; if ( ( VAR\\&undsc4 ) > instanceof  ( this . METHOD\\&undsc5 ( ) . j ( ) , \"text ) - ( VAR\\&undsc3 ) ) ) ) ) { METHOD\\&undsc6 ( ) ; this . getString ( ( STRING\\&undsc1 + ( ( this . position ) ; return true ; } return text ; }else ; }return this ; }this ; }\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"public \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kfPQ8qOo47NM"
   },
   "source": [
    "# Encoder\n",
    "Direct copy and paste from  https://github.com/openai/gpt-2 with a few extra comments for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "504vF8pP49zu"
   },
   "outputs": [],
   "source": [
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 bytes and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73ckOHpXVRJ4"
   },
   "outputs": [],
   "source": [
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGUzTE0hVTAz"
   },
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hYT-GWuKVjp9"
   },
   "outputs": [],
   "source": [
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4AX61lMVoZC"
   },
   "source": [
    "# Model\n",
    "Updated from https://github.com/openai/gpt-2 for tensorflow 2.0 and be more readable using https://github.com/graykode/gpt-2-Pytorch and the following tutorial https://www.tensorflow.org/alpha/tutorials/text/transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MX7xtMkNV0HB"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters for the 345M model\n",
    "num_vocab = 50257\n",
    "num_ctx = 1024\n",
    "num_embd = 1024\n",
    "num_heads = 16\n",
    "num_layers = 24\n",
    "num_state = num_embd # used for the MLP's dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raQxJ1fhX-ak"
   },
   "outputs": [],
   "source": [
    "# Gaussian Error Linear Unit (GELU) from https://arxiv.org/abs/1606.08415\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x+0.044715 * tf.pow(x, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T0x4Z3J0RuaU"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "  \"\"\"Calculate the attention weights.\n",
    "  q, k, v must have matching leading dimensions.\n",
    "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "  The mask has different shapes depending on its type(padding or look ahead) \n",
    "  but it must be broadcastable for addition.\n",
    "  \n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "    \n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  # scale matmul_qk\n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "  # add the mask to the scaled tensor.\n",
    "  if mask is not None:\n",
    "    scaled_attention_logits += (mask * -1e9)\n",
    "\n",
    "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "  # add up to 1.\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_v, depth_v)\n",
    "\n",
    "  return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 325,
     "status": "ok",
     "timestamp": 1559331809237,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "M4uRLjjGiJ2G",
    "outputId": "3387b65d-9756-465d-d6b0-69de0e689203"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_state, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = num_state\n",
    "    \n",
    "    assert num_state % self.num_heads == 0\n",
    "    \n",
    "    self.depth = num_state // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(num_state)\n",
    "    self.wk = tf.keras.layers.Dense(num_state)\n",
    "    self.wv = tf.keras.layers.Dense(num_state)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(num_state)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len, num_state)\n",
    "    k = self.wk(k)  # (batch_size, seq_len, num_state)\n",
    "    v = self.wv(v)  # (batch_size, seq_len, num_state)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "    \n",
    "    present = tf.stack([k, v], axis=1)\n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_v, depth)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "        q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_v, num_heads, depth)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_v, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_v, d_model)\n",
    "        \n",
    "    return output, present\n",
    "  \n",
    "# Testing data is flowing correctly\n",
    "temp_mha = MultiHeadAttention(num_state, num_heads)\n",
    "y = tf.random.uniform((1, 60, num_state))  # (batch_size, encoder_sequence, d_model)\n",
    "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1559331696152,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "nUnCQtYaX6TL",
    "outputId": "e5c2695c-bcea-4806-c90a-522fd4ba05a7"
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_state, num_embd):\n",
    "    super(MLP, self).__init__()\n",
    "    self.dense_1 = tf.keras.layers.Dense(num_embd, activation=gelu)\n",
    "    self.dense_2 = tf.keras.layers.Dense(num_state)\n",
    "    \n",
    "  def call(self, x):\n",
    "    x = self.dense_1(x)\n",
    "    x = self.dense_2(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Testing data is flowing correctly\n",
    "sample_mlp_layer = MLP(4 * num_state, num_embd)\n",
    "sample_mlp_layer_out = sample_mlp_layer(tf.random.uniform((64, 40, num_state)))\n",
    "sample_mlp_layer_out.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1559331936484,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "wqiBThEji-L0",
    "outputId": "b1104b64-9f16-4011-84ef-afa091443311"
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_state, num_heads, num_embd):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    \n",
    "    # Block layout is as follows\n",
    "    self.layer_norm_1 = tf.keras.layers.experimental.LayerNormalization(epsilon=1e-5)\n",
    "    self.attn_layer = MultiHeadAttention(num_state, num_heads)\n",
    "    self.layer_norm_2 = tf.keras.layers.experimental.LayerNormalization(epsilon=1e-5)\n",
    "    self.mlp = MLP(4 * num_state, num_embd)\n",
    "    \n",
    "  def call(self, x, past=None):\n",
    "    x = self.layer_norm_1(x) # Layer Norm\n",
    "    attn, present = self.attn_layer(x, x, x, mask=None) # Attend\n",
    "    x = x + attn # Residual Connection\n",
    "    x = self.layer_norm_2(x) # Layer Norm\n",
    "    m = self.mlp(x) # Feedforward\n",
    "    x = x + m # Residual Connection\n",
    "    \n",
    "    return x, present\n",
    "  \n",
    "# Testing data is flowing correctly\n",
    "sample_decoder_block = DecoderBlock(num_state, num_heads, num_embd)\n",
    "sample_decoder_block_output, _ = sample_decoder_block(tf.random.uniform((64, 50, num_state)))\n",
    "sample_decoder_block_output.shape  # (batch_size, target_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvvi2L7Vnbhe"
   },
   "outputs": [],
   "source": [
    "class GPT(tf.keras.layers.Model):\n",
    "  def __init__(self):\n",
    "    self.wrd_tkn_embd = tf.keras.layers.Embedding(input_vocab_size, dim_model)\n",
    "    self.wrd_pos_enc = tf.keras.layers.Embedding(input_vocab_size, dim_model)\n",
    "    \n",
    "    self.dec_blks = [DecoderBlock(d_model, num_heads, dff) for _ in range(num_layers)]\n",
    "    self.layer_norm = tf.keras.layers.experimental.LayerNormalization(epsilon=1e-5)\n",
    "    self.flatn = tf.keras.layers.Flatten()\n",
    "    pass\n",
    "  \n",
    "  def call(self, x, pos_x, pasts=None): # Past = enc_output since this is just a decoder, not encoder -> decoder architecture\n",
    "    if pasts is None:\n",
    "      pasts_length = 0\n",
    "      pasts = [None] * n_layer\n",
    "    else:\n",
    "      pasts_length = pasts.shape[-2]\n",
    "      pasts = tf.unstack(pasts, axis=1)\n",
    "      \n",
    "    x = self.wrd_tk_embd(x)\n",
    "    x = x + self.wrd_pos_enc(pos_x)\n",
    "    \n",
    "    presents = []\n",
    "    for block, past in zip(self.dec_blks, pasts):\n",
    "      x, present = block(x, past)\n",
    "      presents.append(present)\n",
    "    \n",
    "    results['present'] = tf.stack(presents, axis=1)\n",
    "    x = self.layer_norm(x)\n",
    "    x = self.flatn(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XcTSvAyBHpN"
   },
   "source": [
    "# Reloading Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3RoOZNXdBRFs"
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "tensorflow2.0_GPT-2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
