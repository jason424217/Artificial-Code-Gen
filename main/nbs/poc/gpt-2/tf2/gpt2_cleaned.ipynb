{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/prototypes/gpt-2/tf2/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
      "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
      "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
      "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from statistics import median\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1)\n",
    "        ]\n",
    "\n",
    "    past, prev, output = body(None, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "    _, _, tokens = tf.while_loop(\n",
    "        cond=cond, body=body,\n",
    "        maximum_iterations=length - 1,\n",
    "        loop_vars=[\n",
    "            past,\n",
    "            prev,\n",
    "            output\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "        ],\n",
    "        back_prop=False,\n",
    "    )\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "                \n",
    "#             if i == 500000:\n",
    "#                 print(\"Breaking\")\n",
    "#                 break\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "        \n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "#         if 'after.java' not in path:\n",
    "#             continue\n",
    "\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "            tokens = np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except:\n",
    "            print(e)\n",
    "#         if i >= 500000:\n",
    "#             break\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every, pretrained, iterations):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every\n",
    "        self.pretrained = pretrained\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                dataset=\"../methods/DATA00M_[god-r]\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1, # DO NOT TOUCH. INCREASING THIS WILL RAIN DOWN HELL FIRE ONTO YOUR COMPUTER.\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=40,\n",
    "                top_p=0.0,\n",
    "                run_name=\"run4\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=None,\n",
    "                val_batch_size=1,\n",
    "                val_batch_count=40,\n",
    "                val_every=100,\n",
    "                pretrained=False,\n",
    "                iterations=200000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11138/1215964 [00:41<1:13:39, 272.60it/s]"
     ]
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "data_set = load_dataset(enc, args.dataset, args.combine)\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1215964, 972771, 121596, 121596)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SET_SIZE = len(data_set)\n",
    "TRN_SET_SIZE = int(DATA_SET_SIZE * 0.8)\n",
    "VAL_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "TST_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "\n",
    "trn_set = data_set[:TRN_SET_SIZE]\n",
    "val_set = data_set[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "tst_set = data_set[-TST_SET_SIZE:]\n",
    "DATA_SET_SIZE, len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset has 768821317 tokens\n",
      "Training...\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:01<00:00, 21.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 | 2.06] validation loss = 10.90\n",
      "[1 | 6.10] loss=10.84 avg=9.86\n",
      "[2 | 6.16] loss=10.98 avg=10.40\n",
      "[3 | 6.21] loss=10.79 avg=10.53\n",
      "[4 | 6.27] loss=10.77 avg=10.59\n",
      "[5 | 6.32] loss=11.00 avg=10.67\n",
      "[6 | 6.38] loss=10.87 avg=10.70\n",
      "[7 | 6.43] loss=11.09 avg=10.76\n",
      "[8 | 6.48] loss=10.90 avg=10.78\n",
      "[9 | 6.53] loss=10.85 avg=10.79\n",
      "[10 | 6.59] loss=11.05 avg=10.82\n",
      "[11 | 6.64] loss=10.95 avg=10.83\n",
      "[12 | 6.69] loss=10.51 avg=10.80\n",
      "[13 | 6.75] loss=10.90 avg=10.81\n",
      "[14 | 6.80] loss=10.76 avg=10.80\n",
      "[15 | 6.85] loss=10.46 avg=10.78\n",
      "[16 | 6.91] loss=10.33 avg=10.75\n",
      "[17 | 6.96] loss=10.79 avg=10.75\n",
      "[18 | 7.02] loss=11.04 avg=10.77\n",
      "[19 | 7.07] loss=10.76 avg=10.77\n",
      "[20 | 7.13] loss=10.32 avg=10.74\n",
      "[21 | 7.18] loss=10.98 avg=10.76\n",
      "[22 | 7.23] loss=10.64 avg=10.75\n",
      "[23 | 7.28] loss=10.27 avg=10.73\n",
      "[24 | 7.33] loss=10.45 avg=10.71\n",
      "[25 | 7.39] loss=9.81 avg=10.67\n",
      "[26 | 7.44] loss=10.53 avg=10.67\n",
      "[27 | 7.49] loss=10.11 avg=10.64\n",
      "[28 | 7.55] loss=9.75 avg=10.61\n",
      "[29 | 7.60] loss=9.99 avg=10.58\n",
      "[30 | 7.67] loss=9.34 avg=10.54\n",
      "[31 | 7.72] loss=10.45 avg=10.53\n",
      "[32 | 7.77] loss=10.76 avg=10.54\n",
      "[33 | 7.83] loss=9.68 avg=10.51\n",
      "[34 | 7.88] loss=9.73 avg=10.48\n",
      "[35 | 7.94] loss=9.36 avg=10.45\n",
      "[36 | 7.99] loss=10.89 avg=10.46\n",
      "[37 | 8.05] loss=10.25 avg=10.45\n",
      "[38 | 8.10] loss=9.18 avg=10.41\n",
      "[39 | 8.15] loss=9.46 avg=10.39\n",
      "[40 | 8.21] loss=10.36 avg=10.38\n",
      "[41 | 8.26] loss=10.39 avg=10.38\n",
      "[42 | 8.31] loss=10.79 avg=10.40\n",
      "[43 | 8.37] loss=9.35 avg=10.37\n",
      "[44 | 8.42] loss=10.47 avg=10.37\n",
      "[45 | 8.48] loss=10.78 avg=10.38\n",
      "[46 | 8.53] loss=9.59 avg=10.36\n",
      "[47 | 8.59] loss=9.53 avg=10.34\n",
      "[48 | 8.64] loss=10.07 avg=10.33\n",
      "[49 | 8.70] loss=9.95 avg=10.32\n",
      "[50 | 8.75] loss=10.48 avg=10.32\n",
      "[51 | 8.81] loss=10.10 avg=10.32\n",
      "[52 | 8.86] loss=10.19 avg=10.32\n",
      "[53 | 8.91] loss=10.31 avg=10.32\n",
      "[54 | 8.97] loss=9.92 avg=10.31\n",
      "[55 | 9.03] loss=8.57 avg=10.27\n",
      "[56 | 9.08] loss=9.74 avg=10.25\n",
      "[57 | 9.13] loss=9.93 avg=10.25\n",
      "[58 | 9.19] loss=8.79 avg=10.21\n",
      "[59 | 9.24] loss=9.22 avg=10.19\n",
      "[60 | 9.30] loss=10.69 avg=10.20\n",
      "[61 | 9.35] loss=7.86 avg=10.15\n",
      "[62 | 9.41] loss=10.49 avg=10.16\n",
      "[63 | 9.46] loss=10.69 avg=10.17\n",
      "[64 | 9.52] loss=9.76 avg=10.16\n",
      "[65 | 9.57] loss=9.59 avg=10.15\n",
      "[66 | 9.63] loss=10.57 avg=10.16\n",
      "[67 | 9.69] loss=9.01 avg=10.13\n",
      "[68 | 9.74] loss=9.86 avg=10.13\n",
      "[69 | 9.80] loss=10.37 avg=10.13\n",
      "[70 | 9.85] loss=8.86 avg=10.11\n",
      "[71 | 9.91] loss=8.36 avg=10.07\n",
      "[72 | 9.96] loss=7.34 avg=10.02\n",
      "[73 | 10.01] loss=10.35 avg=10.03\n",
      "[74 | 10.07] loss=8.49 avg=10.00\n",
      "[75 | 10.12] loss=10.44 avg=10.01\n",
      "[76 | 10.17] loss=9.28 avg=9.99\n",
      "[77 | 10.23] loss=9.08 avg=9.98\n",
      "[78 | 10.28] loss=8.58 avg=9.95\n",
      "[79 | 10.34] loss=7.96 avg=9.91\n",
      "[80 | 10.39] loss=10.23 avg=9.92\n",
      "[81 | 10.45] loss=10.78 avg=9.93\n",
      "[82 | 10.50] loss=7.38 avg=9.89\n",
      "[83 | 10.55] loss=7.53 avg=9.85\n",
      "[84 | 10.60] loss=9.92 avg=9.85\n",
      "[85 | 10.66] loss=9.09 avg=9.84\n",
      "[86 | 10.71] loss=7.58 avg=9.80\n",
      "[87 | 10.77] loss=10.80 avg=9.81\n",
      "[88 | 10.82] loss=9.85 avg=9.81\n",
      "[89 | 10.87] loss=6.79 avg=9.76\n",
      "[90 | 10.92] loss=10.32 avg=9.77\n",
      "[91 | 10.98] loss=8.49 avg=9.75\n",
      "[92 | 11.03] loss=10.29 avg=9.76\n",
      "[93 | 11.08] loss=8.91 avg=9.75\n",
      "[94 | 11.14] loss=9.74 avg=9.75\n",
      "[95 | 11.19] loss=8.72 avg=9.73\n",
      "[96 | 11.25] loss=9.55 avg=9.73\n",
      "[97 | 11.31] loss=10.18 avg=9.73\n",
      "[98 | 11.36] loss=10.32 avg=9.74\n",
      "[99 | 11.41] loss=9.07 avg=9.73\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 43.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " Effectschecked oppress sigMeasure Tank recentלYork feed Commentary Folkuffy platescos Owen Holduffy Johns wearable Masters stimulation \". stimulation registered Mastersulhu sabot1900Thunder Surfacecos consulting sabot contacting platesFlag contactingpec Mastersokyall Pearce esteemppard splits Depth users contactingppardascular Johnsoky Pixel Johns PsyNet Depth splits glim mentionscibleTS mornings Pixel Mercywisewise morningsascular contacting talkskip splits splitsATEDARA lacking lackingWheel gon Johns Flow RoboWheelATED PixelHP arithmetic monkeys Flowcosmark deft relies sabot 1911 RomeDevelop annexation mornings Pixel Bretlehem thirteenadish subtract reservesadish Iss Publisher Flow legalize Romealid pushing Johns macrolehem glim monkeys terminated athlete AMA magician Whalehack sweetness splits owningilater contacting suitableguided Pixel Firefly Firefly Iss Her Johns Firefly drinkers Johns Whale contacting contactingmatter rave introductory Alv 1911 refuge refugeinet Bret monkeysshi jury pal Johnscos Bravo drinkersilater WhaleRedditlehem Barton thirteenlegraph introductoryRAM latex Iss appeared Inside justified latex contacting hom titaniummatter introductory Shu refuge appeared pulled AlvWheel introductoryenburg Pledge 1000 owning macro initializationlehem 1000adish floating Iss Robo terminatedviation birthplace introductoryshimattermatter Alvcos glimshiadish drinkers users Firefly introductory contacting rustfacebookmattershi sarcastilater Gast contacting homadish evolves Whale inaccessible Firefly annexation hom refugeWheel latexrolled Attribution Johns Inside sarcastenburg thirteendom Alv refugeilatermatterfts Johnsjianglehem introductory rotting AMAshi latex annexation Hermatter 1911shi Pledgelehem 1000 contacting 1000ittoadish justified curvedlehemshi Johns Ellie � evolves esp introductory sweetness Johnsfts sweetnessadish morningscos Johnslehem Pixelrecy sabotshi 1911 Johns Elliedom Slip esp introductoryudlehembratesMax rave Tolkien introductory Pledgelehem 1911 evolves contacting Kelvin FAAInstalllehemdom annexation Pledge warfareadish justified latex Reboot massive Johns Pledgelehem Maintenanceadish latex Iss philosophylehem unearthedmatterNV Robo thirteendom latex rave 1000 latexud 1911 198uploads ShuInstall rave rave rave Alv GastNV Attribution reportedlyBegin colordom Killinglehem QUEST ardu JohnsadishIIone EllieReddit bundrolled559erve thirteen contacting Maintenance starving FAAscene rave named AMA1969rolled Iss initialization Issfts Maintenance von AMA Lobby 1911 vocalscos Johns glim Slip unearthed rave 1911inezIIbal magician thirteen reimbcos German Alv Alv refuge disposable starving FAA FAA introductory monkeys inaccessible unearthedftsfts agriculture latex Johns NESlehem FAAodefts owningfts justified Johnso unearthedlehem FAA labourdom inaccessible glim unearthedlehem Inside disposable Pixel introductoryDoulehemReddit Insideinez Maintenance curved penchant justifiedoneadishrecydomo Issiro magicianadish763 righteousnessshi unearthedlehemlehem Killing latex fi Maintenancematter Inside latexInstallud GastudSGfts reimb Attributionlehem Pledge AMA glimlehem Pledge PokemonDou reimb lawmaker 1911 vocalsbrates infer Inside thirteen Maintenance Wim introductory FAAodelehem ardu vocalsciblerolled ironic NES ardu Wim victimliness latex Killing contacting 1911 annexation inaccessible justifieddom Iss rave unearthed Pledge Killing latex inaccessibleuffy 170 streetcar Iss introductorydom Iss Springfield streetcarilaterReddit Maintenancerecy introductory Killing inaccessible surgesilater Lobby tragically Mercy drifted floatingReddit 170 Typh penchantbrates reimbDoumatter pushing 1911ε reimbdomlehem FAA Maintenance floating QUEST Stun thirteen594594 ravematter 1911 1000 198magicmatter Inside introductory longer penchant singlesrecy inaccessible vocals Iss Roboshiemale Pledge Edge Johns InsidematterWrittenWrittenfacebook Pledgelehem morningsInstall Wim Energy Insidefts inaccessible latex righteousness Mercy vocals FAA justifiedReddit Ritual Chance 1911matter 1911ε inaccessibleLuckilyε Tolkien latexftsadish private Johns FAA ardu 198 FAASGo WizardsDou AMAÍ unearthed AMA FAA ardu VI vocals Kelvin Isso Johns drifted Johnslehem Giuliani QUEST Giuliani reimbadish Iss peasant lashedRedditDou Pool Pledgeo rave 1911 vocalslehemfacebookadish FAA guarding Tolkien)? Issilater arduilatershi Iss penchant sabot introductoryshiftscoso Maintenance Maintenancematter penchant Inside unearthed 1911facebookftscos vocals Gast contacting Wimbrates ardu Killing ironic German guardingRedditcible 1911 Typh Iss vocals ardu floating Maintenance unearthedo 170 labour lucrative labourbrates adolescenceReddit surges latex arduReddituploads tragically contacting Inside763 righteousnessftsDoumatter Wim introductoryo inaccessiblematter Slip JohnsadishSGWritten FAA Wimbrates vocals Killing ironicilaterInstall compassilater justified AMAotics arduWritten ironicodomfacebook annotation singles Killing tragically Maintenancecos Springfieldilaterfacebookbrates agriculture 1911o Aly justifieddom massive MaintenanceRedditbrates Pledgerecy 1911 glim AMA adolescence guarding streetcarlehem 1911brates aspiring Bethinezε FAA banditsReddit Bethuploads talesmatterfacebook Gast contactingjianguploadsfts Ritual 1911 vocals Kelvindom unearthed Wimilatercible 198 Killing Iss invitesdom rave tragicallyDou justified Hel763 ironicobrates guarding Sting introductoryscene victim justified introductory righteousness Pledge 1911facebook Beth cruiseargument rave Archives justified Her annotation Floyd Archives ironic sabot Johnsotics763 latex648otics Iss Johns dangerous supremacists biologyo 170cible singlesRedditmatter 1911 Floydfts Iss Maintenance curved introductory introductorymatterlehem rave TDs magician magician soclehemdom tragically FAAoodoo lucrativeSGbrates Springfield648oonefacebooko vocals penchant648 aspiring Giuliani Subject ardu curvedilater introductoryε compass PledgeftsReddit aspiring Beth aideII Killing private lucrative BlumenthalzebInstall vocals 1000 outsider exha splits Iss Wizards Inside Springfield Wim Inside labour laptop Killing 170shi Killing contactingmatterlehem 198 Killing648 introductoryfacebook 1911 Johns unearthed splits ardumatterlehem designingReddit Karin victimJReddit tales tragically curved Bust curved FAA Johns Her Ree 1911 justifiedwise Judgment tragically labour adolescence Johns drifted laptop Typh QUEST QUESTilater labouruilt lucrative sweetnessbrates Sting tragicallyε ironic Wim ardu unearthed designing763brates VIbrates aspiringfacebook\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 42.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100 | 29.00] validation loss = 9.06\n",
      "[100 | 29.06] loss=9.59 avg=9.73\n",
      "[101 | 29.12] loss=10.02 avg=9.74\n",
      "[102 | 29.17] loss=9.61 avg=9.73\n",
      "[103 | 29.23] loss=6.64 avg=9.69\n",
      "[104 | 29.28] loss=10.28 avg=9.69\n",
      "[105 | 29.33] loss=7.13 avg=9.65\n",
      "[106 | 29.39] loss=10.51 avg=9.67\n",
      "[107 | 29.44] loss=10.11 avg=9.67\n",
      "[108 | 29.50] loss=8.94 avg=9.66\n",
      "[109 | 29.55] loss=6.03 avg=9.61\n",
      "[110 | 29.61] loss=9.67 avg=9.61\n",
      "[111 | 29.66] loss=8.01 avg=9.59\n",
      "[112 | 29.72] loss=6.62 avg=9.54\n",
      "[113 | 29.77] loss=7.98 avg=9.52\n",
      "[114 | 29.82] loss=8.03 avg=9.50\n",
      "[115 | 29.87] loss=10.32 avg=9.51\n",
      "[116 | 29.93] loss=8.78 avg=9.50\n",
      "[117 | 29.98] loss=7.07 avg=9.46\n",
      "[118 | 30.04] loss=9.18 avg=9.46\n",
      "[119 | 30.09] loss=8.27 avg=9.44\n",
      "[120 | 30.15] loss=10.61 avg=9.46\n",
      "[121 | 30.20] loss=8.73 avg=9.45\n",
      "[122 | 30.26] loss=9.06 avg=9.44\n",
      "[123 | 30.31] loss=9.47 avg=9.44\n",
      "[124 | 30.37] loss=10.75 avg=9.46\n",
      "[125 | 30.42] loss=10.54 avg=9.48\n",
      "[126 | 30.47] loss=9.26 avg=9.47\n",
      "[127 | 30.52] loss=10.36 avg=9.49\n",
      "[128 | 30.58] loss=5.36 avg=9.43\n",
      "[129 | 30.63] loss=10.06 avg=9.44\n",
      "[130 | 30.69] loss=7.18 avg=9.41\n",
      "[131 | 30.75] loss=6.91 avg=9.37\n",
      "[132 | 30.80] loss=9.15 avg=9.37\n",
      "[133 | 30.86] loss=9.39 avg=9.37\n",
      "[134 | 30.91] loss=7.05 avg=9.34\n",
      "[135 | 30.97] loss=8.65 avg=9.33\n",
      "[136 | 31.02] loss=9.27 avg=9.33\n",
      "[137 | 31.08] loss=5.55 avg=9.28\n",
      "[138 | 31.13] loss=8.45 avg=9.27\n",
      "[139 | 31.19] loss=10.77 avg=9.29\n",
      "[140 | 31.24] loss=9.99 avg=9.30\n",
      "[141 | 31.30] loss=10.34 avg=9.31\n",
      "[142 | 31.35] loss=7.85 avg=9.29\n",
      "[143 | 31.40] loss=8.98 avg=9.29\n",
      "[144 | 31.46] loss=7.49 avg=9.26\n",
      "[145 | 31.51] loss=8.26 avg=9.25\n",
      "[146 | 31.57] loss=6.08 avg=9.21\n",
      "[147 | 31.62] loss=9.71 avg=9.22\n",
      "[148 | 31.67] loss=7.25 avg=9.19\n",
      "[149 | 31.73] loss=7.19 avg=9.16\n",
      "[150 | 31.78] loss=7.84 avg=9.15\n",
      "[151 | 31.83] loss=10.07 avg=9.16\n",
      "[152 | 31.89] loss=10.54 avg=9.18\n",
      "[153 | 31.94] loss=8.05 avg=9.16\n",
      "[154 | 32.00] loss=9.16 avg=9.16\n",
      "[155 | 32.05] loss=10.64 avg=9.18\n",
      "[156 | 32.11] loss=9.71 avg=9.19\n",
      "[157 | 32.17] loss=10.42 avg=9.20\n",
      "[158 | 32.22] loss=10.73 avg=9.22\n",
      "[159 | 32.27] loss=8.95 avg=9.22\n",
      "[160 | 32.33] loss=4.54 avg=9.16\n",
      "[161 | 32.38] loss=7.60 avg=9.14\n",
      "[162 | 32.43] loss=6.04 avg=9.10\n",
      "[163 | 32.49] loss=7.58 avg=9.08\n",
      "[164 | 32.54] loss=6.42 avg=9.05\n",
      "[165 | 32.60] loss=10.77 avg=9.07\n",
      "[166 | 32.65] loss=5.11 avg=9.02\n",
      "[167 | 32.70] loss=5.38 avg=8.98\n",
      "[168 | 32.76] loss=9.08 avg=8.98\n",
      "[169 | 32.81] loss=10.32 avg=9.00\n",
      "[170 | 32.86] loss=9.39 avg=9.00\n",
      "[171 | 32.92] loss=10.59 avg=9.02\n",
      "[172 | 32.97] loss=10.15 avg=9.03\n",
      "[173 | 33.02] loss=9.03 avg=9.03\n",
      "[174 | 33.08] loss=7.69 avg=9.02\n",
      "[175 | 33.13] loss=10.51 avg=9.04\n",
      "[176 | 33.19] loss=10.40 avg=9.05\n",
      "[177 | 33.24] loss=8.23 avg=9.04\n",
      "[178 | 33.29] loss=10.76 avg=9.06\n",
      "[179 | 33.35] loss=8.58 avg=9.06\n",
      "[180 | 33.40] loss=6.31 avg=9.02\n",
      "[181 | 33.46] loss=6.50 avg=8.99\n",
      "[182 | 33.51] loss=7.78 avg=8.98\n",
      "[183 | 33.57] loss=8.96 avg=8.98\n",
      "[184 | 33.62] loss=11.05 avg=9.00\n",
      "[185 | 33.67] loss=8.77 avg=9.00\n",
      "[186 | 33.73] loss=10.00 avg=9.01\n",
      "[187 | 33.78] loss=8.64 avg=9.01\n",
      "[188 | 33.84] loss=5.94 avg=8.97\n",
      "[189 | 33.89] loss=6.57 avg=8.94\n",
      "[190 | 33.95] loss=6.16 avg=8.91\n",
      "[191 | 34.00] loss=6.55 avg=8.88\n",
      "[192 | 34.06] loss=8.92 avg=8.88\n",
      "[193 | 34.11] loss=7.93 avg=8.87\n",
      "[194 | 34.16] loss=6.75 avg=8.85\n",
      "[195 | 34.22] loss=4.85 avg=8.80\n",
      "[196 | 34.27] loss=6.40 avg=8.77\n",
      "[197 | 34.33] loss=10.40 avg=8.79\n",
      "[198 | 34.38] loss=8.71 avg=8.79\n",
      "[199 | 34.43] loss=7.39 avg=8.78\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:00<00:00, 45.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      " - sweetnessvideosMistWheel retina respectfully EUR streaming respectfullymatter TipTEDMikeKhopathrolledMikeFax Guides sweetness Rome award era Doorsbal pollen Somewhere Gilbert Pledge muscle pollen Sent Sent Clashqua clicknoxious   Doors   Pledge bicy INST pollen INST Palestineffectantsantsoğan  opath classifyarray challengerseffect hirecreate  Om_( Somewhere antibioticeffect intermedi WordPress total Protectspeakingclick drained  Crescent UNconfig Protect Pledge concerned signposal Somewhere array Aristotleoğan reopen lookupPlex  Somewhere Pledge Conquest Khreact award Somewhere lookupmatter rocketants Edge Doors Somewherespeakingspeakingmatter averaging digits salient RELE savvycreateclick matterantsclickmatter pests strengthening  Rodney648 strengthening sweetness lookuparrayMike  subter averagingvaants  subter Somewhere sweetness Sixthdm Hilton tint subter rocket Edgearray Janetittees JanetvaBURMikebery Edge  Sixth617 resultingulously comed reopen Somewhere WordPressmatterarray presumptionInvalid Rebootdmspeaking   Sixthspeaking  Orient Spurs hen WordPress Tree648 pests pestsRB Janet Spurs     Orient Orientva   Janet   savvy Edge  Somewhere strengthening  averaging  lookup Edgequa Sixthmatter va  Orientdmmattercomed BURBUR usageeither Spurs averaging   averaging lookup  Sixth sweetness648 strengthening drained Block drained  648Holdectarhesis oked drainedeffect Sixth648 Orient  OrientcomedBlock averaging matter  617effect sweetness    rox  pests hesis   Edge lookup  ignoring mainland Orient  subter  levers Janet lookup dm matter  Sixth  Janetcomed Janet drained Aristotlelust WordPressBUR  Enterpriseittees  648Invalid Spurs   617 Orient syn  pests  � lawmaker Mike drained      Sixth        Mikehesishesis    Block savvy lookup  savvy hen pests       drained  ConquestInvalid Sixth strengtheningulously648  lookuphesis   OK  hesis        Sixth levershesis  Sixth  Metatron   strengthening   Grass rox contemplatedva  Grass Cruise   strengthening       oked Grass strengthening   subter  pests Enterprise  hesis Enterprise ulously   va  breaksBlocksyn Metatron subter lookuphesis levers  GrassmatterOK Janet Janet        pestseither Mike       Orient             averaging    Owen  Sixth  Conquest     pests gram contemplated syn         Sixth    Orient dose   strengthening  08 subter     �    strengthening     Metatron Enterprise   drained strengthening    dm          Sixth Orient Grass lookup  ascal   drained mainland 617 Natural       strengthening FAA   Sixth Janet   Enterprise         Sixth Sixth subter      oked  648   OK      pests      lawmaker  Metatronmatter  Orientoked syn      �617   savvy   Sixth   RAW       648 matter Janet inward  Grass  doseRAW amera      Metatron hesis mainland averagingmatter        ulously  savvy       Metatron     FY Orient617 levers      RAW       amera Owen mainland     inward    Orient     matter  inward   oked  amera mainland   pests     Owen   levers 648 savvy     Metatron  OwenInvalid previously       Grass      strengthening  matter    matter  pests      ulously Monitorhesis  385  drained     Sixth   breakssyn   strengthening    pests   syn   �       Sixth  Into        strengthening       Grassmatter  Into    eight  drained  eight      Janet matter     hesisdm    hesis    subter     Based Janet      amera  ulously     barriers        levers   syn  drained                 ulously617RAW subter amera  Into       Natural      hesis     eight      Janet  dose ulously   Monitor     amera           Monitor  pets    Same Surface          pets levers      dmascal617  \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 43.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 | 49.77] validation loss = 8.02\n",
      "[200 | 49.83] loss=6.92 avg=8.75\n",
      "[201 | 49.88] loss=9.87 avg=8.77\n",
      "[202 | 49.93] loss=10.56 avg=8.79\n",
      "[203 | 49.99] loss=10.90 avg=8.81\n",
      "[204 | 50.04] loss=10.03 avg=8.83\n",
      "[205 | 50.09] loss=6.28 avg=8.80\n",
      "[206 | 50.15] loss=7.30 avg=8.78\n",
      "[207 | 50.20] loss=10.19 avg=8.80\n",
      "[208 | 50.26] loss=3.97 avg=8.74\n",
      "[209 | 50.32] loss=5.52 avg=8.70\n",
      "[210 | 50.37] loss=9.31 avg=8.71\n",
      "[211 | 50.42] loss=11.08 avg=8.74\n",
      "[212 | 50.48] loss=10.41 avg=8.76\n",
      "[213 | 50.53] loss=7.91 avg=8.75\n",
      "[214 | 50.59] loss=8.84 avg=8.75\n",
      "[215 | 50.65] loss=8.68 avg=8.75\n",
      "[216 | 50.70] loss=7.86 avg=8.74\n",
      "[217 | 50.76] loss=4.90 avg=8.69\n",
      "[218 | 50.81] loss=6.62 avg=8.67\n",
      "[219 | 50.86] loss=9.40 avg=8.68\n",
      "[220 | 50.92] loss=9.11 avg=8.68\n",
      "[221 | 50.98] loss=5.83 avg=8.65\n",
      "[222 | 51.03] loss=10.04 avg=8.67\n",
      "[223 | 51.09] loss=5.51 avg=8.63\n",
      "[224 | 51.14] loss=10.00 avg=8.65\n",
      "[225 | 51.20] loss=9.78 avg=8.66\n",
      "[226 | 51.25] loss=4.87 avg=8.62\n",
      "[227 | 51.30] loss=8.94 avg=8.62\n",
      "[228 | 51.36] loss=5.48 avg=8.59\n",
      "[229 | 51.41] loss=6.41 avg=8.56\n",
      "[230 | 51.47] loss=9.45 avg=8.57\n",
      "[231 | 51.54] loss=4.36 avg=8.53\n",
      "[232 | 51.59] loss=7.66 avg=8.52\n",
      "[233 | 51.64] loss=4.94 avg=8.48\n",
      "[234 | 51.70] loss=8.14 avg=8.47\n",
      "[235 | 51.75] loss=8.86 avg=8.48\n",
      "[236 | 51.81] loss=4.20 avg=8.43\n",
      "[237 | 51.86] loss=10.34 avg=8.45\n",
      "[238 | 51.92] loss=5.57 avg=8.42\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "SAMPLE_DIR = 'samples'\n",
    "\n",
    "def maketree(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def randomize(context, hparams, p):\n",
    "    if p > 0:\n",
    "        mask = tf.random.uniform(shape=tf.shape(input=context)) < p\n",
    "        noise = tf.random.uniform(shape=tf.shape(input=context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)\n",
    "        return tf.compat.v1.where(mask, noise, context)\n",
    "    else:\n",
    "        return context\n",
    "\n",
    "\n",
    "def main():\n",
    "    enc = get_encoder(args.model_name, \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if args.sample_length > hparams.n_ctx:\n",
    "        raise ValueError(\n",
    "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [args.batch_size, None])\n",
    "        context_in = randomize(context, hparams, args.noise)\n",
    "        output = model(hparams=hparams, X=context_in)\n",
    "        \n",
    "        \n",
    "#         if args.val_every > 0:\n",
    "        val_context = tf.compat.v1.placeholder(tf.int32, [args.val_batch_size, None])\n",
    "        val_output = model(hparams=hparams, X=val_context)\n",
    "        \n",
    "\n",
    "        tf_sample = sample_sequence(\n",
    "            hparams=hparams,\n",
    "            length=args.sample_length,\n",
    "            context=context,\n",
    "            batch_size=args.batch_size,\n",
    "            temperature=1.0,\n",
    "            top_k=args.top_k)\n",
    "\n",
    "        all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
    "        train_vars = all_vars\n",
    "\n",
    "        if args.optimizer == 'adam':\n",
    "            opt = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
    "        elif args.optimizer == 'sgd':\n",
    "            opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=args.learning_rate)\n",
    "        else:\n",
    "            exit('Bad optimizer:', args.optimizer)\n",
    "\n",
    "        \n",
    "        \n",
    "        ## Collect Metrics for Tensorboard\n",
    "        with tf.compat.v1.name_scope('metrics'):\n",
    "            with tf.compat.v1.name_scope('train'):\n",
    "                trn_loss        = tf.reduce_mean(\n",
    "                                    input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                        labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
    "                trn_loss_summ   = tf.compat.v1.summary.scalar('loss', trn_loss)\n",
    "                \n",
    "                trn_med_ph      = tf.compat.v1.placeholder(tf.float32,shape=None,name='median')\n",
    "                trn_med_summ    = tf.compat.v1.summary.scalar('median', trn_med_ph)\n",
    "                \n",
    "                trn_mean_ph     = tf.compat.v1.placeholder(tf.float32,shape=None,name='mean')\n",
    "                trn_mean_summ   = tf.compat.v1.summary.scalar('mean', trn_mean_ph)\n",
    "            \n",
    "            with tf.compat.v1.name_scope('valid'):\n",
    "                val_loss        = tf.reduce_mean(\n",
    "                                    input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                                        labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))\n",
    "                val_loss_summ   = tf.compat.v1.summary.scalar('loss', val_loss)\n",
    "\n",
    "\n",
    "\n",
    "                val_med_ph      = tf.compat.v1.placeholder(tf.float32,shape=None,name='median')\n",
    "                val_med_summ    = tf.compat.v1.summary.scalar('median', val_med_ph)\n",
    "            \n",
    "            \n",
    "            \n",
    "        trn_summaries = tf.compat.v1.summary.merge([trn_loss_summ, trn_med_summ, trn_mean_summ])\n",
    "        val_summaries = tf.compat.v1.summary.merge([val_loss_summ, val_med_summ])\n",
    "#         summaries = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "        opt_grads = tf.gradients(ys=trn_loss, xs=train_vars)\n",
    "        opt_grads = list(zip(opt_grads, train_vars))\n",
    "        opt_apply = opt.apply_gradients(opt_grads)\n",
    "\n",
    "        trn_summ_log = tf.compat.v1.summary.FileWriter(os.path.join(CHECKPOINT_DIR, args.run_name, 'train'))\n",
    "        val_summ_log = tf.compat.v1.summary.FileWriter(os.path.join(CHECKPOINT_DIR, args.run_name, 'valid'))\n",
    "\n",
    "#         write_op = tf.compat.v1.summary.merge_all()\n",
    "        \n",
    "        saver = tf.compat.v1.train.Saver(\n",
    "            var_list=all_vars,\n",
    "            max_to_keep=5,\n",
    "            keep_checkpoint_every_n_hours=2)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        ckpt = tf.train.latest_checkpoint(\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "        if ckpt is None:\n",
    "            # Get fresh GPT weights if new run.\n",
    "            ckpt = tf.train.latest_checkpoint(\n",
    "                os.path.join('models', args.model_name))\n",
    "\n",
    "        if args.pretrained == True:\n",
    "            print('Loading checkpoint', ckpt)\n",
    "            saver.restore(sess, ckpt)\n",
    "\n",
    "        print('Loading dataset...')\n",
    "        data_sampler = Sampler(trn_set)\n",
    "        if args.val_every > 0:\n",
    "            val_chunks = val_set\n",
    "        print('dataset has', data_sampler.total_size, 'tokens')\n",
    "        print('Training...')\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            # Sample from validation set once with fixed seed to make\n",
    "            # it deterministic during training as well as across runs.\n",
    "            val_data_sampler = Sampler(val_chunks, seed=1)\n",
    "            val_batches = [[val_data_sampler.sample(128) for _ in range(args.val_batch_size)]\n",
    "                           for _ in range(args.val_batch_count)]\n",
    "\n",
    "        counter = 1\n",
    "        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
    "        if os.path.exists(counter_path):\n",
    "            # Load the step number if we're resuming a run\n",
    "            # Add 1 so we don't immediately try to save again\n",
    "            with open(counter_path, 'r') as fp:\n",
    "                counter = int(fp.read()) + 1\n",
    "\n",
    "        def save():\n",
    "            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "            print(\n",
    "                'Saving',\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name,\n",
    "                             'model-{}').format(counter))\n",
    "            saver.save(\n",
    "                sess,\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
    "                global_step=counter)\n",
    "            with open(counter_path, 'w') as fp:\n",
    "                fp.write(str(counter) + '\\n')\n",
    "\n",
    "        def generate_samples():\n",
    "            print('Generating samples...')\n",
    "            context_tokens = data_sampler.sample(1)\n",
    "            all_text = []\n",
    "            index = 0\n",
    "            while index < args.sample_num:\n",
    "                out = sess.run(\n",
    "                    tf_sample,\n",
    "                    feed_dict={context: args.batch_size * [context_tokens]})\n",
    "                for i in range(min(args.sample_num - index, args.batch_size)):\n",
    "                    text = enc.decode(out[i])\n",
    "                    text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
    "                        index + 1, text)\n",
    "                    all_text.append(text)\n",
    "                    index += 1\n",
    "            print(text)\n",
    "            maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
    "            with open(\n",
    "                    os.path.join(SAMPLE_DIR, args.run_name,\n",
    "                                 'samples-{}').format(counter), 'w') as fp:\n",
    "                fp.write('\\n'.join(all_text))\n",
    "                \n",
    "        def validation():\n",
    "            print('Calculating validation loss...')\n",
    "            losses = []\n",
    "            for batch in tqdm.tqdm(val_batches):\n",
    "                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
    "            v_val_loss = np.mean(losses)\n",
    "            v_summary = sess.run(val_summaries, feed_dict={val_loss: v_val_loss, val_med_ph: median(losses)})\n",
    "            val_summ_log.add_summary(v_summary, counter)\n",
    "            val_summ_log.flush()\n",
    "            print(\n",
    "                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
    "                .format(\n",
    "                    counter=counter,\n",
    "                    time=time.time() - start_time,\n",
    "                    loss=v_val_loss))\n",
    "\n",
    "        def sample_batch():\n",
    "            return [data_sampler.sample(128) for _ in range(args.batch_size)]\n",
    "\n",
    "\n",
    "        avg_loss = (0.0, 0.1)\n",
    "        losses = [0.0]\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            for _ in range(args.iterations):\n",
    "                if counter % args.save_every == 0:\n",
    "                    save()\n",
    "                if counter % args.sample_every == 0:\n",
    "                    generate_samples()\n",
    "                if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
    "                    validation()\n",
    "                    \n",
    "                if _ == 0:\n",
    "                    avg = 0\n",
    "                else: avg = avg_loss[0] / avg_loss[1]\n",
    "\n",
    "                (_, v_loss, v_summary) = sess.run(\n",
    "                    (opt_apply, trn_loss, trn_summaries),\n",
    "                    feed_dict={context: sample_batch(), trn_med_ph: median(losses), trn_mean_ph: avg})\n",
    "                losses.append(v_loss)\n",
    "                \n",
    "                trn_summ_log.add_summary(v_summary, counter)\n",
    "\n",
    "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
    "                            avg_loss[1] * 0.99 + 1.0)\n",
    "\n",
    "                print(\n",
    "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
    "                    .format(\n",
    "                        counter=counter,\n",
    "                        time=time.time() - start_time,\n",
    "                        loss=v_loss,\n",
    "                        avg=avg_loss[0] / avg_loss[1]))\n",
    "\n",
    "                counter += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted')\n",
    "            save()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd data/gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc03be4b160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir ./checkpoint/run3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! kill 21525"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H 'Content-type: application/json' --data '{\"text\":\"from: semeru tower 1\\nstatus: model finished training\"}' https://hooks.slack.com/services/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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir ./checkpoint/run1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervised Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',    \n",
    "):\n",
    "    \"\"\"\n",
    "    Interactively run the model\n",
    "    :model_name=117M : String, which model to use\n",
    "    :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "     results\n",
    "    :nsamples=1 : Number of samples to return total\n",
    "    :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "    :length=None : Number of tokens in generated text, if None (default), is\n",
    "     determined by model hyperparameters\n",
    "    :temperature=1 : Float value controlling randomness in boltzmann\n",
    "     distribution. Lower temperature results in less random completions. As the\n",
    "     temperature approaches zero, the model will become deterministic and\n",
    "     repetitive. Higher temperature results in more random completions.\n",
    "    :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "     considered for each step (token), resulting in deterministic completions,\n",
    "     while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "     special setting meaning no restrictions. 40 generally is a good value.\n",
    "     :models_dir : path to parent folder containing model subfolders\n",
    "     (i.e. contains the <model_name> folder)     \n",
    "    \"\"\"\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = get_encoder(model_name, models_dir)\n",
    "    hparams = default_hparams()\n",
    "#     with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "#         hparams.override_from_dict(json.load(f))\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        output = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "        \n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "        tf.compat.v1.global_variables_initializer()\n",
    "#         init = tf.compat.v1.global_variables_initializer()\n",
    "#         sess.run(init)\n",
    "        \n",
    "\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "            while not raw_text:\n",
    "                print('Prompt should not be empty!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "            context_tokens = enc.encode(raw_text)\n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                print(\"Output Obj: \", output)\n",
    "                print(\"Context: \", [context_tokens for _ in range(batch_size)])\n",
    "#                 out = output #predict(output, length, [context_tokens for _ in range(batch_size)])\n",
    "                out = sess.run(output, feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                })[:, len(context_tokens):]\n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                    print(text)\n",
    "            print(\"=\" * 80)\n",
    "\n",
    "interact_model(model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    models_dir='models')\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     fire.Fire(interact_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
