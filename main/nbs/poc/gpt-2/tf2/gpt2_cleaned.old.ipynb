{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'data/gpt-2/'\n",
      "/tf/prototypes/gpt-2/tf2/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.1.3)\n",
      "Requirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (2017.4.5)\n",
      "Requirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (2.21.0)\n",
      "Requirement already satisfied: tqdm==4.31.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (4.31.1)\n",
      "Requirement already satisfied: toposort==1.5 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1)\n",
    "        ]\n",
    "\n",
    "    past, prev, output = body(None, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "    _, _, tokens = tf.while_loop(\n",
    "        cond=cond, body=body,\n",
    "        maximum_iterations=length - 1,\n",
    "        loop_vars=[\n",
    "            past,\n",
    "            prev,\n",
    "            output\n",
    "        ],\n",
    "        shape_invariants=[\n",
    "            tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "            tf.TensorShape([batch_size, None]),\n",
    "        ],\n",
    "        back_prop=False,\n",
    "    )\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            if i % 5000 == 0:\n",
    "                print(i)\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "                \n",
    "            if i == 50000:\n",
    "                print(\"Breaking\")\n",
    "                break\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "        \n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "        if 'after.java' not in path:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "            tokens = np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except:\n",
    "            print(e)\n",
    "        if i >= 100000:\n",
    "            break\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                dataset=\"../sciclone/data10/mtufano/deepLearningMutants/out/changes/code\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1,\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=40,\n",
    "                top_p=0.0,\n",
    "                run_name=\"run1\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=None,\n",
    "                val_batch_size=2,\n",
    "                val_batch_count=40,\n",
    "                val_every=100\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/115158 [00:00<07:35, 252.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Breaking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 99948/115158 [01:25<00:09, 1660.01it/s]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25358"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "data_set = load_dataset(enc, args.dataset, args.combine)\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20286, 2535, 2535)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SET_SIZE = len(data_set)\n",
    "TRN_SET_SIZE = int(DATA_SET_SIZE * 0.8)\n",
    "VAL_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "TST_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "\n",
    "trn_set = data_set[:TRN_SET_SIZE]\n",
    "val_set = data_set[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "tst_set = data_set[-TST_SET_SIZE:]\n",
    "len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 99948/115158 [01:40<00:09, 1660.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint checkpoint/run1/model-101\n",
      "Loading dataset...\n",
      "dataset has 15705181 tokens\n",
      "Training...\n",
      "[102 | 5.33] loss=2.31 avg=2.31\n",
      "[103 | 5.39] loss=1.16 avg=1.73\n",
      "[104 | 5.44] loss=3.00 avg=2.16\n",
      "[105 | 5.50] loss=1.70 avg=2.04\n",
      "[106 | 5.55] loss=3.01 avg=2.24\n",
      "[107 | 5.60] loss=2.16 avg=2.23\n",
      "[108 | 5.66] loss=2.31 avg=2.24\n",
      "[109 | 5.71] loss=1.57 avg=2.15\n",
      "[110 | 5.76] loss=3.10 avg=2.26\n",
      "[111 | 5.81] loss=2.13 avg=2.25\n",
      "[112 | 5.86] loss=2.05 avg=2.23\n",
      "[113 | 5.91] loss=2.31 avg=2.23\n",
      "[114 | 5.97] loss=1.19 avg=2.15\n",
      "[115 | 6.03] loss=2.47 avg=2.17\n",
      "[116 | 6.08] loss=2.97 avg=2.23\n",
      "[117 | 6.13] loss=1.20 avg=2.16\n",
      "[118 | 6.19] loss=2.04 avg=2.15\n",
      "[119 | 6.25] loss=1.93 avg=2.14\n",
      "[120 | 6.30] loss=1.75 avg=2.12\n",
      "[121 | 6.35] loss=2.07 avg=2.11\n",
      "[122 | 6.41] loss=2.89 avg=2.16\n",
      "[123 | 6.46] loss=3.39 avg=2.22\n",
      "[124 | 6.52] loss=2.22 avg=2.22\n",
      "[125 | 6.58] loss=1.27 avg=2.17\n",
      "[126 | 6.64] loss=2.08 avg=2.17\n",
      "[127 | 6.69] loss=2.23 avg=2.17\n",
      "[128 | 6.74] loss=0.96 avg=2.12\n",
      "[129 | 6.80] loss=3.36 avg=2.17\n",
      "[130 | 6.85] loss=2.21 avg=2.17\n",
      "[131 | 6.91] loss=1.45 avg=2.15\n",
      "[132 | 6.96] loss=0.04 avg=2.07\n",
      "[133 | 7.02] loss=2.51 avg=2.08\n",
      "[134 | 7.07] loss=0.82 avg=2.04\n",
      "[135 | 7.13] loss=1.84 avg=2.03\n",
      "[136 | 7.18] loss=2.10 avg=2.03\n",
      "[137 | 7.23] loss=1.20 avg=2.01\n",
      "[138 | 7.28] loss=1.87 avg=2.00\n",
      "[139 | 7.33] loss=2.62 avg=2.02\n",
      "[140 | 7.39] loss=2.43 avg=2.03\n",
      "[141 | 7.44] loss=2.43 avg=2.05\n",
      "[142 | 7.50] loss=2.33 avg=2.05\n",
      "[143 | 7.55] loss=2.53 avg=2.07\n",
      "[144 | 7.61] loss=0.93 avg=2.04\n",
      "[145 | 7.66] loss=1.98 avg=2.03\n",
      "[146 | 7.71] loss=2.82 avg=2.06\n",
      "[147 | 7.77] loss=2.08 avg=2.06\n",
      "[148 | 7.82] loss=2.82 avg=2.08\n",
      "[149 | 7.88] loss=2.26 avg=2.08\n",
      "[150 | 7.93] loss=2.19 avg=2.08\n",
      "[151 | 7.99] loss=1.47 avg=2.07\n",
      "[152 | 8.04] loss=2.12 avg=2.07\n",
      "[153 | 8.09] loss=1.93 avg=2.07\n",
      "[154 | 8.15] loss=3.45 avg=2.10\n",
      "[155 | 8.20] loss=2.31 avg=2.11\n",
      "[156 | 8.25] loss=2.90 avg=2.12\n",
      "[157 | 8.31] loss=2.83 avg=2.14\n",
      "[158 | 8.36] loss=2.12 avg=2.14\n",
      "[159 | 8.42] loss=1.48 avg=2.12\n",
      "[160 | 8.47] loss=3.03 avg=2.15\n",
      "[161 | 8.53] loss=1.79 avg=2.14\n",
      "[162 | 8.58] loss=1.37 avg=2.12\n",
      "[163 | 8.63] loss=1.64 avg=2.11\n",
      "[164 | 8.69] loss=1.17 avg=2.09\n",
      "[165 | 8.74] loss=3.13 avg=2.11\n",
      "[166 | 8.79] loss=2.54 avg=2.12\n",
      "[167 | 8.85] loss=0.99 avg=2.10\n",
      "[168 | 8.90] loss=2.84 avg=2.11\n",
      "[169 | 8.96] loss=1.79 avg=2.11\n",
      "[170 | 9.01] loss=2.17 avg=2.11\n",
      "[171 | 9.06] loss=3.82 avg=2.14\n",
      "[172 | 9.11] loss=1.77 avg=2.13\n",
      "[173 | 9.17] loss=2.72 avg=2.15\n",
      "[174 | 9.22] loss=0.64 avg=2.12\n",
      "[175 | 9.27] loss=1.57 avg=2.11\n",
      "[176 | 9.33] loss=2.11 avg=2.11\n",
      "[177 | 9.38] loss=2.24 avg=2.11\n",
      "[178 | 9.43] loss=2.20 avg=2.11\n",
      "[179 | 9.49] loss=2.52 avg=2.12\n",
      "[180 | 9.54] loss=1.70 avg=2.11\n",
      "[181 | 9.59] loss=2.23 avg=2.11\n",
      "[182 | 9.65] loss=3.40 avg=2.14\n",
      "[183 | 9.70] loss=2.12 avg=2.14\n",
      "[184 | 9.76] loss=0.79 avg=2.11\n",
      "[185 | 9.81] loss=1.47 avg=2.10\n",
      "[186 | 9.86] loss=3.84 avg=2.13\n",
      "[187 | 9.92] loss=1.45 avg=2.12\n",
      "[188 | 9.97] loss=1.50 avg=2.11\n",
      "[189 | 10.03] loss=2.23 avg=2.11\n",
      "[190 | 10.08] loss=2.28 avg=2.11\n",
      "[191 | 10.14] loss=1.50 avg=2.10\n",
      "[192 | 10.19] loss=2.67 avg=2.11\n",
      "[193 | 10.24] loss=1.68 avg=2.11\n",
      "[194 | 10.30] loss=1.33 avg=2.09\n",
      "[195 | 10.35] loss=1.98 avg=2.09\n",
      "[196 | 10.40] loss=0.93 avg=2.07\n",
      "[197 | 10.45] loss=2.14 avg=2.07\n",
      "[198 | 10.51] loss=0.67 avg=2.05\n",
      "[199 | 10.56] loss=2.22 avg=2.05\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "  The most important thing that you will NOT want to get in an old car                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  2%|▎         | 1/40 [00:02<01:30,  2.31s/it]\u001b[A\n",
      " 15%|█▌        | 6/40 [00:02<00:55,  1.63s/it]\u001b[A\n",
      " 28%|██▊       | 11/40 [00:02<00:33,  1.14s/it]\u001b[A\n",
      " 40%|████      | 16/40 [00:02<00:19,  1.24it/s]\u001b[A\n",
      " 50%|█████     | 20/40 [00:02<00:11,  1.74it/s]\u001b[A\n",
      " 62%|██████▎   | 25/40 [00:02<00:06,  2.45it/s]\u001b[A\n",
      " 75%|███████▌  | 30/40 [00:03<00:02,  3.41it/s]\u001b[A\n",
      " 85%|████████▌ | 34/40 [00:03<00:01,  4.69it/s]\u001b[A\n",
      " 98%|█████████▊| 39/40 [00:03<00:00,  6.40it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:03<00:00, 12.28it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200 | 32.34] validation loss = 2.09\n",
      "[200 | 32.39] loss=1.77 avg=2.05\n",
      "[201 | 32.44] loss=1.61 avg=2.04\n",
      "[202 | 32.49] loss=2.45 avg=2.05\n",
      "[203 | 32.54] loss=1.62 avg=2.04\n",
      "[204 | 32.59] loss=1.53 avg=2.03\n",
      "[205 | 32.65] loss=0.99 avg=2.02\n",
      "[206 | 32.70] loss=2.68 avg=2.03\n",
      "[207 | 32.75] loss=3.38 avg=2.05\n",
      "[208 | 32.80] loss=2.37 avg=2.05\n",
      "[209 | 32.86] loss=2.72 avg=2.06\n",
      "[210 | 32.91] loss=2.15 avg=2.06\n",
      "[211 | 32.96] loss=2.78 avg=2.08\n",
      "[212 | 33.01] loss=2.48 avg=2.08\n",
      "[213 | 33.07] loss=1.88 avg=2.08\n",
      "[214 | 33.12] loss=1.42 avg=2.07\n",
      "[215 | 33.17] loss=2.51 avg=2.07\n",
      "[216 | 33.23] loss=2.56 avg=2.08\n",
      "[217 | 33.28] loss=3.23 avg=2.10\n",
      "[218 | 33.34] loss=2.11 avg=2.10\n",
      "[219 | 33.39] loss=2.32 avg=2.10\n",
      "[220 | 33.45] loss=1.48 avg=2.09\n",
      "[221 | 33.50] loss=2.91 avg=2.10\n",
      "[222 | 33.56] loss=0.16 avg=2.08\n",
      "[223 | 33.61] loss=2.39 avg=2.08\n",
      "[224 | 33.66] loss=2.59 avg=2.09\n",
      "[225 | 33.72] loss=3.55 avg=2.11\n",
      "[226 | 33.77] loss=2.55 avg=2.12\n",
      "[227 | 33.82] loss=2.81 avg=2.13\n",
      "[228 | 33.88] loss=2.06 avg=2.12\n",
      "[229 | 33.93] loss=2.06 avg=2.12\n",
      "[230 | 33.99] loss=1.05 avg=2.11\n",
      "[231 | 34.04] loss=2.67 avg=2.12\n",
      "[232 | 34.09] loss=0.04 avg=2.09\n",
      "[233 | 34.15] loss=2.36 avg=2.09\n",
      "[234 | 34.20] loss=1.94 avg=2.09\n",
      "[235 | 34.25] loss=2.67 avg=2.10\n",
      "[236 | 34.31] loss=1.52 avg=2.09\n",
      "[237 | 34.36] loss=2.52 avg=2.10\n",
      "[238 | 34.41] loss=0.96 avg=2.08\n",
      "[239 | 34.46] loss=1.82 avg=2.08\n",
      "[240 | 34.52] loss=1.63 avg=2.07\n",
      "[241 | 34.57] loss=0.33 avg=2.05\n",
      "[242 | 34.62] loss=1.19 avg=2.04\n",
      "[243 | 34.68] loss=2.17 avg=2.04\n",
      "[244 | 34.73] loss=1.93 avg=2.04\n",
      "[245 | 34.79] loss=1.63 avg=2.03\n",
      "[246 | 34.84] loss=2.51 avg=2.04\n",
      "[247 | 34.90] loss=1.24 avg=2.03\n",
      "[248 | 34.95] loss=2.57 avg=2.03\n",
      "[249 | 35.00] loss=1.22 avg=2.02\n",
      "[250 | 35.05] loss=2.27 avg=2.03\n",
      "[251 | 35.11] loss=1.73 avg=2.02\n",
      "[252 | 35.16] loss=1.99 avg=2.02\n",
      "[253 | 35.22] loss=2.04 avg=2.02\n",
      "[254 | 35.27] loss=2.06 avg=2.02\n",
      "[255 | 35.32] loss=2.48 avg=2.03\n",
      "[256 | 35.38] loss=1.20 avg=2.02\n",
      "[257 | 35.43] loss=0.86 avg=2.00\n",
      "[258 | 35.48] loss=2.41 avg=2.01\n",
      "[259 | 35.54] loss=0.84 avg=1.99\n",
      "[260 | 35.59] loss=1.67 avg=1.99\n",
      "[261 | 35.64] loss=2.66 avg=2.00\n",
      "[262 | 35.69] loss=2.15 avg=2.00\n",
      "[263 | 35.75] loss=1.54 avg=1.99\n",
      "[264 | 35.80] loss=1.75 avg=1.99\n",
      "[265 | 35.85] loss=2.25 avg=1.99\n",
      "[266 | 35.91] loss=2.29 avg=2.00\n",
      "[267 | 35.96] loss=2.40 avg=2.00\n",
      "[268 | 36.01] loss=2.44 avg=2.01\n",
      "[269 | 36.07] loss=0.42 avg=1.99\n",
      "[270 | 36.12] loss=1.93 avg=1.99\n",
      "[271 | 36.17] loss=2.75 avg=2.00\n",
      "[272 | 36.23] loss=0.41 avg=1.98\n",
      "[273 | 36.28] loss=3.32 avg=2.00\n",
      "[274 | 36.34] loss=2.40 avg=2.00\n",
      "[275 | 36.39] loss=1.12 avg=1.99\n",
      "[276 | 36.44] loss=1.65 avg=1.99\n",
      "[277 | 36.50] loss=2.53 avg=1.99\n",
      "[278 | 36.55] loss=1.94 avg=1.99\n",
      "[279 | 36.61] loss=2.42 avg=2.00\n",
      "[280 | 36.66] loss=2.74 avg=2.01\n",
      "[281 | 36.71] loss=1.76 avg=2.00\n",
      "[282 | 36.77] loss=2.05 avg=2.00\n",
      "[283 | 36.82] loss=2.01 avg=2.00\n",
      "[284 | 36.88] loss=2.23 avg=2.01\n",
      "[285 | 36.93] loss=1.82 avg=2.00\n",
      "[286 | 36.98] loss=1.71 avg=2.00\n",
      "[287 | 37.04] loss=2.21 avg=2.00\n",
      "[288 | 37.09] loss=1.63 avg=2.00\n",
      "[289 | 37.14] loss=2.97 avg=2.01\n",
      "[290 | 37.20] loss=1.70 avg=2.01\n",
      "[291 | 37.25] loss=2.55 avg=2.01\n",
      "[292 | 37.30] loss=1.76 avg=2.01\n",
      "[293 | 37.36] loss=2.76 avg=2.02\n",
      "[294 | 37.41] loss=3.02 avg=2.03\n",
      "[295 | 37.46] loss=1.99 avg=2.03\n",
      "[296 | 37.51] loss=1.55 avg=2.02\n",
      "[297 | 37.57] loss=2.16 avg=2.03\n",
      "[298 | 37.62] loss=2.17 avg=2.03\n",
      "[299 | 37.68] loss=1.21 avg=2.02\n",
      "Generating samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/40 [00:00<?, ?it/s]\u001b[A\n",
      " 12%|█▎        | 5/40 [00:00<00:00, 41.57it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== SAMPLE 1 ========\n",
      "(s)) * s.type.length * s.length + 2); // check if s.is_null (s.type.length) return s[s]; } return true; }\n",
      "\n",
      "The two functions are really in the same file. I created these from the source code, just because we're getting in front of that. We could then do things like update s and add some arguments. But there aren't many calls in that file. It is better to make it read only. I'm not happy about that. If we want to call things, we need to set up our own methods. Let's break down those.\n",
      "\n",
      "Update / Initialize s with s.update({}); // update the contents with .apply(s.first, s.last, ...);\n",
      "\n",
      "The code below will call s.update(... args) in order to update the contents of a file. We should be able to read all the files. We'll only get one file. The function s.update(s) now takes in different arguments to update a new file. First, we must update s.first. We should also update s first before any of s.last. Finally, we should replace these arguments with values from our function. For example, we could look at this:\n",
      "\n",
      "{-# LANGUAGE TemplateHaskell, TemplateHaskell.Extension #-} import qualified Data.Text as T as T.Source s.getInt(s.last, s.first, ...) = s.(.apply(s, S.LENGTH(, ...), ...)); s.***s = s.length; s.apply(s); /* add some functions */ def update(s, ... args): if s.first.get(s._full, ...): s.first.add(s); end -- ... return s; } class GtkWidget that extends GtkWidget { def getInt(s, ... args): if s.first.get(s._full, ...): s.first.get(s._length, args) = s.last; end def update(s, ... args): if s.first.get(s._full, ...): s.first.update(s); end def update(s, ... args): if s.last.get(s._full, ...): s.last.update(s); end def update(s, ... args): if s.last.get(s._full, ...): s.last.update(s); end } class GtkWidget { def getInt(s, ... args): if s.first.get(s._full, ...): s.first.get(s[s]) = S.LENGTH(, ...), s.last; end def update(s, ... args): if s.last.get(s._full, ...): s.last.update(s); end def getInt(s, ... args): if s.last.get(s._full, ...): s.last.get(s[s]) = S.LENGTH(, ...), s; end def update(s, ... args): if s.last.get(s._full, ...): s.last.get(s[s]) = S.LENGTH(, ...), s; end } class GuiWidget : Custom Gui { def getChar(s, ... args): from GtkWidget._source add: GkGui.add (s, GkGui.size_t.length); for x in range (0, args): if x == null: if abs(x) >= g.max_length: return g.char_for_all(x); return g.char_for_all(x, 1); } class GtkButton : GtkWidget { def getInt(s, ... args): from GtkWidget._source add: GkButton.add (s, GkButton.size_t.length); for x in range (0, args): g.max_length = x.length + 1; g.max_length = g.max_length + 1; } class GtkButton : GraphicsButton { def getInt(s, ... args): from GtkWidget._source add: GkButton.add (s, GkButton.size_t.length); g.add = g.add x; g.add.setAttribute( \"start\" , \"left\" , x); g.add().setAttribute( \"border-radius\" , \"0px 0px 100px 100px\" , GtkButton.getFloatWidth(x, 1.3, 0)); g.add().setAttribute( \"width\" , \"100%(x);\"); g.add().setAttribute( \"height\" , \"400%(x);\"); g\n",
      "\n",
      "Calculating validation loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 25%|██▌       | 10/40 [00:00<00:00, 41.75it/s]\u001b[A\n",
      " 35%|███▌      | 14/40 [00:00<00:00, 40.28it/s]\u001b[A\n",
      " 48%|████▊     | 19/40 [00:00<00:00, 40.73it/s]\u001b[A\n",
      " 60%|██████    | 24/40 [00:00<00:00, 41.18it/s]\u001b[A\n",
      " 72%|███████▎  | 29/40 [00:00<00:00, 41.61it/s]\u001b[A\n",
      " 82%|████████▎ | 33/40 [00:00<00:00, 41.05it/s]\u001b[A\n",
      " 95%|█████████▌| 38/40 [00:00<00:00, 41.19it/s]\u001b[A\n",
      "100%|██████████| 40/40 [00:00<00:00, 41.08it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300 | 52.61] validation loss = 2.08\n",
      "[300 | 52.67] loss=1.86 avg=2.02\n",
      "[301 | 52.72] loss=2.26 avg=2.02\n",
      "[302 | 52.77] loss=1.99 avg=2.02\n",
      "[303 | 52.83] loss=1.60 avg=2.01\n",
      "[304 | 52.88] loss=2.58 avg=2.02\n",
      "[305 | 52.93] loss=2.55 avg=2.03\n",
      "[306 | 52.99] loss=2.05 avg=2.03\n",
      "[307 | 53.05] loss=2.34 avg=2.03\n",
      "[308 | 53.10] loss=1.87 avg=2.03\n",
      "[309 | 53.15] loss=1.57 avg=2.02\n",
      "[310 | 53.20] loss=0.25 avg=2.00\n",
      "[311 | 53.26] loss=1.81 avg=2.00\n",
      "[312 | 53.31] loss=2.58 avg=2.01\n",
      "[313 | 53.36] loss=0.04 avg=1.98\n",
      "[314 | 53.42] loss=2.40 avg=1.99\n",
      "[315 | 53.47] loss=2.64 avg=2.00\n",
      "[316 | 53.52] loss=0.32 avg=1.98\n",
      "[317 | 53.58] loss=1.18 avg=1.97\n",
      "[318 | 53.63] loss=1.84 avg=1.97\n",
      "[319 | 53.69] loss=2.21 avg=1.97\n",
      "[320 | 53.74] loss=1.18 avg=1.96\n",
      "[321 | 53.79] loss=3.04 avg=1.97\n",
      "[322 | 53.84] loss=2.31 avg=1.98\n",
      "[323 | 53.90] loss=1.87 avg=1.98\n",
      "[324 | 53.95] loss=2.46 avg=1.98\n",
      "[325 | 54.00] loss=1.84 avg=1.98\n",
      "[326 | 54.06] loss=1.42 avg=1.97\n",
      "[327 | 54.11] loss=0.76 avg=1.96\n",
      "[328 | 54.16] loss=2.78 avg=1.97\n",
      "[329 | 54.21] loss=1.60 avg=1.96\n",
      "[330 | 54.27] loss=2.02 avg=1.97\n",
      "[331 | 54.33] loss=1.69 avg=1.96\n",
      "[332 | 54.38] loss=0.23 avg=1.94\n",
      "[333 | 54.43] loss=2.04 avg=1.94\n",
      "[334 | 54.49] loss=2.24 avg=1.95\n",
      "[335 | 54.54] loss=1.32 avg=1.94\n",
      "[336 | 54.60] loss=1.00 avg=1.93\n",
      "[337 | 54.65] loss=1.53 avg=1.93\n",
      "[338 | 54.70] loss=1.70 avg=1.92\n",
      "[339 | 54.76] loss=1.21 avg=1.92\n",
      "[340 | 54.81] loss=1.48 avg=1.91\n",
      "[341 | 54.87] loss=1.71 avg=1.91\n",
      "[342 | 54.92] loss=2.56 avg=1.92\n",
      "[343 | 54.97] loss=1.92 avg=1.92\n",
      "[344 | 55.03] loss=1.65 avg=1.91\n",
      "[345 | 55.08] loss=2.56 avg=1.92\n",
      "[346 | 55.13] loss=1.82 avg=1.92\n",
      "[347 | 55.18] loss=2.27 avg=1.92\n",
      "[348 | 55.24] loss=1.75 avg=1.92\n",
      "[349 | 55.29] loss=2.58 avg=1.93\n",
      "[350 | 55.34] loss=1.09 avg=1.92\n",
      "[351 | 55.39] loss=1.52 avg=1.91\n",
      "[352 | 55.44] loss=0.04 avg=1.89\n",
      "[353 | 55.50] loss=2.55 avg=1.90\n",
      "[354 | 55.56] loss=1.76 avg=1.90\n",
      "[355 | 55.61] loss=3.46 avg=1.92\n",
      "[356 | 55.66] loss=2.54 avg=1.92\n",
      "[357 | 55.71] loss=2.81 avg=1.93\n",
      "[358 | 55.76] loss=1.43 avg=1.93\n",
      "[359 | 55.81] loss=2.26 avg=1.93\n",
      "[360 | 55.87] loss=0.68 avg=1.92\n",
      "[361 | 55.92] loss=1.20 avg=1.91\n",
      "[362 | 55.98] loss=2.72 avg=1.92\n",
      "[363 | 56.03] loss=2.76 avg=1.93\n",
      "[364 | 56.09] loss=0.04 avg=1.91\n",
      "[365 | 56.14] loss=2.98 avg=1.92\n",
      "[366 | 56.19] loss=2.22 avg=1.92\n",
      "[367 | 56.24] loss=2.07 avg=1.92\n",
      "[368 | 56.30] loss=2.40 avg=1.93\n",
      "[369 | 56.35] loss=1.35 avg=1.92\n",
      "[370 | 56.40] loss=1.14 avg=1.91\n",
      "[371 | 56.46] loss=1.38 avg=1.91\n",
      "[372 | 56.51] loss=0.04 avg=1.89\n",
      "[373 | 56.57] loss=2.15 avg=1.89\n",
      "[374 | 56.62] loss=1.22 avg=1.88\n",
      "[375 | 56.68] loss=2.41 avg=1.89\n",
      "[376 | 56.73] loss=0.66 avg=1.88\n",
      "[377 | 56.79] loss=1.67 avg=1.87\n",
      "[378 | 56.84] loss=2.70 avg=1.88\n",
      "[379 | 56.90] loss=2.07 avg=1.88\n",
      "[380 | 56.95] loss=2.31 avg=1.89\n",
      "[381 | 57.01] loss=2.51 avg=1.90\n",
      "[382 | 57.07] loss=0.19 avg=1.88\n",
      "[383 | 57.12] loss=1.42 avg=1.87\n",
      "[384 | 57.17] loss=2.92 avg=1.88\n",
      "[385 | 57.23] loss=1.39 avg=1.88\n",
      "[386 | 57.28] loss=1.84 avg=1.88\n",
      "[387 | 57.34] loss=1.68 avg=1.88\n",
      "[388 | 57.39] loss=1.61 avg=1.87\n",
      "[389 | 57.44] loss=1.10 avg=1.86\n",
      "[390 | 57.50] loss=2.90 avg=1.88\n",
      "[391 | 57.55] loss=2.15 avg=1.88\n",
      "[392 | 57.61] loss=2.54 avg=1.89\n",
      "[393 | 57.66] loss=2.25 avg=1.89\n",
      "[394 | 57.71] loss=2.58 avg=1.90\n",
      "[395 | 57.76] loss=2.94 avg=1.91\n",
      "[396 | 57.82] loss=1.92 avg=1.91\n",
      "[397 | 57.87] loss=2.84 avg=1.92\n",
      "[398 | 57.92] loss=1.82 avg=1.92\n",
      "[399 | 57.98] loss=0.56 avg=1.90\n",
      "Generating samples...\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-400\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_DIR = 'checkpoint'\n",
    "SAMPLE_DIR = 'samples'\n",
    "\n",
    "def maketree(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def randomize(context, hparams, p):\n",
    "    if p > 0:\n",
    "        mask = tf.random.uniform(shape=tf.shape(input=context)) < p\n",
    "        noise = tf.random.uniform(shape=tf.shape(input=context), minval=0, maxval=hparams.n_vocab, dtype=tf.int32)\n",
    "        return tf.compat.v1.where(mask, noise, context)\n",
    "    else:\n",
    "        return context\n",
    "\n",
    "\n",
    "def main():\n",
    "    enc = get_encoder(args.model_name, \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if args.sample_length > hparams.n_ctx:\n",
    "        raise ValueError(\n",
    "            \"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    config = tf.compat.v1.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.graph_options.rewrite_options.layout_optimizer = rewriter_config_pb2.RewriterConfig.OFF\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [args.batch_size, None])\n",
    "        context_in = context # randomize(context, hparams, args.noise)\n",
    "        output = model(hparams=hparams, X=context_in)\n",
    "        loss = tf.reduce_mean(\n",
    "            input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=context[:, 1:], logits=output['logits'][:, :-1]))\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            val_context = tf.compat.v1.placeholder(tf.int32, [args.val_batch_size, None])\n",
    "            val_output = model(hparams=hparams, X=val_context)\n",
    "            val_loss = tf.reduce_mean(\n",
    "                input_tensor=tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    labels=val_context[:, 1:], logits=val_output['logits'][:, :-1]))\n",
    "            val_loss_summary = tf.compat.v1.summary.scalar('val_loss', val_loss)\n",
    "\n",
    "\n",
    "        tf_sample = sample_sequence(\n",
    "            hparams=hparams,\n",
    "            length=args.sample_length,\n",
    "            context=context,\n",
    "            batch_size=args.batch_size,\n",
    "            temperature=1.0,\n",
    "            top_k=args.top_k)\n",
    "\n",
    "        all_vars = [v for v in tf.compat.v1.trainable_variables() if 'model' in v.name]\n",
    "        train_vars = all_vars\n",
    "\n",
    "        if args.optimizer == 'adam':\n",
    "            opt = tf.compat.v1.train.AdamOptimizer(learning_rate=args.learning_rate)\n",
    "        elif args.optimizer == 'sgd':\n",
    "            opt = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=args.learning_rate)\n",
    "        else:\n",
    "            exit('Bad optimizer:', args.optimizer)\n",
    "\n",
    "        opt_grads = tf.gradients(ys=loss, xs=train_vars)\n",
    "        opt_grads = list(zip(opt_grads, train_vars))\n",
    "        opt_apply = opt.apply_gradients(opt_grads)\n",
    "        summary_loss = tf.compat.v1.summary.scalar('loss', loss)\n",
    "\n",
    "        summaries = tf.compat.v1.summary.merge([summary_loss])\n",
    "\n",
    "        summary_log = tf.compat.v1.summary.FileWriter(\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver(\n",
    "            var_list=all_vars,\n",
    "            max_to_keep=5,\n",
    "            keep_checkpoint_every_n_hours=2)\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        ckpt = tf.train.latest_checkpoint(\n",
    "            os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "        if ckpt is None:\n",
    "            # Get fresh GPT weights if new run.\n",
    "            ckpt = tf.train.latest_checkpoint(\n",
    "                os.path.join('models', args.model_name))\n",
    "\n",
    "        print('Loading checkpoint', ckpt)\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        print('Loading dataset...')\n",
    "        data_sampler = Sampler(trn_set)\n",
    "        if args.val_every > 0:\n",
    "            val_chunks = val_set\n",
    "        print('dataset has', data_sampler.total_size, 'tokens')\n",
    "        print('Training...')\n",
    "\n",
    "        if args.val_every > 0:\n",
    "            # Sample from validation set once with fixed seed to make\n",
    "            # it deterministic during training as well as across runs.\n",
    "            val_data_sampler = Sampler(val_chunks, seed=1)\n",
    "            val_batches = [[val_data_sampler.sample(128) for _ in range(args.val_batch_size)]\n",
    "                           for _ in range(args.val_batch_count)]\n",
    "\n",
    "        counter = 1\n",
    "        counter_path = os.path.join(CHECKPOINT_DIR, args.run_name, 'counter')\n",
    "        if os.path.exists(counter_path):\n",
    "            # Load the step number if we're resuming a run\n",
    "            # Add 1 so we don't immediately try to save again\n",
    "            with open(counter_path, 'r') as fp:\n",
    "                counter = int(fp.read()) + 1\n",
    "\n",
    "        def save():\n",
    "            maketree(os.path.join(CHECKPOINT_DIR, args.run_name))\n",
    "            print(\n",
    "                'Saving',\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name,\n",
    "                             'model-{}').format(counter))\n",
    "            saver.save(\n",
    "                sess,\n",
    "                os.path.join(CHECKPOINT_DIR, args.run_name, 'model'),\n",
    "                global_step=counter)\n",
    "            with open(counter_path, 'w') as fp:\n",
    "                fp.write(str(counter) + '\\n')\n",
    "\n",
    "        def generate_samples():\n",
    "            print('Generating samples...')\n",
    "            context_tokens = data_sampler.sample(1)\n",
    "            all_text = []\n",
    "            index = 0\n",
    "            while index < args.sample_num:\n",
    "                out = sess.run(\n",
    "                    tf_sample,\n",
    "                    feed_dict={context: args.batch_size * [context_tokens]})\n",
    "                for i in range(min(args.sample_num - index, args.batch_size)):\n",
    "                    text = enc.decode(out[i])\n",
    "                    text = '======== SAMPLE {} ========\\n{}\\n'.format(\n",
    "                        index + 1, text)\n",
    "                    all_text.append(text)\n",
    "                    index += 1\n",
    "            print(text)\n",
    "            maketree(os.path.join(SAMPLE_DIR, args.run_name))\n",
    "            with open(\n",
    "                    os.path.join(SAMPLE_DIR, args.run_name,\n",
    "                                 'samples-{}').format(counter), 'w') as fp:\n",
    "                fp.write('\\n'.join(all_text))\n",
    "\n",
    "        def validation():\n",
    "            print('Calculating validation loss...')\n",
    "            losses = []\n",
    "            for batch in tqdm.tqdm(val_batches):\n",
    "                losses.append(sess.run(val_loss, feed_dict={val_context: batch}))\n",
    "            v_val_loss = np.mean(losses)\n",
    "            v_summary = sess.run(val_loss_summary, feed_dict={val_loss: v_val_loss})\n",
    "            summary_log.add_summary(v_summary, counter)\n",
    "            summary_log.flush()\n",
    "            print(\n",
    "                '[{counter} | {time:2.2f}] validation loss = {loss:2.2f}'\n",
    "                .format(\n",
    "                    counter=counter,\n",
    "                    time=time.time() - start_time,\n",
    "                    loss=v_val_loss))\n",
    "\n",
    "        def sample_batch():\n",
    "            return [data_sampler.sample(128) for _ in range(args.batch_size)]\n",
    "\n",
    "\n",
    "        avg_loss = (0.0, 0.0)\n",
    "        start_time = time.time()\n",
    "\n",
    "        try:\n",
    "            while True:\n",
    "                if counter % args.save_every == 0:\n",
    "                    save()\n",
    "                if counter % args.sample_every == 0:\n",
    "                    generate_samples()\n",
    "                if args.val_every > 0 and (counter % args.val_every == 0 or counter == 1):\n",
    "                    validation()\n",
    "\n",
    "                (_, v_loss, v_summary) = sess.run(\n",
    "                    (opt_apply, loss, summaries),\n",
    "                    feed_dict={context: sample_batch()})\n",
    "\n",
    "                summary_log.add_summary(v_summary, counter)\n",
    "\n",
    "                avg_loss = (avg_loss[0] * 0.99 + v_loss,\n",
    "                            avg_loss[1] * 0.99 + 1.0)\n",
    "\n",
    "                print(\n",
    "                    '[{counter} | {time:2.2f}] loss={loss:2.2f} avg={avg:2.2f}'\n",
    "                    .format(\n",
    "                        counter=counter,\n",
    "                        time=time.time() - start_time,\n",
    "                        loss=v_loss,\n",
    "                        avg=avg_loss[0] / avg_loss[1]))\n",
    "\n",
    "                counter += 1\n",
    "        except KeyboardInterrupt:\n",
    "            print('interrupted')\n",
    "            save()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14104), started 0:38:40 ago. (Use '!kill 14104' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:6006\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f15b46a1da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir checkpoint/run1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   31,  2398,    13, 16469, 30604,    13, 12384,    13, 21653,\n",
       "          13,  1236, 14221,    13, 18453,    44,  5912,     7,  8367,\n",
       "         796, 12813,  1177, 17752,  4943,   198, 11377,  8745,    13,\n",
       "       16469, 30604,    13,  4023,    13, 31077, 32398, 47934,    29,\n",
       "        1570,    41,  1559,  3419,  1391,   198,   220,   220,   220,\n",
       "         401,    13, 28864,  2419,    13,  5420,  1324,    13, 27830,\n",
       "          13, 16922,    62, 36479,  1095,  6631,    62, 36479,  1095,\n",
       "         796,   649,   401,    13, 28864,  2419,    13,  5420,  1324,\n",
       "          13, 27830,    13, 16922,    62, 36479,  1095,  9783,   198,\n",
       "         220,   220,   220,  6631,    62, 36479,  1095,    13,  2617,\n",
       "       41972, 10430,     7,  3605, 20129,    13, 22602,    13, 10430,\n",
       "       35430,   198,   220,   220,   220,  6631,    62, 36479,  1095,\n",
       "          13,  2617,  5956, 17354, 10430,     7,  3605, 20129,    13,\n",
       "       22602,    13, 10430, 35430,   198,   220,   220,   220,  6631,\n",
       "          62, 36479,  1095,    13,  2617, 10669,     7, 12355,    13,\n",
       "       22602,    13,    52, 27586,    13, 25120,    52, 27586, 22446,\n",
       "        1462, 10100, 35430,   198,   220,   220,   220,  6631,    62,\n",
       "       36479,  1095,    13,  2617, 12837,  7203, 26198,  1892,  4062,\n",
       "       15341,   198,   220,   220,   220,  1441,   649,  8745,    13,\n",
       "       16469, 30604,    13,  4023,    13, 31077, 32398,    27,   785,\n",
       "          13, 28864,  2419,    13,  5420,  1324,    13, 27830,    13,\n",
       "       16922,    62, 36479,  1095, 33994,  1069,  4516, 16177,    13,\n",
       "        4598, 22210,     7,  1069,  4516,    62, 36479,  1095,   828,\n",
       "        8745,    13, 16469, 30604,    13,  4023,    13, 43481, 19580,\n",
       "          13, 11380,  1776,   198,    92])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
