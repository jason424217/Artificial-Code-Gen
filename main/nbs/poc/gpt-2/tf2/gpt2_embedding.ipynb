{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/prototypes/gpt-2/tf2/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9"
   },
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from statistics import median\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "#         print(h.shape)\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "        results['embedding'] = h\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, past=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "            'embedding': lm_output['embedding']\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output, embedding):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "#         print(\"Logits:\", next_outputs['logits'].shape)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "#         print(\"Logits:\", logits.shape)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1),\n",
    "            next_outputs['embedding'],\n",
    "            next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        ]\n",
    "\n",
    "    past, prev, output, h, logits = body(past, context, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "#     _, _, tokens, embedding = tf.while_loop(\n",
    "#         cond=cond, body=body,\n",
    "#         maximum_iterations=length - 1,\n",
    "#         loop_vars=[\n",
    "#             past,\n",
    "#             prev,\n",
    "#             output,\n",
    "#             embedding\n",
    "#         ],\n",
    "#         shape_invariants=[\n",
    "#             tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([None, None, 768]),\n",
    "#         ],\n",
    "#         back_prop=False,\n",
    "#     )\n",
    "\n",
    "    return output, past, h, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "                \n",
    "            if i == 10000:\n",
    "                print(\"Breaking\")\n",
    "                break\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "#         if 'after.java' not in path:\n",
    "#             continue\n",
    "\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "#             if len(raw_text) > 35000: continue\n",
    "            tokens = raw_text# np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#         if i >= 500000:\n",
    "#             break\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every, pretrained, iterations):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every\n",
    "        self.pretrained = pretrained\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                dataset=\"../methods/DATA00M_[god-r]\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1, # DO NOT TOUCH. INCREASING THIS WILL RAIN DOWN HELL FIRE ONTO YOUR COMPUTER.\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=1,\n",
    "                top_p=0.0,\n",
    "                run_name=\"run1\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=None,\n",
    "                val_batch_size=1,\n",
    "                val_batch_count=40,\n",
    "                val_every=100,\n",
    "                pretrained=True,\n",
    "                iterations=200000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1215964/1215964 [01:11<00:00, 16944.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1215964"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "data_set = load_dataset(enc, args.dataset, args.combine)\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_SIZE = len(data_set)\n",
    "TRN_SET_SIZE = int(DATA_SET_SIZE * 0.8)\n",
    "VAL_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "TST_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "\n",
    "trn_set = data_set[:TRN_SET_SIZE]\n",
    "val_set = data_set[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "tst_set = data_set[-TST_SET_SIZE:]\n",
    "len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervised Pre-Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1215964 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 1/1215964 [00:00<258:17:44,  1.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 3/1215964 [00:01<204:38:02,  1.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 4/1215964 [00:01<172:19:36,  1.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 7/1215964 [00:01<125:01:10,  2.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 8/1215964 [00:01<100:13:38,  3.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 10/1215964 [00:01<77:11:25,  4.38it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 13/1215964 [00:02<58:24:41,  5.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 15/1215964 [00:02<71:24:32,  4.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 17/1215964 [00:02<61:27:01,  5.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 20/1215964 [00:03<47:32:56,  7.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 22/1215964 [00:03<47:01:09,  7.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 24/1215964 [00:04<94:46:34,  3.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 25/1215964 [00:04<88:09:23,  3.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 28/1215964 [00:04<66:05:28,  5.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 31/1215964 [00:04<50:40:51,  6.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 33/1215964 [00:05<50:56:21,  6.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 35/1215964 [00:05<44:58:00,  7.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 37/1215964 [00:05<58:21:17,  5.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 40/1215964 [00:06<60:30:34,  5.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 42/1215964 [00:06<49:05:24,  6.88it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 45/1215964 [00:06<38:52:45,  8.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 48/1215964 [00:06<31:11:51, 10.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 50/1215964 [00:07<28:45:11, 11.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 52/1215964 [00:07<26:49:54, 12.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 54/1215964 [00:08<62:05:06,  5.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 56/1215964 [00:08<55:27:56,  6.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 58/1215964 [00:08<45:23:21,  7.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 61/1215964 [00:08<39:27:36,  8.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 63/1215964 [00:09<54:55:22,  6.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 64/1215964 [00:09<51:56:47,  6.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 65/1215964 [00:09<53:48:41,  6.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 66/1215964 [00:09<50:15:56,  6.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 68/1215964 [00:09<41:13:22,  8.19it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 70/1215964 [00:09<37:10:50,  9.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 72/1215964 [00:10<35:21:21,  9.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 74/1215964 [00:10<31:56:51, 10.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 76/1215964 [00:10<28:47:26, 11.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 78/1215964 [00:10<45:34:19,  7.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 80/1215964 [00:11<38:49:32,  8.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 83/1215964 [00:11<50:09:02,  6.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 86/1215964 [00:11<40:57:40,  8.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 88/1215964 [00:12<35:09:14,  9.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 91/1215964 [00:12<36:13:42,  9.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 93/1215964 [00:12<34:32:01,  9.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 95/1215964 [00:13<64:28:25,  5.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 98/1215964 [00:13<49:45:11,  6.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 101/1215964 [00:14<58:24:23,  5.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 104/1215964 [00:14<46:49:23,  7.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 107/1215964 [00:14<37:25:35,  9.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 109/1215964 [00:14<49:00:26,  6.89it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 112/1215964 [00:15<39:07:04,  8.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 114/1215964 [00:15<34:24:50,  9.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 116/1215964 [00:15<35:37:22,  9.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 119/1215964 [00:15<29:07:02, 11.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 122/1215964 [00:15<25:04:53, 13.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 125/1215964 [00:15<23:45:56, 14.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 127/1215964 [00:16<23:15:53, 14.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 129/1215964 [00:16<23:03:24, 14.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 131/1215964 [00:16<22:49:17, 14.80it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 133/1215964 [00:16<41:38:37,  8.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 135/1215964 [00:17<66:07:53,  5.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 138/1215964 [00:17<50:26:24,  6.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 141/1215964 [00:17<39:55:47,  8.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 143/1215964 [00:17<34:49:42,  9.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 145/1215964 [00:18<40:04:20,  8.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 147/1215964 [00:18<50:59:06,  6.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 149/1215964 [00:18<44:37:52,  7.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 151/1215964 [00:18<37:30:59,  9.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 154/1215964 [00:19<35:22:01,  9.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 156/1215964 [00:19<38:18:13,  8.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 159/1215964 [00:19<31:14:01, 10.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 161/1215964 [00:19<28:52:47, 11.69it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 164/1215964 [00:19<26:00:14, 12.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 166/1215964 [00:20<24:36:45, 13.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 169/1215964 [00:20<21:53:54, 15.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 171/1215964 [00:20<22:14:51, 15.18it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 173/1215964 [00:20<22:38:18, 14.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 175/1215964 [00:20<27:17:40, 12.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 178/1215964 [00:20<24:03:07, 14.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 180/1215964 [00:21<25:15:44, 13.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 183/1215964 [00:21<22:19:36, 15.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 186/1215964 [00:21<20:14:28, 16.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 188/1215964 [00:21<22:54:00, 14.75it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 190/1215964 [00:21<24:43:09, 13.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 192/1215964 [00:21<24:05:40, 14.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 195/1215964 [00:21<21:10:33, 15.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 198/1215964 [00:22<19:43:02, 17.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 200/1215964 [00:22<31:47:22, 10.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 202/1215964 [00:22<28:08:32, 12.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 205/1215964 [00:22<24:06:25, 14.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 208/1215964 [00:22<22:34:42, 14.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 210/1215964 [00:23<24:41:37, 13.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 213/1215964 [00:23<23:28:24, 14.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 215/1215964 [00:23<32:25:37, 10.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 218/1215964 [00:23<27:07:50, 12.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "(1, 141, 768)\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n",
      "H for Chunk: (1, 141, 768) 334 (1, 12, 2, 12, 141, 64) 141\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-ef2add2472b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     models_dir='checkpoint', ds=data_set)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-ef2add2472b9>\u001b[0m in \u001b[0;36minteract_model\u001b[0;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, models_dir, ds)\u001b[0m\n\u001b[1;32m     67\u001b[0m                         \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcontext_tokens\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mpast\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                     }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;31m#                 print(p.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"H for Chunk:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "MAX_CHUNK = 1024\n",
    "\n",
    "def interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',\n",
    "    ds=[]\n",
    "):\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = get_encoder(\"117M\", \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        output, past, hidden, logits = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            past=None,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "        \n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "#         tf.compat.v1.global_variables_initializer()\n",
    "#         init = tf.compat.v1.global_variables_initializer()\n",
    "#         sess.run(init)\n",
    "        \n",
    "        feature_dict = {}\n",
    "#         h = h5py.File('features.hdf5')\n",
    "#         for k, v in d.items():\n",
    "#             h.create_dataset(k.strftime('%Y-%m-%dT%H:%M:%SZ'), data=np.array(v, dtype=np.int8))\n",
    "#         p = np.empty(0)\n",
    "        p = None\n",
    "        for method in tqdm.tqdm(ds):\n",
    "            all_logs = []\n",
    "            for i in range(math.ceil(len(method) / MAX_CHUNK)):\n",
    "                raw_text = method[i * MAX_CHUNK : (i + 1) * MAX_CHUNK]\n",
    "                context_tokens = enc.encode(raw_text)\n",
    "                # output, past, h, logits\n",
    "                if p is None:\n",
    "                    out, p, h, logs = sess.run([output, past, hidden, logits], feed_dict={\n",
    "                        context: [context_tokens for _ in range(batch_size)]\n",
    "        #                     past: None\n",
    "                    }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "                else:\n",
    "                    out, p, h, logs = sess.run([output, past, hidden, logits], feed_dict={\n",
    "                        context: [context_tokens for _ in range(batch_size)],\n",
    "                        past: p\n",
    "                    }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "#                 print(p.shape)\n",
    "                print(\"H for Chunk:\", h.shape, len(raw_text), p.shape, len(context_tokens))\n",
    "                all_logs.append(logs)\n",
    "            features = np.squeeze(np.sum(np.array(all_logs), axis = 0))\n",
    "#                 h.create_dataset(raw_text, data=logs)\n",
    "            feature_dict[method] = features\n",
    "            p = None\n",
    "            print(h.shape)\n",
    "#         print(feature_dict)\n",
    "        with open('features.pickle', 'wb') as f:\n",
    "            pickle.dump(feature_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "interact_model(model_name='run1',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    models_dir='checkpoint', ds=data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set[100]) % 1024\n",
    "# assert data_set[4][:] == data_set[4][:3000]#, data_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pickle', 'rb') as f:\n",
    "    b = pickle.load(f)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File('features.hdf5')\n",
    "for ds in h.keys():\n",
    "#     if '2012-04' in ds:\n",
    "    print(\"Test\", h[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
