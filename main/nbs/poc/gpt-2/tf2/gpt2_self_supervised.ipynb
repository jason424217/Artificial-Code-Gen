{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/prototypes/gpt-2/tf2/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
      "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 609kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 14.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Collecting urllib3<1.25,>=1.21.1 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 1.3MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "Installing collected packages: fire, regex, urllib3, chardet, certifi, requests, tqdm, toposort\n",
      "Successfully installed certifi-2019.6.16 chardet-3.0.4 fire-0.1.3 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1 urllib3-1.24.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from statistics import median\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "#         print(h.shape)\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "#         results['embedding'] = h\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents\n",
    "#             'embedding': lm_output['embedding']\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output, embedding):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "#         print(\"Logits:\", next_outputs['logits'].shape)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "#         print(\"Logits:\", logits.shape)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1),\n",
    "#             next_outputs['embedding'],\n",
    "            next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        ]\n",
    "\n",
    "    past, prev, output, logits = body(None, context, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "#     _, _, tokens, embedding = tf.while_loop(\n",
    "#         cond=cond, body=body,\n",
    "#         maximum_iterations=length - 1,\n",
    "#         loop_vars=[\n",
    "#             past,\n",
    "#             prev,\n",
    "#             output,\n",
    "#             embedding\n",
    "#         ],\n",
    "#         shape_invariants=[\n",
    "#             tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([None, None, 768]),\n",
    "#         ],\n",
    "#         back_prop=False,\n",
    "#     )\n",
    "\n",
    "    return output, palogits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            if i % 10000 == 0:\n",
    "                print(i)\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "                \n",
    "#             if i == 100000:\n",
    "#                 print(\"Breaking\")\n",
    "#                 break\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "#         if 'after.java' not in path:\n",
    "#             continue\n",
    "\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "#             if len(raw_text) > 35000: continue\n",
    "            tokens = raw_text# np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#         if i >= 500000:\n",
    "#             break\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every, pretrained, iterations):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every\n",
    "        self.pretrained = pretrained\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                dataset=\"../methods/DATA00M_[god-r]\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1, # DO NOT TOUCH. INCREASING THIS WILL RAIN DOWN HELL FIRE ONTO YOUR COMPUTER.\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=1,\n",
    "                top_p=0.0,\n",
    "                run_name=\"run1\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=None,\n",
    "                val_batch_size=1,\n",
    "                val_batch_count=40,\n",
    "                val_every=100,\n",
    "                pretrained=True,\n",
    "                iterations=200000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 34/22603 [00:00<01:07, 332.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "Breaking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22603/22603 [00:37<00:00, 603.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5706"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "data_set = load_dataset(enc, args.dataset, args.combine)\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4564, 570, 570)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_SET_SIZE = len(data_set)\n",
    "TRN_SET_SIZE = int(DATA_SET_SIZE * 0.8)\n",
    "VAL_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "TST_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "\n",
    "trn_set = data_set[:TRN_SET_SIZE]\n",
    "val_set = data_set[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "tst_set = data_set[-TST_SET_SIZE:]\n",
    "len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervised Pre-Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5706/5706 [09:35<00:00, 13.02it/s]  \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MAX_CHUNK = 1024\n",
    "\n",
    "def interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',\n",
    "    ds=[]\n",
    "):\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = get_encoder(\"117M\", \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        output, logits = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "        \n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "#         tf.compat.v1.global_variables_initializer()\n",
    "#         init = tf.compat.v1.global_variables_initializer()\n",
    "#         sess.run(init)\n",
    "        \n",
    "        feature_dict = {}\n",
    "#         h = h5py.File('features.hdf5')\n",
    "#         for k, v in d.items():\n",
    "#             h.create_dataset(k.strftime('%Y-%m-%dT%H:%M:%SZ'), data=np.array(v, dtype=np.int8))\n",
    "        for method in tqdm.tqdm(ds):\n",
    "            all_logs = []\n",
    "            for i in range(math.ceil(len(method) / MAX_CHUNK)):\n",
    "                raw_text = method[i * MAX_CHUNK : (i + 1) * MAX_CHUNK]\n",
    "                context_tokens = enc.encode(raw_text)\n",
    "                out, logs = sess.run([output, logits], feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "                all_logs.append(logs)\n",
    "            features = np.squeeze(np.sum(np.array(all_logs), axis = 0))\n",
    "#                 h.create_dataset(raw_text, data=logs)\n",
    "            feature_dict[method] = features\n",
    "#         print(feature_dict)\n",
    "        with open('features.pickle', 'wb') as f:\n",
    "            pickle.dump(feature_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "interact_model(model_name='run1',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    models_dir='checkpoint', ds=data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set[4])\n",
    "assert data_set[4][:] == data_set[4][:3000]#, data_set[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pickle', 'rb') as f:\n",
    "    b = pickle.load(f)\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test <HDF5 group \"/@java.lang.Override\n",
      "public android.view.View onCreateView(android.view.LayoutInflater inflater, android.view.ViewGroup container, android.os.Bundle savedInstanceState) {\n",
      "    android.view.View rootView = inflater.inflate(R.layout.fragment_details, container, false);\n",
      "    posterView = ((android.widget.ImageView) (rootView.findViewById(R.id.details_poster)));\n",
      "    titleView = ((android.widget.TextView) (rootView.findViewById(R.id.title)));\n",
      "    releaseDateView = ((android.widget.TextView) (rootView.findViewById(R.id.release_date)));\n",
      "    voteView = ((android.widget.TextView) (rootView.findViewById(R.id.vote)));\n",
      "    plotView = ((android.widget.TextView) (rootView.findViewById(R.id.plot)));\n",
      "    genresView = ((android.widget.TextView) (rootView.findViewById(R.id.genres)));\n",
      "    length = ((android.widget.TextView) (rootView.findViewById(R.id.length)));\n",
      "    budget = ((android.widget.TextView) (rootView.findViewById(R.id.budget)));\n",
      "    actors = ((android.widget.TextView) (rootView.findViewById(R.id.actors)));\n",
      "    director = ((android.widget.TextView) (rootView.findViewById(R.id.director)));\n",
      "    android.widget.ExpandableListView reviews = ((android.widget.ExpandableListView) (rootView.findViewById(R.id.reviews)));\n",
      "    context = getActivity();\n",
      "    back = ((android.widget.ImageView) (rootView.findViewById(R.id.backdrop)));\n",
      "    progressBar = rootView.findViewById(R.id.progressBar);\n",
      "    playButton = ((android.widget.ImageButton) (rootView.findViewById(R.id.play_button)));\n",
      "    card = ((android.support.v7.widget.CardView) (rootView.findViewById(R.id.card_view)));\n",
      "    int fragmentHeight;\n",
      "    if (!(com.example.igorklimov.popularmoviesdemo.helpers.Utility.isTabletPreference(context))) {\n",
      "        fragmentHeight = this.getResources().getDisplayMetrics().heightPixels;\n",
      "        fragmentWidth = this.getResources().getDisplayMetrics().widthPixels;\n",
      "    }else {\n",
      "        android.os.Bundle arguments = getArguments();\n",
      "        fragmentHeight = arguments.getInt(\"fragmentHeight\");\n",
      "        fragmentWidth = arguments.getInt(\"fragmentWidth\");\n",
      "    }\n",
      "    minHeight = fragmentHeight \" (1 members)>\n",
      "Test <HDF5 group \"/@org.springframework.web.bind.annotation.RequestMapping(value = \"\" (1 members)>\n",
      "Test <HDF5 dataset \"private static android.support.design.widget.AppBarLayout findFirstAppBarLayout(java.util.List<android.view.View> views) {\n",
      "    int i = 0;\n",
      "    for (int z = views.size(); i < z; ++i) {\n",
      "        android.view.View view = views.get(i);\n",
      "        if (view instanceof android.support.design.widget.AppBarLayout) {\n",
      "            return ((android.support.design.widget.AppBarLayout) (view));\n",
      "        }\n",
      "    }\n",
      "    return null;\n",
      "}\": shape (1, 50257), type \"<f4\">\n",
      "Test <HDF5 dataset \"private void setListViewHeight(android.widget.ExpandableListView listView, int group) {\n",
      "    android.widget.ExpandableListAdapter listAdapter = listView.getExpandableListAdapter();\n",
      "    if ((defaultHeight) == 0)\n",
      "        defaultHeight = listView.getHeight();\n",
      "    \n",
      "    int totalHeight = 0;\n",
      "    int desiredWidth = View.MeasureSpec.makeMeasureSpec(listView.getWidth(), View.MeasureSpec.EXACTLY);\n",
      "    for (int i = 0; i < (listAdapter.getGroupCount()); i++) {\n",
      "        if (((listView.isGroupExpanded(i)) && (i != group)) || ((!(listView.isGroupExpanded(i))) && (i == group))) {\n",
      "            for (int j = 0; j < (listAdapter.getChildrenCount(i)); j++) {\n",
      "                android.view.View listItem = listAdapter.getChildView(i, j, false, null, listView);\n",
      "                listItem.measure(desiredWidth, View.MeasureSpec.UNSPECIFIED);\n",
      "                totalHeight += listItem.getMeasuredHeight();\n",
      "            }\n",
      "            totalHeight += defaultHeight;\n",
      "        }\n",
      "    }\n",
      "    android.view.ViewGroup.LayoutParams params = listView.getLayoutParams();\n",
      "    int height = totalHeight + ((listView.getDividerHeight()) * ((listAdapter.getGroupCount()) - 1));\n",
      "    if (height < (defaultHeight))\n",
      "        height = defaultHeight;\n",
      "    \n",
      "    params.height = height;\n",
      "    listView.setLayoutParams(params);\n",
      "    listView.requestLayout();\n",
      "}\": shape (1, 50257), type \"<f4\">\n",
      "Test <HDF5 dataset \"public boolean onMeasureChild(android.support.design.widget.CoordinatorLayout parent, android.view.View child, int parentWidthMeasureSpec, int widthUsed, int parentHeightMeasureSpec, int heightUsed) {\n",
      "    if ((child.getLayoutParams().height) == (-1)) {\n",
      "        java.util.List dependencies = parent.getDependencies(child);\n",
      "        if (dependencies.isEmpty()) {\n",
      "            return false;\n",
      "        }\n",
      "        android.support.design.widget.AppBarLayout appBar = com.example.igorklimov.popularmoviesdemo.helpers.FixedScrollingViewBehavior.findFirstAppBarLayout(dependencies);\n",
      "        if ((appBar != null) && (android.support.v4.view.ViewCompat.isLaidOut(appBar))) {\n",
      "            if (android.support.v4.view.ViewCompat.getFitsSystemWindows(appBar)) {\n",
      "                android.support.v4.view.ViewCompat.setFitsSystemWindows(child, true);\n",
      "            }\n",
      "            int scrollRange = appBar.getTotalScrollRange();\n",
      "            int height = ((parent.getHeight()) - (appBar.getMeasuredHeight())) + (java.lang.Math.min(scrollRange, ((parent.getHeight()) - heightUsed)));\n",
      "            int heightMeasureSpec = View.MeasureSpec.makeMeasureSpec(height, View.MeasureSpec.EXACTLY);\n",
      "            parent.onMeasureChild(child, parentWidthMeasureSpec, widthUsed, heightMeasureSpec, heightUsed);\n",
      "            return true;\n",
      "        }\n",
      "    }\n",
      "    return false;\n",
      "}\": shape (1, 50257), type \"<f4\">\n",
      "Test <HDF5 dataset \"public void getBytes(java.io.OutputStream outputStream) throws java.io.IOException {\n",
      "    java.io.BufferedOutputStream bufferedOutputStream = new java.io.BufferedOutputStream(outputStream);\n",
      "    java.nio.ByteBuffer byteBuffer = java.nio.ByteBuffer.allocate(((java.lang.Long.BYTES) * 2));\n",
      "    byteBuffer.putLong(id.getMostSignificantBits());\n",
      "    byteBuffer.putLong(id.getLeastSignificantBits());\n",
      "    bufferedOutputStream.write(byteBuffer.array());\n",
      "    byteBuffer = java.nio.ByteBuffer.allocate(java.lang.Integer.BYTES);\n",
      "    byteBuffer.putInt(tracklet.locationSequence.length);\n",
      "    bufferedOutputStream.write(byteBuffer.array());\n",
      "    for (org.casia.cripac.isee.pedestrian.tracking.Tracklet.BoundingBox box : tracklet.locationSequence) {\n",
      "        byteBuffer = java.nio.ByteBuffer.allocate(((java.lang.Integer.BYTES) * 4));\n",
      "        byteBuffer.putInt(box.x);\n",
      "        byteBuffer.putInt(box.y);\n",
      "        byteBuffer.putInt(box.width);\n",
      "        byteBuffer.putInt(box.height);\n",
      "        bufferedOutputStream.write(byteBuffer.array());\n",
      "        bufferedOutputStream.write(box.patchData);\n",
      "    }\n",
      "    bufferedOutputStream.flush();\n",
      "}\": shape (1, 50257), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "h = h5py.File('features.hdf5')\n",
    "for ds in h.keys():\n",
    "#     if '2012-04' in ds:\n",
    "    print(\"Test\", h[ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
