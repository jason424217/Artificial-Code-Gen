{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iLXW02eIYpcB"
   },
   "source": [
    "clone and cd into repo, nshepperd's fork https://github.com/nshepperd/gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2753,
     "status": "ok",
     "timestamp": 1559179823276,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "ICYu3w9hIJkC",
    "outputId": "2087d87c-0b0e-4e06-dd32-2ca79e13330f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into './data/gpt-2'...\n",
      "remote: Enumerating objects: 301, done.\u001b[K\n",
      "remote: Total 301 (delta 0), reused 0 (delta 0), pack-reused 301\u001b[K\n",
      "Receiving objects: 100% (301/301), 4.40 MiB | 955.00 KiB/s, done.\n",
      "Resolving deltas: 100% (165/165), done.\n",
      "Checking connectivity... done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ncoop57/gpt-2.git ./data/gpt-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1559179823520,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "6eEIs3ApZUVO",
    "outputId": "3440ede5-4fa8-4d64-c54e-6ccc9446add4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/prototypes/gpt-2/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd data/gpt-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qtn1qZPgZLb0"
   },
   "source": [
    "Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4257,
     "status": "ok",
     "timestamp": 1559084518006,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "434oOx0bZH6J",
    "outputId": "978bac50-18f1-4d92-e266-2d02a95e25c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fire>=0.1.3 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/5a/b7/205702f348aab198baecd1d8344a90748cb68f53bdcd1cc30cbc08e47d3e/fire-0.1.3.tar.gz\n",
      "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 3.9MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 3.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.12.0)\n",
      "Collecting certifi>=2017.4.17 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/75/f692a584e85b7eaba0e03827b3d51f45f571c2e793dd731e598828d380aa/certifi-2019.3.9-py2.py3-none-any.whl (158kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting chardet<3.1.0,>=3.0.2 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.21.1 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 2.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.9,>=2.5 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 2.9MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/1a/4d/6b30377c3051e76559d1185c1dbbfff15aed31f87acdd14c22\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built fire regex\n",
      "Installing collected packages: fire, regex, certifi, chardet, urllib3, idna, requests, tqdm, toposort\n",
      "Successfully installed certifi-2019.3.9 chardet-3.0.4 fire-0.1.3 idna-2.8 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1 urllib3-1.24.3\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1hrgeKFYsuE"
   },
   "source": [
    "Download the model data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25190,
     "status": "ok",
     "timestamp": 1559179853688,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "5UDpEGjfO8Q2",
    "outputId": "73851f02-2db7-48f6-d99e-6ed034c1e0b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching checkpoint: 1.00kit [00:00, 276kit/s]                                                      \n",
      "Fetching encoder.json: 1.04Mit [00:00, 1.75Mit/s]                                                   \n",
      "Fetching hparams.json: 1.00kit [00:00, 222kit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 1.42Git [05:48, 4.07Mit/s]                                 \n",
      "Fetching model.ckpt.index: 11.0kit [00:00, 2.14Mit/s]                                               \n",
      "Fetching model.ckpt.meta: 927kit [00:00, 1.55Mit/s]                                                 \n",
      "Fetching vocab.bpe: 457kit [00:00, 1.15Mit/s]                                                       \n"
     ]
    }
   ],
   "source": [
    "!python download_model.py 345M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zq-YwRnNOBYO"
   },
   "source": [
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7oJPQtdLbbeK"
   },
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  wget\n",
      "0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.\n",
      "Need to get 299 kB of archives.\n",
      "After this operation, 905 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 wget amd64 1.17.1-1ubuntu1.5 [299 kB]\n",
      "Fetched 299 kB in 0s (1242 kB/s)\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package wget.\n",
      "(Reading database ... 15495 files and directories currently installed.)\n",
      "Preparing to unpack .../wget_1.17.1-1ubuntu1.5_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 16%]\u001b[49m\u001b[39m [##########................................................] \u001b8Unpacking wget (1.17.1-1ubuntu1.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 33%]\u001b[49m\u001b[39m [####################......................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 50%]\u001b[49m\u001b[39m [#############################.............................] \u001b8Setting up wget (1.17.1-1ubuntu1.5) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 66%]\u001b[49m\u001b[39m [#######################################...................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 83%]\u001b[49m\u001b[39m [#################################################.........] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install wget -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0p--9zwqQRTc"
   },
   "source": [
    "\n",
    "Let's get our train on! In this case the file is A Tale of Two Cities (Charles Dickens) from Project Gutenberg. To change the dataset GPT-2 models will fine-tune on, change this URL to another .txt file, and change corresponding part of the next cell. Note that you can use small datasets if you want but you will have to be sure not to run the fine-tuning for too long or you will overfit badly. Roughly, expect interesting results within minutes to hours in the 1-10s of megabyte ballpark, and below this you may want to stop the run early as fine-tuning can be very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 172427,
     "status": "ok",
     "timestamp": 1559056398480,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "QOCvrs-DHvxa",
    "outputId": "4cd80ef2-2ed0-4f55-89e5-0791ab23a726"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-09 04:09:43--  http://groups.inf.ed.ac.uk/cup/javaGithub/java_projects.tar.gz\n",
      "Resolving groups.inf.ed.ac.uk (groups.inf.ed.ac.uk)... 129.215.202.26\n",
      "Connecting to groups.inf.ed.ac.uk (groups.inf.ed.ac.uk)|129.215.202.26|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1925914209 (1.8G) [application/gzip]\n",
      "Saving to: ‘java_projects.tar.gz’\n",
      "\n",
      "java_projects.tar.g 100%[===================>]   1.79G  3.92MB/s    in 18m 45s \n",
      "\n",
      "2019-06-09 04:28:28 (1.63 MB/s) - ‘java_projects.tar.gz’ saved [1925914209/1925914209]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://groups.inf.ed.ac.uk/cup/javaGithub/java_projects.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJObRkjwO2fQ"
   },
   "outputs": [],
   "source": [
    "!tar xzf java_projects.tar.gz\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4143,
     "status": "ok",
     "timestamp": 1559078862534,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "RfP7MFOxL6Ze",
    "outputId": "4bc26adf-75c5-4920-8b9e-7ff7df553dec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toposort in /usr/local/lib/python3.6/dist-packages (1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install toposort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPfJ5b3CQXqr"
   },
   "source": [
    "\n",
    "Start training, add --model_name '345M' to use 345 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93996
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1540240,
     "status": "ok",
     "timestamp": 1559092407520,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "pEn_ihcGI00T",
    "outputId": "2e663d39-12d0-43c8-a571-52c7c41b76cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-28 23:03:05.387967: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-05-28 23:03:05.388196: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2086680 executing computations on platform Host. Devices:\n",
      "2019-05-28 23:03:05.388230: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-05-28 23:03:05.559017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-05-28 23:03:05.559523: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2085fa0 executing computations on platform CUDA. Devices:\n",
      "2019-05-28 23:03:05.559555: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2019-05-28 23:03:05.559970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
      "2019-05-28 23:03:05.560010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-05-28 23:03:06.023073: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-05-28 23:03:06.023134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-05-28 23:03:06.023146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-05-28 23:03:06.023438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Loading checkpoint checkpoint/run1/model-1335\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Loading dataset...\n",
      "  3% 4486/165815 [01:00<20:15, 132.73it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "  3% 5704/165815 [01:13<21:22, 124.87it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "  4% 6228/165815 [01:19<31:24, 84.67it/s]Skipping file due to incorrect encoding...\n",
      "  4% 6915/165815 [01:23<17:30, 151.29it/s]Skipping file due to incorrect encoding...\n",
      "  6% 10027/165815 [01:48<32:54, 78.89it/s]Skipping file due to incorrect encoding...\n",
      "  6% 10053/165815 [01:49<33:36, 77.25it/s]Skipping file due to incorrect encoding...\n",
      "  6% 10062/165815 [01:49<44:38, 58.16it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "  6% 10334/165815 [01:51<17:10, 150.86it/s]Skipping file due to incorrect encoding...\n",
      "  6% 10369/165815 [01:51<24:04, 107.58it/s]Skipping file due to incorrect encoding...\n",
      "  7% 10854/165815 [01:54<14:55, 173.00it/s]Skipping file due to incorrect encoding...\n",
      "  7% 11325/165815 [01:57<12:21, 208.37it/s]Skipping file due to incorrect encoding...\n",
      " 13% 22096/165815 [03:48<13:24, 178.72it/s]Skipping file due to incorrect encoding...\n",
      " 14% 23240/165815 [04:00<20:35, 115.39it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 15% 24902/165815 [04:16<1:04:51, 36.21it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 16% 25773/165815 [04:23<14:30, 160.95it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 17% 27554/165815 [04:41<28:56, 79.61it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 17% 27747/165815 [04:43<33:38, 68.40it/s]Skipping file due to incorrect encoding...\n",
      " 18% 29057/165815 [04:59<35:56, 63.43it/s]Skipping file due to incorrect encoding...\n",
      " 18% 29339/165815 [05:02<29:13, 77.85it/s]Skipping file due to incorrect encoding...\n",
      " 18% 29789/165815 [05:05<23:12, 97.66it/s] Skipping file due to incorrect encoding...\n",
      " 18% 29898/165815 [05:07<25:16, 89.60it/s]Skipping file due to incorrect encoding...\n",
      " 18% 29910/165815 [05:07<38:49, 58.34it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 18% 29984/165815 [05:08<42:55, 52.75it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 18% 30101/165815 [05:10<32:19, 69.99it/s]Skipping file due to incorrect encoding...\n",
      " 18% 30115/165815 [05:10<27:58, 80.84it/s]Skipping file due to incorrect encoding...\n",
      " 20% 33722/165815 [05:50<16:54, 130.19it/s]Skipping file due to incorrect encoding...\n",
      " 22% 35848/165815 [06:11<21:13, 102.08it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 24% 39455/165815 [07:08<3:30:19, 10.01it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 31% 50927/165815 [08:59<17:44, 107.90it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 31% 50948/165815 [08:59<15:39, 122.29it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 31% 51100/165815 [09:00<08:34, 222.92it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 31% 51588/165815 [09:02<14:41, 129.52it/s]Skipping file due to incorrect encoding...\n",
      " 31% 51650/165815 [09:03<12:45, 149.10it/s]Skipping file due to incorrect encoding...\n",
      " 31% 51689/165815 [09:03<13:28, 141.16it/s]Skipping file due to incorrect encoding...\n",
      " 32% 52591/165815 [09:14<18:00, 104.82it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 32% 52613/165815 [09:15<17:23, 108.48it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 32% 53798/165815 [09:28<15:57, 117.01it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 33% 54015/165815 [09:29<11:53, 156.76it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 33% 54061/165815 [09:29<09:35, 194.18it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 33% 54098/165815 [09:29<08:22, 222.32it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 38% 63170/165815 [10:54<30:09, 56.73it/s]Skipping file due to incorrect encoding...\n",
      " 38% 63189/165815 [10:54<26:43, 64.00it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 38% 63419/165815 [10:55<09:50, 173.27it/s]Skipping file due to incorrect encoding...\n",
      " 38% 63670/165815 [10:58<24:15, 70.18it/s]Skipping file due to incorrect encoding...\n",
      " 38% 63701/165815 [10:58<17:57, 94.76it/s]Skipping file due to incorrect encoding...\n",
      " 38% 63767/165815 [10:58<11:42, 145.18it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 63898/165815 [11:00<27:18, 62.20it/s] Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 64312/165815 [11:03<17:22, 97.41it/s]Skipping file due to incorrect encoding...\n",
      " 39% 64569/165815 [11:06<15:32, 108.52it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 64639/165815 [11:06<11:49, 142.69it/s]Skipping file due to incorrect encoding...\n",
      " 39% 65041/165815 [11:09<14:18, 117.36it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 65074/165815 [11:09<11:32, 145.38it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 65095/165815 [11:09<10:30, 159.62it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 39% 65127/165815 [11:10<09:02, 185.63it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 40% 65672/165815 [11:13<19:27, 85.74it/s] Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 40% 66059/165815 [11:17<17:02, 97.58it/s]Skipping file due to incorrect encoding...\n",
      " 40% 66109/165815 [11:17<22:55, 72.50it/s]Skipping file due to incorrect encoding...\n",
      " 42% 70274/165815 [11:44<08:12, 193.83it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 42% 70315/165815 [11:44<07:05, 224.18it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 43% 70750/165815 [11:47<15:49, 100.09it/s]Skipping file due to incorrect encoding...\n",
      " 43% 70789/165815 [11:48<12:11, 129.87it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 43% 70852/165815 [11:48<11:05, 142.64it/s]Skipping file due to incorrect encoding...\n",
      " 43% 70948/165815 [11:49<16:54, 93.50it/s]Skipping file due to incorrect encoding...\n",
      " 44% 72955/165815 [12:09<12:32, 123.36it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 44% 72993/165815 [12:09<10:39, 145.11it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 44% 73033/165815 [12:10<09:24, 164.34it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 45% 74943/165815 [12:21<53:24, 28.36it/s]  Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 46% 76824/165815 [12:42<21:06, 70.26it/s]Skipping file due to incorrect encoding...\n",
      " 46% 76851/165815 [12:42<24:21, 60.85it/s]Skipping file due to incorrect encoding...\n",
      " 48% 79143/165815 [13:03<19:51, 72.72it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 48% 79743/165815 [13:06<07:12, 199.18it/s]Skipping file due to incorrect encoding...\n",
      " 48% 79774/165815 [13:06<07:10, 199.68it/s]Skipping file due to incorrect encoding...\n",
      " 48% 79824/165815 [13:07<09:43, 147.38it/s]Skipping file due to incorrect encoding...\n",
      " 49% 81031/165815 [13:17<11:14, 125.72it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 50% 82813/165815 [13:27<13:34, 101.96it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 50% 83051/165815 [13:29<11:42, 117.74it/s]Skipping file due to incorrect encoding...\n",
      " 51% 84636/165815 [13:42<09:34, 141.35it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 51% 84727/165815 [13:43<16:05, 83.94it/s] Skipping file due to incorrect encoding...\n",
      " 51% 84739/165815 [13:44<19:21, 69.81it/s]Skipping file due to incorrect encoding...\n",
      " 53% 87346/165815 [14:13<09:59, 130.93it/s]Skipping file due to incorrect encoding...\n",
      " 55% 91918/165815 [14:50<07:16, 169.31it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 56% 93436/165815 [15:01<14:28, 83.30it/s] Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 57% 94833/165815 [15:15<38:44, 30.53it/s]Skipping file due to incorrect encoding...\n",
      " 57% 94883/165815 [15:16<28:26, 41.56it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 57% 95102/165815 [15:17<08:58, 131.34it/s]Skipping file due to incorrect encoding...\n",
      " 57% 95118/165815 [15:17<08:29, 138.79it/s]Skipping file due to incorrect encoding...\n",
      " 57% 95134/165815 [15:18<08:15, 142.72it/s]Skipping file due to incorrect encoding...\n",
      " 57% 95241/165815 [15:18<07:40, 153.26it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 57% 95335/165815 [15:18<06:06, 192.40it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 58% 95369/165815 [15:19<05:23, 218.08it/s]Skipping file due to incorrect encoding...\n",
      " 58% 96165/165815 [15:29<08:41, 133.46it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 64% 105534/165815 [17:20<08:48, 114.04it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 64% 105579/165815 [17:20<07:20, 136.73it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 64% 105653/165815 [17:20<05:33, 180.52it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114727/165815 [18:31<04:53, 174.15it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114747/165815 [18:31<10:17, 82.75it/s] Skipping file due to incorrect encoding...\n",
      " 69% 114762/165815 [18:32<16:44, 50.82it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114780/165815 [18:32<13:08, 64.72it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114794/165815 [18:32<12:46, 66.56it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114814/165815 [18:32<10:52, 78.18it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 69% 114855/165815 [18:33<08:38, 98.24it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 70% 115442/165815 [18:39<20:15, 41.43it/s]Skipping file due to incorrect encoding...\n",
      " 70% 115448/165815 [18:39<18:24, 45.62it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 70% 115458/165815 [18:39<15:27, 54.28it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 70% 115491/165815 [18:39<11:35, 72.31it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 70% 115506/165815 [18:39<12:15, 68.43it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 72% 119740/165815 [19:10<06:59, 109.73it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 72% 119824/165815 [19:10<04:36, 166.53it/s]Skipping file due to incorrect encoding...\n",
      " 78% 129835/165815 [20:07<01:23, 431.72it/s]Skipping file due to incorrect encoding...\n",
      " 79% 130172/165815 [20:09<05:06, 116.37it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 79% 130231/165815 [20:10<06:52, 86.21it/s] Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 79% 131386/165815 [20:18<06:04, 94.46it/s] Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 81% 133714/165815 [20:41<07:08, 74.98it/s]Skipping file due to incorrect encoding...\n",
      " 81% 134003/165815 [20:44<04:02, 131.06it/s]Skipping file due to incorrect encoding...\n",
      " 81% 134248/165815 [20:45<02:42, 194.77it/s]Skipping file due to incorrect encoding...\n",
      " 81% 134698/165815 [20:47<02:49, 183.34it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 81% 134722/165815 [20:47<03:05, 167.53it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 82% 135655/165815 [20:54<07:55, 63.36it/s]Skipping file due to incorrect encoding...\n",
      " 82% 135697/165815 [20:54<06:40, 75.21it/s]Skipping file due to incorrect encoding...\n",
      " 82% 135736/165815 [20:55<05:36, 89.40it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 84% 139740/165815 [21:29<04:10, 104.21it/s]Skipping file due to incorrect encoding...\n",
      " 84% 139779/165815 [21:29<04:08, 104.78it/s]Skipping file due to incorrect encoding...\n",
      "Skipping file due to incorrect encoding...\n",
      " 91% 150129/165815 [23:41<02:29, 104.72it/s]Skipping file due to incorrect encoding...\n",
      " 94% 155561/165815 [26:11<02:11, 78.27it/s] Skipping file due to incorrect encoding...\n",
      " 98% 163010/165815 [27:20<00:24, 112.52it/s]Skipping file due to incorrect encoding...\n",
      "100% 165815/165815 [27:47<00:00, 99.47it/s] \n",
      "dataset has 388898577 tokens\n",
      "Training...\n",
      "2019-05-28 23:31:56.686901: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "[1336 | 9.51] loss=1.67 avg=1.67\n",
      "[1337 | 11.00] loss=0.95 avg=1.31\n",
      "[1338 | 12.50] loss=0.81 avg=1.14\n",
      "[1339 | 13.99] loss=0.53 avg=0.99\n",
      "[1340 | 15.49] loss=0.63 avg=0.91\n",
      "[1341 | 16.98] loss=0.42 avg=0.83\n",
      "[1342 | 18.48] loss=1.45 avg=0.92\n",
      "[1343 | 19.99] loss=1.03 avg=0.93\n",
      "[1344 | 21.49] loss=1.30 avg=0.98\n",
      "[1345 | 23.00] loss=0.58 avg=0.93\n",
      "[1346 | 24.51] loss=1.34 avg=0.97\n",
      "[1347 | 26.02] loss=1.61 avg=1.03\n",
      "[1348 | 27.53] loss=1.12 avg=1.04\n",
      "[1349 | 29.04] loss=1.19 avg=1.05\n",
      "[1350 | 30.55] loss=1.46 avg=1.08\n",
      "[1351 | 32.07] loss=1.34 avg=1.10\n",
      "[1352 | 33.58] loss=0.61 avg=1.06\n",
      "[1353 | 35.10] loss=1.04 avg=1.06\n",
      "[1354 | 36.62] loss=1.05 avg=1.06\n",
      "[1355 | 38.15] loss=1.26 avg=1.07\n",
      "[1356 | 39.68] loss=0.84 avg=1.06\n",
      "[1357 | 41.21] loss=0.98 avg=1.06\n",
      "[1358 | 42.74] loss=0.75 avg=1.04\n",
      "[1359 | 44.27] loss=0.76 avg=1.03\n",
      "[1360 | 45.80] loss=0.74 avg=1.02\n",
      "[1361 | 47.33] loss=1.22 avg=1.02\n",
      "[1362 | 48.87] loss=0.69 avg=1.01\n",
      "[1363 | 50.41] loss=0.77 avg=1.00\n",
      "[1364 | 51.96] loss=0.46 avg=0.98\n",
      "[1365 | 53.50] loss=0.90 avg=0.98\n",
      "[1366 | 55.04] loss=1.29 avg=0.99\n",
      "[1367 | 56.58] loss=1.49 avg=1.01\n",
      "[1368 | 58.13] loss=1.34 avg=1.02\n",
      "[1369 | 59.68] loss=0.70 avg=1.01\n",
      "[1370 | 61.23] loss=1.27 avg=1.02\n",
      "[1371 | 62.79] loss=1.25 avg=1.02\n",
      "[1372 | 64.34] loss=1.58 avg=1.04\n",
      "[1373 | 65.90] loss=0.99 avg=1.04\n",
      "[1374 | 67.46] loss=1.05 avg=1.04\n",
      "[1375 | 69.01] loss=1.68 avg=1.06\n",
      "[1376 | 70.57] loss=1.34 avg=1.07\n",
      "[1377 | 72.13] loss=0.92 avg=1.06\n",
      "[1378 | 73.69] loss=1.05 avg=1.06\n",
      "[1379 | 75.25] loss=0.75 avg=1.05\n",
      "[1380 | 76.81] loss=0.97 avg=1.05\n",
      "[1381 | 78.38] loss=0.45 avg=1.04\n",
      "[1382 | 79.95] loss=0.92 avg=1.03\n",
      "[1383 | 81.52] loss=0.95 avg=1.03\n",
      "[1384 | 83.09] loss=1.19 avg=1.03\n",
      "[1385 | 84.67] loss=1.10 avg=1.04\n",
      "[1386 | 86.24] loss=0.69 avg=1.03\n",
      "[1387 | 87.82] loss=0.93 avg=1.03\n",
      "[1388 | 89.42] loss=1.36 avg=1.03\n",
      "[1389 | 91.01] loss=0.73 avg=1.03\n",
      "[1390 | 92.60] loss=0.91 avg=1.02\n",
      "[1391 | 94.19] loss=0.63 avg=1.01\n",
      "[1392 | 95.78] loss=0.81 avg=1.01\n",
      "[1393 | 97.37] loss=0.83 avg=1.01\n",
      "[1394 | 98.97] loss=0.88 avg=1.00\n",
      "[1395 | 100.56] loss=0.71 avg=1.00\n",
      "[1396 | 102.16] loss=1.22 avg=1.00\n",
      "[1397 | 103.75] loss=0.72 avg=1.00\n",
      "[1398 | 105.35] loss=1.19 avg=1.00\n",
      "[1399 | 106.95] loss=0.87 avg=1.00\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "(i.position + 1),\n",
      "                                                                                                                                                            }\n",
      "\n",
      "                                                                                                                                                                                                         };\n",
      "                    }\n",
      "\n",
      "                     {\n",
      "                                                                                     }\n",
      "\n",
      "                 }\n",
      "\n",
      "                  {\n",
      "                                                                               }\n",
      "\n",
      "            }\n",
      "\n",
      "          //                                                                                 <\n",
      "                                                                              }\n",
      "             \n",
      "\n",
      "           //           //          /*\n",
      "                                          / \\`\\`\\`\\`/\n",
      "                                       / \\`\\`\\`\\`/\n",
      "                                    // \\`\\`\\`\\`//\\`\\`\\`\\`\\`\\`\\`\\`\n",
      "                  \n",
      "\n",
      "[1400 | 133.11] loss=1.06 avg=1.00\n",
      "[1401 | 134.71] loss=0.76 avg=0.99\n",
      "[1402 | 136.32] loss=0.99 avg=0.99\n",
      "[1403 | 137.91] loss=0.57 avg=0.98\n",
      "[1404 | 139.51] loss=1.32 avg=0.99\n",
      "[1405 | 141.11] loss=0.89 avg=0.99\n",
      "[1406 | 142.71] loss=1.36 avg=1.00\n",
      "[1407 | 144.30] loss=1.11 avg=1.00\n",
      "[1408 | 145.90] loss=1.05 avg=1.00\n",
      "[1409 | 147.48] loss=1.32 avg=1.01\n",
      "[1410 | 149.07] loss=0.93 avg=1.00\n",
      "[1411 | 150.66] loss=1.31 avg=1.01\n",
      "[1412 | 152.25] loss=0.58 avg=1.00\n",
      "[1413 | 153.84] loss=0.71 avg=1.00\n",
      "[1414 | 155.42] loss=0.30 avg=0.98\n",
      "[1415 | 157.01] loss=1.35 avg=0.99\n",
      "[1416 | 158.60] loss=1.46 avg=1.00\n",
      "[1417 | 160.18] loss=0.96 avg=1.00\n",
      "[1418 | 161.77] loss=1.13 avg=1.00\n",
      "[1419 | 163.36] loss=0.89 avg=1.00\n",
      "[1420 | 164.95] loss=0.75 avg=0.99\n",
      "[1421 | 166.54] loss=1.16 avg=1.00\n",
      "[1422 | 168.13] loss=0.66 avg=0.99\n",
      "[1423 | 169.72] loss=0.25 avg=0.98\n",
      "[1424 | 171.32] loss=0.76 avg=0.98\n",
      "[1425 | 172.91] loss=1.58 avg=0.99\n",
      "[1426 | 174.50] loss=0.72 avg=0.98\n",
      "[1427 | 176.09] loss=1.37 avg=0.99\n",
      "[1428 | 177.68] loss=1.43 avg=1.00\n",
      "[1429 | 179.28] loss=1.04 avg=1.00\n",
      "[1430 | 180.87] loss=2.17 avg=1.01\n",
      "[1431 | 182.47] loss=1.05 avg=1.02\n",
      "[1432 | 184.06] loss=2.11 avg=1.03\n",
      "[1433 | 185.65] loss=0.91 avg=1.03\n",
      "[1434 | 187.24] loss=1.45 avg=1.04\n",
      "[1435 | 188.84] loss=0.87 avg=1.03\n",
      "[1436 | 190.43] loss=1.94 avg=1.05\n",
      "[1437 | 192.03] loss=1.42 avg=1.05\n",
      "[1438 | 193.63] loss=1.80 avg=1.07\n",
      "[1439 | 195.23] loss=0.82 avg=1.06\n",
      "[1440 | 196.82] loss=1.25 avg=1.07\n",
      "[1441 | 198.42] loss=1.18 avg=1.07\n",
      "[1442 | 200.02] loss=0.96 avg=1.07\n",
      "[1443 | 201.62] loss=1.11 avg=1.07\n",
      "[1444 | 203.22] loss=0.88 avg=1.06\n",
      "[1445 | 204.82] loss=0.65 avg=1.06\n",
      "[1446 | 206.42] loss=1.25 avg=1.06\n",
      "[1447 | 208.02] loss=0.94 avg=1.06\n",
      "[1448 | 209.61] loss=0.09 avg=1.04\n",
      "[1449 | 211.22] loss=0.63 avg=1.04\n",
      "[1450 | 212.82] loss=1.27 avg=1.04\n",
      "[1451 | 214.41] loss=1.78 avg=1.05\n",
      "[1452 | 216.01] loss=1.44 avg=1.06\n",
      "[1453 | 217.61] loss=0.58 avg=1.05\n",
      "[1454 | 219.21] loss=0.56 avg=1.04\n",
      "[1455 | 220.82] loss=1.13 avg=1.04\n",
      "[1456 | 222.42] loss=1.01 avg=1.04\n",
      "[1457 | 224.02] loss=0.79 avg=1.04\n",
      "[1458 | 225.61] loss=0.73 avg=1.04\n",
      "[1459 | 227.21] loss=0.90 avg=1.03\n",
      "[1460 | 228.81] loss=1.31 avg=1.04\n",
      "[1461 | 230.40] loss=0.84 avg=1.04\n",
      "[1462 | 232.00] loss=1.18 avg=1.04\n",
      "[1463 | 233.60] loss=0.91 avg=1.04\n",
      "[1464 | 235.20] loss=1.55 avg=1.04\n",
      "[1465 | 236.79] loss=0.38 avg=1.03\n",
      "[1466 | 238.39] loss=1.19 avg=1.04\n",
      "[1467 | 239.99] loss=0.63 avg=1.03\n",
      "[1468 | 241.59] loss=1.14 avg=1.03\n",
      "[1469 | 243.18] loss=1.36 avg=1.04\n",
      "[1470 | 244.78] loss=0.95 avg=1.04\n",
      "[1471 | 246.38] loss=1.02 avg=1.04\n",
      "[1472 | 247.98] loss=0.80 avg=1.03\n",
      "[1473 | 249.57] loss=1.10 avg=1.03\n",
      "[1474 | 251.17] loss=0.78 avg=1.03\n",
      "[1475 | 252.76] loss=1.70 avg=1.04\n",
      "[1476 | 254.36] loss=0.58 avg=1.03\n",
      "[1477 | 255.95] loss=1.38 avg=1.04\n",
      "[1478 | 257.55] loss=0.83 avg=1.03\n",
      "[1479 | 259.14] loss=1.08 avg=1.04\n",
      "[1480 | 260.74] loss=0.67 avg=1.03\n",
      "[1481 | 262.33] loss=0.86 avg=1.03\n",
      "[1482 | 263.92] loss=0.93 avg=1.03\n",
      "[1483 | 265.52] loss=1.08 avg=1.03\n",
      "[1484 | 267.11] loss=1.58 avg=1.03\n",
      "[1485 | 268.70] loss=0.57 avg=1.03\n",
      "[1486 | 270.30] loss=0.53 avg=1.02\n",
      "[1487 | 271.89] loss=0.88 avg=1.02\n",
      "[1488 | 273.48] loss=1.06 avg=1.02\n",
      "[1489 | 275.07] loss=0.95 avg=1.02\n",
      "[1490 | 276.66] loss=2.10 avg=1.03\n",
      "[1491 | 278.26] loss=1.13 avg=1.04\n",
      "[1492 | 279.85] loss=0.49 avg=1.03\n",
      "[1493 | 281.45] loss=1.03 avg=1.03\n",
      "[1494 | 283.04] loss=1.78 avg=1.04\n",
      "[1495 | 284.63] loss=1.06 avg=1.04\n",
      "[1496 | 286.23] loss=1.30 avg=1.04\n",
      "[1497 | 287.82] loss=0.69 avg=1.04\n",
      "[1498 | 289.41] loss=1.02 avg=1.04\n",
      "[1499 | 291.00] loss=1.54 avg=1.04\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "this method).setIs(true);\n",
      "  setResult().setDataSource(NewDataSource.getData(this));\n",
      "  }\n",
      "\n",
      "  /** The `null` method.\n",
      "  This is called by the data source if `null` is specified.\n",
      "  If true the data source will never be null. */\n",
      "  public void              `null`() {\n",
      "              // this is for if the data source is null to avoid a race condition:\n",
      "              this.initialiseDataSource();\n",
      "               // for a nullable data source\n",
      "   int dataSource = null;\n",
      "              this.initialiseDataSource(\"null\", null);\n",
      "      }\n",
      "   }\n",
      "\n",
      "  @ValidationCheck (dataSource = null)\n",
      "  public void setDataSource() {\n",
      "    SetDataSource = new NewDataSource(dataSource);\n",
      "    }\n",
      "\n",
      "  @ValidationCheck (dataSource = null)\n",
      "  public int finalString() {\n",
      "    String dataString = \"\";\n",
      "    throw new UnsupportedFormatException(\"Data source is not '\" + dataString);\n",
      "   }\n",
      "\n",
      "  @ValidationCheck (dataString = null)\n",
      "  public int finalString() {\n",
      "    String dataString2 = \"\";\n",
      "    throw new UnsupportedFormatException(\"Data source is not '\" + dataString2);\n",
      "   }\n",
      "\n",
      "  @ValidationCheck (dataString2 = null)\n",
      "  public int finalString() {\n",
      "    String dataString222 = \"\";\n",
      "    throw new UnsupportedFormatException(\"Data source is not '\" + dataString2);\n",
      "   }\n",
      "\n",
      "  @ValidationCheck (dataString222 = null)\n",
      "  public int finalString() {\n",
      "    String dataString23 = \"\";\n",
      "    throw new UnsupportedFormatException(\"Data source is not '\" + dataString222);\n",
      "   }\n",
      "\n",
      "  @ValidationCheck (dataString2222)\n",
      "  public boolean dataValidation() {\n",
      "    SetDataValidity = new bool(dataString222);\n",
      "\n",
      "    if (!testValue(dataString3)) {\n",
      "     throw new UnsupportedFormatException(\"Test not validated\");\n",
      "    }\n",
      "\n",
      "    if (dataValidation(dataString22)) {\n",
      "      // initialise the data type if it is new\n",
      "       // no need to initialize the null to avoid race condition\n",
      "       return true;\n",
      "   }\n",
      "    if (testValue(dataString3)) {\n",
      "      SetDataDataValidity.set(new bool(dataString2, dataString3));\n",
      "    }\n",
      "    if (!testValue(dataString22)) {\n",
      "      return false;\n",
      "   }\n",
      "    if (testValidation(dataString2)) {\n",
      "      dataValidation.set(new boolean(dataString2, dataString3));\n",
      "    }\n",
      "    if (testValue(dataStringF))) {\n",
      "      return false;\n",
      "   }\n",
      "    if (testValidation(dataStringF)) {\n",
      "      dataValidation.set(new boolean(dataStringF, dataString2))\n",
      "              .setName(dataStringFF).setValue(dataStringFF);\n",
      "       dataValidation.set(new boolean(dataStringF, dataString3))\n",
      "              .setName(dataStringFF).setValue(dataStringFF);\n",
      "        dataValidation.set(new boolean(dataStringF, dataString2F))\n",
      "              .setName(dataStringFF).setValue(dataStringFF);\n",
      "        dataValidation.set(new boolean(dataStringF, dataString1F))\n",
      "              .setName(dataStringFF).setValue(dataStringFF);\n",
      "        dataValidation.set(new boolean(dataStringF, dataString4F))\n",
      " \n",
      "\n",
      "[1500 | 314.45] loss=0.77 avg=1.04\n",
      "[1501 | 316.05] loss=1.33 avg=1.04\n",
      "[1502 | 317.65] loss=0.94 avg=1.04\n",
      "[1503 | 319.24] loss=1.26 avg=1.04\n",
      "[1504 | 320.83] loss=1.39 avg=1.05\n",
      "[1505 | 322.42] loss=0.63 avg=1.04\n",
      "[1506 | 324.01] loss=1.06 avg=1.04\n",
      "[1507 | 325.60] loss=0.95 avg=1.04\n",
      "[1508 | 327.19] loss=1.18 avg=1.04\n",
      "[1509 | 328.77] loss=0.89 avg=1.04\n",
      "[1510 | 330.36] loss=1.13 avg=1.04\n",
      "[1511 | 331.95] loss=1.76 avg=1.05\n",
      "[1512 | 333.54] loss=1.42 avg=1.06\n",
      "[1513 | 335.13] loss=1.39 avg=1.06\n",
      "[1514 | 336.73] loss=1.11 avg=1.06\n",
      "[1515 | 338.32] loss=0.95 avg=1.06\n",
      "[1516 | 339.91] loss=0.56 avg=1.05\n",
      "[1517 | 341.51] loss=0.64 avg=1.05\n",
      "[1518 | 343.09] loss=1.01 avg=1.05\n",
      "[1519 | 344.68] loss=0.76 avg=1.05\n",
      "[1520 | 346.28] loss=0.94 avg=1.04\n",
      "[1521 | 347.87] loss=1.51 avg=1.05\n",
      "[1522 | 349.47] loss=0.52 avg=1.04\n",
      "[1523 | 351.06] loss=1.12 avg=1.04\n",
      "[1524 | 352.66] loss=1.17 avg=1.05\n",
      "[1525 | 354.25] loss=1.08 avg=1.05\n",
      "[1526 | 355.85] loss=1.06 avg=1.05\n",
      "[1527 | 357.44] loss=0.62 avg=1.04\n",
      "[1528 | 359.04] loss=1.12 avg=1.04\n",
      "[1529 | 360.64] loss=0.88 avg=1.04\n",
      "[1530 | 362.23] loss=0.47 avg=1.03\n",
      "[1531 | 363.82] loss=0.51 avg=1.03\n",
      "[1532 | 365.42] loss=1.32 avg=1.03\n",
      "[1533 | 367.02] loss=1.08 avg=1.03\n",
      "[1534 | 368.61] loss=0.84 avg=1.03\n",
      "[1535 | 370.21] loss=0.91 avg=1.03\n",
      "[1536 | 371.80] loss=0.94 avg=1.03\n",
      "[1537 | 373.40] loss=0.66 avg=1.02\n",
      "[1538 | 374.99] loss=0.93 avg=1.02\n",
      "[1539 | 376.59] loss=1.25 avg=1.02\n",
      "[1540 | 378.19] loss=0.98 avg=1.02\n",
      "[1541 | 379.78] loss=0.80 avg=1.02\n",
      "[1542 | 381.38] loss=0.55 avg=1.02\n",
      "[1543 | 382.98] loss=1.03 avg=1.02\n",
      "[1544 | 384.57] loss=1.79 avg=1.02\n",
      "[1545 | 386.17] loss=0.67 avg=1.02\n",
      "[1546 | 387.77] loss=1.16 avg=1.02\n",
      "[1547 | 389.37] loss=1.03 avg=1.02\n",
      "[1548 | 390.97] loss=1.13 avg=1.02\n",
      "[1549 | 392.56] loss=1.41 avg=1.03\n",
      "[1550 | 394.16] loss=1.29 avg=1.03\n",
      "[1551 | 395.76] loss=1.94 avg=1.04\n",
      "[1552 | 397.35] loss=1.13 avg=1.04\n",
      "[1553 | 398.95] loss=0.13 avg=1.03\n",
      "[1554 | 400.55] loss=0.54 avg=1.03\n",
      "[1555 | 402.14] loss=1.10 avg=1.03\n",
      "[1556 | 403.74] loss=1.53 avg=1.03\n",
      "[1557 | 405.34] loss=1.69 avg=1.04\n",
      "[1558 | 406.94] loss=0.91 avg=1.04\n",
      "[1559 | 408.54] loss=1.37 avg=1.04\n",
      "[1560 | 410.14] loss=1.11 avg=1.04\n",
      "[1561 | 411.74] loss=0.82 avg=1.04\n",
      "[1562 | 413.34] loss=1.13 avg=1.04\n",
      "[1563 | 414.94] loss=1.08 avg=1.04\n",
      "[1564 | 416.54] loss=1.48 avg=1.05\n",
      "[1565 | 418.14] loss=1.16 avg=1.05\n",
      "[1566 | 419.75] loss=1.40 avg=1.05\n",
      "[1567 | 421.35] loss=0.57 avg=1.05\n",
      "[1568 | 422.95] loss=0.94 avg=1.05\n",
      "[1569 | 424.55] loss=0.90 avg=1.04\n",
      "[1570 | 426.15] loss=1.61 avg=1.05\n",
      "[1571 | 427.74] loss=0.79 avg=1.05\n",
      "[1572 | 429.34] loss=1.20 avg=1.05\n",
      "[1573 | 430.94] loss=0.94 avg=1.05\n",
      "[1574 | 432.54] loss=0.89 avg=1.05\n",
      "[1575 | 434.14] loss=0.68 avg=1.04\n",
      "[1576 | 435.74] loss=0.87 avg=1.04\n",
      "[1577 | 437.34] loss=1.09 avg=1.04\n",
      "[1578 | 438.93] loss=1.22 avg=1.04\n",
      "[1579 | 440.53] loss=0.95 avg=1.04\n",
      "[1580 | 442.13] loss=1.22 avg=1.04\n",
      "[1581 | 443.73] loss=0.74 avg=1.04\n",
      "[1582 | 445.32] loss=0.35 avg=1.03\n",
      "[1583 | 446.92] loss=0.68 avg=1.03\n",
      "[1584 | 448.51] loss=1.26 avg=1.03\n",
      "[1585 | 450.11] loss=0.49 avg=1.03\n",
      "[1586 | 451.70] loss=0.50 avg=1.02\n",
      "[1587 | 453.30] loss=1.02 avg=1.02\n",
      "[1588 | 454.89] loss=0.65 avg=1.02\n",
      "[1589 | 456.49] loss=0.98 avg=1.02\n",
      "[1590 | 458.09] loss=1.27 avg=1.02\n",
      "[1591 | 459.69] loss=1.16 avg=1.02\n",
      "[1592 | 461.28] loss=1.35 avg=1.02\n",
      "[1593 | 462.88] loss=0.82 avg=1.02\n",
      "[1594 | 464.47] loss=1.19 avg=1.02\n",
      "[1595 | 466.07] loss=1.05 avg=1.02\n",
      "[1596 | 467.66] loss=0.62 avg=1.02\n",
      "[1597 | 469.26] loss=1.38 avg=1.02\n",
      "[1598 | 470.86] loss=0.97 avg=1.02\n",
      "[1599 | 472.46] loss=1.14 avg=1.02\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "      {\n",
      "                      getApplicationData()\n",
      "                                                                              {\n",
      "                                                                                                     }\n",
      "            }\n",
      "          \n",
      "         \n",
      "         \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        \n",
      "        }\n",
      "        \n",
      "       \n",
      "}\n",
      "<|endoftext|>/*\n",
      " * Copyright (C) 1993-2009, Red Hat\n",
      " *\n",
      " * Red Hat, Inc.\n",
      " * Red Hat Servers, Inc.\n",
      " * All rights reserved.\n",
      " *\n",
      " * Red Hat, version 2.0, Red Hat Enterprise Linux, and the \"Red Hat \"\n",
      " * Red Hat brand are registered trademarks of Red Hat, Inc. and/or its affiliates.\n",
      " *\n",
      " * Red Hat Servers is a registered trademark of Red Hat Inc.\n",
      " * and the \"Red Hat\" brand are registered trademarks of Red Hat, Inc. and/or its\n",
      " * affiliates.\n",
      " *\n",
      " * If you believe that a trademark is registered, you can obtain a copy of the Red Hat property\n",
      " * registered by the trademark owner (see the \"Contact Information\" section of this\n",
      " * document). You may also contact the Red Hat Servers, Inc. at info[at]redhat.com. You should also confirm\n",
      " * that the domain name in the registration is the correct registration for the domain name.\n",
      " *\n",
      " * Red Hat Red Hat Servers, version 2.0 is available over the Internet from \"http://redhat.com\" and was also\n",
      " * officially licensed under \"Agreement with the Intellectual Property Owners for use in the Red Hat\n",
      " * Servers® System\" in December, 2001, which is incorporated hereunder.\n",
      " *\n",
      "\n",
      "     * The Red Hat brand is a registered trademark of Red Hat, Inc. (\"redhat.com\" or the \"Company\")\n",
      " * and the \"Red Hat\" and the \"Red Hat brand are trademarks of Red Hat, Inc. \n",
      " *\n",
      " * Red Hat Servers, version 2.0 is available over the Internet from \"http://redhat.com\" and was also\n",
      " * officially licensed under \"Agreement with the Intellectual Property Owners for use in the Red Hat\n",
      " * Servers® System\" in December, 2001, which is incorporated hereunder.\n",
      " *\n",
      "\n",
      "     *\n",
      "     * Please review our \"Troubleshooting\" page if applicable, which includes additional information about\n",
      " * how to resolve problems with Red Hat Red Hat Servers 2.0.\n",
      "     *\n",
      "     */\n",
      "package com.redhat.servers.server;\n",
      "\n",
      "import com.redhat.servers.server.server.ServerManager;\n",
      "import com.redhat.servers.server.server.server.ServerResponse;\n",
      "import com.redhat.servers.server.server.server.ServerResponse.ServerResponseHandler;\n",
      "import com.redhat.servers.server.server.server.ServerResponse.ServerResponseHandlerBase;\n",
      "import com.redhat.servers.server.server.server.ServerResponse;\n",
      "import com.redhat.servers.server.server.server.ServerResponseHandler;\n",
      "import com.redhat.servers.server.server.server.ServerResponseHandlerBase;\n",
      "import com.redhat.servers.server.server.server.ServerResponseHandlerBase.ResponseResponse;\n",
      "import com.redhat.servers.server.server.server.ServerResponse.Response;\n",
      "import com.redhat.servers.server.server.server.ServerResponse.ResponseHandler\n",
      "\n",
      "[1600 | 496.51] loss=0.66 avg=1.02\n",
      "[1601 | 498.11] loss=1.34 avg=1.02\n",
      "[1602 | 499.70] loss=0.83 avg=1.02\n",
      "[1603 | 501.29] loss=1.13 avg=1.02\n",
      "[1604 | 502.88] loss=0.56 avg=1.02\n",
      "[1605 | 504.46] loss=1.32 avg=1.02\n",
      "[1606 | 506.05] loss=1.11 avg=1.02\n",
      "[1607 | 507.64] loss=0.99 avg=1.02\n",
      "[1608 | 509.22] loss=0.69 avg=1.02\n",
      "[1609 | 510.81] loss=0.70 avg=1.01\n",
      "[1610 | 512.40] loss=1.10 avg=1.02\n",
      "[1611 | 513.99] loss=1.02 avg=1.02\n",
      "[1612 | 515.58] loss=1.48 avg=1.02\n",
      "[1613 | 517.17] loss=0.93 avg=1.02\n",
      "[1614 | 518.76] loss=0.72 avg=1.02\n",
      "[1615 | 520.35] loss=2.38 avg=1.03\n",
      "[1616 | 521.95] loss=0.97 avg=1.03\n",
      "[1617 | 523.54] loss=0.75 avg=1.03\n",
      "[1618 | 525.13] loss=0.38 avg=1.02\n",
      "[1619 | 526.72] loss=1.03 avg=1.02\n",
      "[1620 | 528.32] loss=1.16 avg=1.02\n",
      "[1621 | 529.91] loss=0.52 avg=1.02\n",
      "[1622 | 531.50] loss=1.51 avg=1.02\n",
      "[1623 | 533.10] loss=1.45 avg=1.03\n",
      "[1624 | 534.69] loss=0.80 avg=1.02\n",
      "[1625 | 536.28] loss=1.12 avg=1.02\n",
      "[1626 | 537.88] loss=1.23 avg=1.03\n",
      "[1627 | 539.48] loss=0.89 avg=1.03\n",
      "[1628 | 541.08] loss=1.02 avg=1.03\n",
      "[1629 | 542.68] loss=1.23 avg=1.03\n",
      "[1630 | 544.28] loss=0.81 avg=1.03\n",
      "[1631 | 545.87] loss=0.89 avg=1.02\n",
      "[1632 | 547.47] loss=0.98 avg=1.02\n",
      "[1633 | 549.07] loss=1.13 avg=1.02\n",
      "[1634 | 550.67] loss=1.19 avg=1.03\n",
      "[1635 | 552.27] loss=0.94 avg=1.03\n",
      "[1636 | 553.87] loss=1.28 avg=1.03\n",
      "[1637 | 555.47] loss=0.74 avg=1.02\n",
      "[1638 | 557.07] loss=1.14 avg=1.03\n",
      "[1639 | 558.67] loss=0.68 avg=1.02\n",
      "[1640 | 560.27] loss=1.13 avg=1.02\n",
      "[1641 | 561.87] loss=0.88 avg=1.02\n",
      "[1642 | 563.47] loss=2.17 avg=1.03\n",
      "[1643 | 565.07] loss=1.00 avg=1.03\n",
      "[1644 | 566.67] loss=0.29 avg=1.03\n",
      "[1645 | 568.26] loss=1.13 avg=1.03\n",
      "[1646 | 569.86] loss=1.03 avg=1.03\n",
      "[1647 | 571.45] loss=0.65 avg=1.02\n",
      "[1648 | 573.05] loss=1.01 avg=1.02\n",
      "[1649 | 574.65] loss=2.81 avg=1.04\n",
      "[1650 | 576.24] loss=1.34 avg=1.04\n",
      "[1651 | 577.83] loss=1.13 avg=1.05\n",
      "[1652 | 579.43] loss=0.83 avg=1.04\n",
      "[1653 | 581.03] loss=0.61 avg=1.04\n",
      "[1654 | 582.63] loss=1.28 avg=1.04\n",
      "[1655 | 584.23] loss=0.98 avg=1.04\n",
      "[1656 | 585.82] loss=1.16 avg=1.04\n",
      "[1657 | 587.41] loss=0.61 avg=1.04\n",
      "[1658 | 589.01] loss=0.94 avg=1.04\n",
      "[1659 | 590.60] loss=1.07 avg=1.04\n",
      "[1660 | 592.19] loss=1.40 avg=1.04\n",
      "[1661 | 593.78] loss=0.87 avg=1.04\n",
      "[1662 | 595.38] loss=1.48 avg=1.04\n",
      "[1663 | 596.97] loss=1.01 avg=1.04\n",
      "[1664 | 598.57] loss=1.23 avg=1.05\n",
      "[1665 | 600.16] loss=0.72 avg=1.04\n",
      "[1666 | 601.75] loss=1.07 avg=1.04\n",
      "[1667 | 603.34] loss=0.98 avg=1.04\n",
      "[1668 | 604.93] loss=1.90 avg=1.05\n",
      "[1669 | 606.52] loss=1.20 avg=1.05\n",
      "[1670 | 608.11] loss=0.64 avg=1.05\n",
      "[1671 | 609.71] loss=0.77 avg=1.04\n",
      "[1672 | 611.30] loss=0.56 avg=1.04\n",
      "[1673 | 612.90] loss=1.33 avg=1.04\n",
      "[1674 | 614.49] loss=0.99 avg=1.04\n",
      "[1675 | 616.08] loss=0.83 avg=1.04\n",
      "[1676 | 617.67] loss=0.73 avg=1.04\n",
      "[1677 | 619.27] loss=0.90 avg=1.04\n",
      "[1678 | 620.86] loss=0.77 avg=1.03\n",
      "[1679 | 622.45] loss=1.00 avg=1.03\n",
      "[1680 | 624.03] loss=1.47 avg=1.04\n",
      "[1681 | 625.62] loss=0.21 avg=1.03\n",
      "[1682 | 627.21] loss=1.01 avg=1.03\n",
      "[1683 | 628.81] loss=0.91 avg=1.03\n",
      "[1684 | 630.40] loss=1.05 avg=1.03\n",
      "[1685 | 632.00] loss=1.05 avg=1.03\n",
      "[1686 | 633.59] loss=1.01 avg=1.03\n",
      "[1687 | 635.19] loss=1.15 avg=1.03\n",
      "[1688 | 636.78] loss=1.59 avg=1.03\n",
      "[1689 | 638.37] loss=0.48 avg=1.03\n",
      "[1690 | 639.97] loss=0.62 avg=1.02\n",
      "[1691 | 641.56] loss=1.24 avg=1.03\n",
      "[1692 | 643.16] loss=0.46 avg=1.02\n",
      "[1693 | 644.74] loss=1.15 avg=1.02\n",
      "[1694 | 646.34] loss=1.18 avg=1.02\n",
      "[1695 | 647.93] loss=1.03 avg=1.02\n",
      "[1696 | 649.52] loss=0.70 avg=1.02\n",
      "[1697 | 651.12] loss=0.58 avg=1.02\n",
      "[1698 | 652.71] loss=0.71 avg=1.01\n",
      "[1699 | 654.30] loss=1.12 avg=1.01\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n",
      "\n",
      "[1700 | 677.73] loss=0.56 avg=1.01\n",
      "[1701 | 679.32] loss=0.91 avg=1.01\n",
      "[1702 | 680.92] loss=0.32 avg=1.00\n",
      "[1703 | 682.52] loss=1.14 avg=1.00\n",
      "[1704 | 684.11] loss=1.50 avg=1.01\n",
      "[1705 | 685.71] loss=1.66 avg=1.01\n",
      "[1706 | 687.31] loss=1.31 avg=1.02\n",
      "[1707 | 688.90] loss=0.90 avg=1.02\n",
      "[1708 | 690.49] loss=0.94 avg=1.02\n",
      "[1709 | 692.08] loss=1.21 avg=1.02\n",
      "[1710 | 693.67] loss=0.38 avg=1.01\n",
      "[1711 | 695.27] loss=0.92 avg=1.01\n",
      "[1712 | 696.86] loss=1.14 avg=1.01\n",
      "[1713 | 698.46] loss=1.03 avg=1.01\n",
      "[1714 | 700.05] loss=1.39 avg=1.02\n",
      "[1715 | 701.64] loss=1.64 avg=1.02\n",
      "[1716 | 703.24] loss=1.70 avg=1.03\n",
      "[1717 | 704.82] loss=0.43 avg=1.02\n",
      "[1718 | 706.41] loss=0.58 avg=1.02\n",
      "[1719 | 708.01] loss=1.01 avg=1.02\n",
      "[1720 | 709.60] loss=0.96 avg=1.02\n",
      "[1721 | 711.20] loss=0.89 avg=1.02\n",
      "[1722 | 712.79] loss=0.58 avg=1.01\n",
      "[1723 | 714.38] loss=0.89 avg=1.01\n",
      "[1724 | 715.97] loss=1.44 avg=1.01\n",
      "[1725 | 717.56] loss=0.84 avg=1.01\n",
      "[1726 | 719.16] loss=1.09 avg=1.01\n",
      "[1727 | 720.75] loss=1.03 avg=1.01\n",
      "[1728 | 722.34] loss=1.26 avg=1.02\n",
      "[1729 | 723.94] loss=0.74 avg=1.01\n",
      "[1730 | 725.54] loss=0.82 avg=1.01\n",
      "[1731 | 727.13] loss=1.34 avg=1.02\n",
      "[1732 | 728.73] loss=1.04 avg=1.02\n",
      "[1733 | 730.32] loss=1.49 avg=1.02\n",
      "[1734 | 731.92] loss=0.79 avg=1.02\n",
      "[1735 | 733.51] loss=0.85 avg=1.02\n",
      "[1736 | 735.10] loss=0.46 avg=1.01\n",
      "[1737 | 736.69] loss=0.88 avg=1.01\n",
      "[1738 | 738.28] loss=1.14 avg=1.01\n",
      "[1739 | 739.87] loss=0.93 avg=1.01\n",
      "[1740 | 741.47] loss=0.83 avg=1.01\n",
      "[1741 | 743.07] loss=0.69 avg=1.00\n",
      "[1742 | 744.66] loss=1.09 avg=1.01\n",
      "[1743 | 746.26] loss=1.42 avg=1.01\n",
      "[1744 | 747.85] loss=1.03 avg=1.01\n",
      "[1745 | 749.44] loss=0.82 avg=1.01\n",
      "[1746 | 751.03] loss=0.28 avg=1.00\n",
      "[1747 | 752.62] loss=0.90 avg=1.00\n",
      "[1748 | 754.21] loss=1.32 avg=1.00\n",
      "[1749 | 755.80] loss=0.55 avg=1.00\n",
      "[1750 | 757.39] loss=0.83 avg=1.00\n",
      "[1751 | 758.98] loss=1.15 avg=1.00\n",
      "[1752 | 760.58] loss=0.52 avg=0.99\n",
      "[1753 | 762.18] loss=1.02 avg=0.99\n",
      "[1754 | 763.77] loss=1.04 avg=0.99\n",
      "[1755 | 765.36] loss=0.68 avg=0.99\n",
      "[1756 | 766.96] loss=1.52 avg=1.00\n",
      "[1757 | 768.56] loss=0.60 avg=0.99\n",
      "[1758 | 770.15] loss=0.97 avg=0.99\n",
      "[1759 | 771.75] loss=1.17 avg=0.99\n",
      "[1760 | 773.34] loss=0.62 avg=0.99\n",
      "[1761 | 774.93] loss=1.01 avg=0.99\n",
      "[1762 | 776.53] loss=0.44 avg=0.98\n",
      "[1763 | 778.12] loss=1.43 avg=0.99\n",
      "[1764 | 779.71] loss=1.10 avg=0.99\n",
      "[1765 | 781.30] loss=0.93 avg=0.99\n",
      "[1766 | 782.89] loss=0.80 avg=0.99\n",
      "[1767 | 784.48] loss=0.90 avg=0.99\n",
      "[1768 | 786.08] loss=2.07 avg=1.00\n",
      "[1769 | 787.67] loss=1.17 avg=1.00\n",
      "[1770 | 789.26] loss=0.35 avg=0.99\n",
      "[1771 | 790.85] loss=1.28 avg=1.00\n",
      "[1772 | 792.45] loss=1.18 avg=1.00\n",
      "[1773 | 794.05] loss=0.25 avg=0.99\n",
      "[1774 | 795.64] loss=0.28 avg=0.98\n",
      "[1775 | 797.23] loss=0.88 avg=0.98\n",
      "[1776 | 798.82] loss=0.91 avg=0.98\n",
      "[1777 | 800.41] loss=0.89 avg=0.98\n",
      "[1778 | 802.00] loss=0.55 avg=0.98\n",
      "[1779 | 803.59] loss=0.67 avg=0.97\n",
      "[1780 | 805.18] loss=1.05 avg=0.97\n",
      "[1781 | 806.78] loss=0.55 avg=0.97\n",
      "[1782 | 808.38] loss=1.03 avg=0.97\n",
      "[1783 | 809.98] loss=0.83 avg=0.97\n",
      "[1784 | 811.57] loss=0.96 avg=0.97\n",
      "[1785 | 813.16] loss=1.27 avg=0.97\n",
      "[1786 | 814.76] loss=0.87 avg=0.97\n",
      "[1787 | 816.36] loss=1.37 avg=0.97\n",
      "[1788 | 817.95] loss=0.90 avg=0.97\n",
      "[1789 | 819.55] loss=1.05 avg=0.97\n",
      "[1790 | 821.14] loss=0.94 avg=0.97\n",
      "[1791 | 822.73] loss=1.35 avg=0.98\n",
      "[1792 | 824.33] loss=1.19 avg=0.98\n",
      "[1793 | 825.92] loss=0.85 avg=0.98\n",
      "[1794 | 827.51] loss=1.20 avg=0.98\n",
      "[1795 | 829.10] loss=1.12 avg=0.98\n",
      "[1796 | 830.70] loss=1.49 avg=0.99\n",
      "[1797 | 832.29] loss=0.59 avg=0.98\n",
      "[1798 | 833.89] loss=1.28 avg=0.99\n",
      "[1799 | 835.49] loss=1.02 avg=0.99\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "    #define CURRENT_SIZE 50\n",
      "    * #define FACTOR_X_SIDE (8)/(3.15+4/5)*10\n",
      "    *  \n",
      "    * int __f0 = (unsigned long)(x) / (unsigned long)(0.4);\n",
      "\n",
      "    * \n",
      "    * const int __x_end_x = ((x - ((n_bytes_from_inputs-1) / 100) * -1)/1.0 + (n_bytes_from_outputs-1), 100);\n",
      "\n",
      "    *\n",
      "    * const int __y_end_y = ((y - ((n_bytes_from_inputs - 1) / 100) * (1.0 - 1)) / (5.3)), (0.25);\n",
      "    *\n",
      "    * int __x_x_y = ((x + (1.0 - (n_bytes_from_inputs + n_bytes_from_outputs))-1)/2.0 * (2.0 + n_bytes_from_outputs))/2.0 + (0.25 - 1);\n",
      "\n",
      "    *\n",
      "    * double __x_f2 = (int)((n_bytes_from_inputs – ((x-1)) * (n_bytes_from_outputs-1)/ (2.0 + n_bytes_from_inputs)) / 6244592 + 1.0 + 0.25 * (1.0 * (1.0 / 6244592-1) * (0.25 + 1.0)));\n",
      "\n",
      "    *\n",
      "    * \n",
      "    * #define NUL_ELEMENT 0\n",
      "    *  int __f1 = (unsigned short)(x);\n",
      "\n",
      "    * \n",
      "    * int __x_f2 = ((x + (1.0 - (n_bytes_from_inputs - 1))*(0.5))/100);\n",
      "    *\n",
      "    * \n",
      "    * // (C) GPL-2.0-NC-2.0 License\n",
      "    * \n",
      "    * #define __x_f1 0\n",
      "    * \n",
      "    * int __x_y = (((n_bytes_from_outputs-1) / 100) * n_bytes_from_inputs);\n",
      "    *\n",
      "    * #define __y_f1 (x)\n",
      "    *  \n",
      "    * \n",
      "    *\n",
      "    * \n",
      "    * double __y_f2 = __y_end_y + (__x_f2/2.0);\n",
      "    *\n",
      "    * \n",
      "    * CURRENT_SIZE = -1\n",
      "    * \n",
      "    * \n",
      "    * \n",
      "    * #define ___p_y 2 / 8 - 100\n",
      "    * \n",
      "    * \n",
      "    * \n",
      "    * \n",
      "    * \n",
      "    * \n",
      "    * \n",
      "     * \n",
      "     * \n",
      "     * \n",
      "     * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      * \n",
      "      */\n",
      "      /**\n",
      "      *  `x` is represented as a byte (i.e., a 32-bit integer).\n",
      "      *  */\n",
      "      /**\n",
      "      *  `y` is represented as a byte (i.e., a 32-bit integer), represented\n",
      "      *  // as a pair of floating-point numbers (which, in turn, is represented by two\n",
      "      *  // 1.0 and 10.0/1.0).\n",
      "\n",
      "      *  \n",
      " \n",
      "\n",
      "[1800 | 859.04] loss=1.38 avg=0.99\n",
      "[1801 | 860.62] loss=0.85 avg=0.99\n",
      "[1802 | 862.21] loss=0.51 avg=0.98\n",
      "[1803 | 863.80] loss=1.52 avg=0.99\n",
      "[1804 | 865.39] loss=1.22 avg=0.99\n",
      "[1805 | 866.98] loss=1.18 avg=0.99\n",
      "[1806 | 868.57] loss=0.69 avg=0.99\n",
      "[1807 | 870.16] loss=0.82 avg=0.99\n",
      "[1808 | 871.76] loss=0.58 avg=0.99\n",
      "[1809 | 873.35] loss=1.32 avg=0.99\n",
      "[1810 | 874.94] loss=0.92 avg=0.99\n",
      "[1811 | 876.52] loss=0.96 avg=0.99\n",
      "[1812 | 878.12] loss=1.07 avg=0.99\n",
      "[1813 | 879.71] loss=0.87 avg=0.99\n",
      "[1814 | 881.30] loss=1.10 avg=0.99\n",
      "[1815 | 882.89] loss=1.20 avg=0.99\n",
      "[1816 | 884.48] loss=1.56 avg=1.00\n",
      "[1817 | 886.07] loss=1.02 avg=1.00\n",
      "[1818 | 887.67] loss=1.10 avg=1.00\n",
      "[1819 | 889.26] loss=1.12 avg=1.00\n",
      "[1820 | 890.86] loss=1.00 avg=1.00\n",
      "[1821 | 892.46] loss=1.06 avg=1.00\n",
      "[1822 | 894.06] loss=1.41 avg=1.00\n",
      "[1823 | 895.65] loss=0.86 avg=1.00\n",
      "[1824 | 897.24] loss=1.83 avg=1.01\n",
      "[1825 | 898.84] loss=0.87 avg=1.01\n",
      "[1826 | 900.43] loss=0.70 avg=1.01\n",
      "[1827 | 902.03] loss=1.44 avg=1.01\n",
      "[1828 | 903.63] loss=1.14 avg=1.01\n",
      "[1829 | 905.23] loss=0.95 avg=1.01\n",
      "[1830 | 906.83] loss=0.69 avg=1.01\n",
      "[1831 | 908.43] loss=1.37 avg=1.01\n",
      "[1832 | 910.03] loss=0.76 avg=1.01\n",
      "[1833 | 911.63] loss=1.25 avg=1.01\n",
      "[1834 | 913.23] loss=1.16 avg=1.01\n",
      "[1835 | 914.82] loss=0.80 avg=1.01\n",
      "[1836 | 916.43] loss=1.27 avg=1.01\n",
      "[1837 | 918.02] loss=0.69 avg=1.01\n",
      "[1838 | 919.62] loss=0.67 avg=1.01\n",
      "[1839 | 921.23] loss=0.40 avg=1.00\n",
      "[1840 | 922.82] loss=0.44 avg=1.00\n",
      "[1841 | 924.43] loss=0.97 avg=0.99\n",
      "[1842 | 926.03] loss=0.85 avg=0.99\n",
      "[1843 | 927.62] loss=0.78 avg=0.99\n",
      "[1844 | 929.22] loss=0.54 avg=0.99\n",
      "[1845 | 930.81] loss=0.81 avg=0.98\n",
      "[1846 | 932.40] loss=0.95 avg=0.98\n",
      "[1847 | 933.99] loss=0.09 avg=0.98\n",
      "[1848 | 935.58] loss=1.03 avg=0.98\n",
      "[1849 | 937.17] loss=1.28 avg=0.98\n",
      "[1850 | 938.77] loss=1.11 avg=0.98\n",
      "[1851 | 940.37] loss=1.16 avg=0.98\n",
      "[1852 | 941.96] loss=1.01 avg=0.98\n",
      "[1853 | 943.56] loss=0.65 avg=0.98\n",
      "[1854 | 945.15] loss=1.20 avg=0.98\n",
      "[1855 | 946.75] loss=0.72 avg=0.98\n",
      "[1856 | 948.34] loss=1.35 avg=0.98\n",
      "[1857 | 949.93] loss=0.91 avg=0.98\n",
      "[1858 | 951.52] loss=0.80 avg=0.98\n",
      "[1859 | 953.12] loss=0.73 avg=0.98\n",
      "[1860 | 954.71] loss=1.09 avg=0.98\n",
      "[1861 | 956.30] loss=1.05 avg=0.98\n",
      "[1862 | 957.89] loss=0.58 avg=0.98\n",
      "[1863 | 959.49] loss=1.16 avg=0.98\n",
      "[1864 | 961.08] loss=0.68 avg=0.97\n",
      "[1865 | 962.68] loss=0.48 avg=0.97\n",
      "[1866 | 964.27] loss=1.03 avg=0.97\n",
      "[1867 | 965.87] loss=1.10 avg=0.97\n",
      "[1868 | 967.46] loss=1.42 avg=0.98\n",
      "[1869 | 969.06] loss=1.01 avg=0.98\n",
      "[1870 | 970.65] loss=0.85 avg=0.97\n",
      "[1871 | 972.24] loss=1.10 avg=0.98\n",
      "[1872 | 973.84] loss=1.48 avg=0.98\n",
      "[1873 | 975.43] loss=1.46 avg=0.99\n",
      "[1874 | 977.03] loss=0.71 avg=0.98\n",
      "[1875 | 978.62] loss=0.87 avg=0.98\n",
      "[1876 | 980.21] loss=1.02 avg=0.98\n",
      "[1877 | 981.81] loss=1.30 avg=0.99\n",
      "[1878 | 983.40] loss=0.74 avg=0.98\n",
      "[1879 | 985.00] loss=0.72 avg=0.98\n",
      "[1880 | 986.59] loss=0.55 avg=0.98\n",
      "[1881 | 988.19] loss=0.70 avg=0.97\n",
      "[1882 | 989.77] loss=1.29 avg=0.98\n",
      "[1883 | 991.36] loss=0.91 avg=0.98\n",
      "[1884 | 992.95] loss=0.74 avg=0.97\n",
      "[1885 | 994.54] loss=1.17 avg=0.98\n",
      "[1886 | 996.13] loss=0.61 avg=0.97\n",
      "[1887 | 997.72] loss=0.77 avg=0.97\n",
      "[1888 | 999.32] loss=1.17 avg=0.97\n",
      "[1889 | 1000.91] loss=0.80 avg=0.97\n",
      "[1890 | 1002.50] loss=1.94 avg=0.98\n",
      "[1891 | 1004.10] loss=0.96 avg=0.98\n",
      "[1892 | 1005.69] loss=0.72 avg=0.98\n",
      "[1893 | 1007.28] loss=0.79 avg=0.98\n",
      "[1894 | 1008.87] loss=1.18 avg=0.98\n",
      "[1895 | 1010.46] loss=1.21 avg=0.98\n",
      "[1896 | 1012.05] loss=1.28 avg=0.98\n",
      "[1897 | 1013.65] loss=0.80 avg=0.98\n",
      "[1898 | 1015.24] loss=1.64 avg=0.99\n",
      "[1899 | 1016.84] loss=1.06 avg=0.99\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "                     0 ) {\n",
      "                                                                                                                              }\n",
      "\n",
      "                                                                                                                           }\n",
      "\n",
      "                                                                                           }\n",
      "\n",
      "                                                                                          ,\n",
      "                                                                                                                                                                                                                                                                                                                                          ]);\n",
      "\n",
      "                                                                                                                                                                                    ;\n",
      "                                                     \n",
      "\n",
      "[1900 | 1040.71] loss=1.11 avg=0.99\n",
      "[1901 | 1042.31] loss=0.42 avg=0.98\n",
      "[1902 | 1043.90] loss=0.85 avg=0.98\n",
      "[1903 | 1045.50] loss=1.66 avg=0.99\n",
      "[1904 | 1047.08] loss=0.69 avg=0.99\n",
      "[1905 | 1048.68] loss=0.90 avg=0.99\n",
      "[1906 | 1050.27] loss=1.23 avg=0.99\n",
      "[1907 | 1051.85] loss=0.78 avg=0.99\n",
      "[1908 | 1053.44] loss=0.26 avg=0.98\n",
      "[1909 | 1055.03] loss=1.35 avg=0.98\n",
      "[1910 | 1056.62] loss=0.72 avg=0.98\n",
      "[1911 | 1058.21] loss=1.41 avg=0.98\n",
      "[1912 | 1059.79] loss=1.01 avg=0.98\n",
      "[1913 | 1061.38] loss=0.94 avg=0.98\n",
      "[1914 | 1062.97] loss=1.24 avg=0.99\n",
      "[1915 | 1064.56] loss=0.96 avg=0.99\n",
      "[1916 | 1066.15] loss=0.79 avg=0.98\n",
      "[1917 | 1067.74] loss=0.64 avg=0.98\n",
      "[1918 | 1069.32] loss=1.73 avg=0.99\n",
      "[1919 | 1070.92] loss=0.62 avg=0.98\n",
      "[1920 | 1072.50] loss=0.55 avg=0.98\n",
      "[1921 | 1074.10] loss=0.98 avg=0.98\n",
      "[1922 | 1075.69] loss=0.83 avg=0.98\n",
      "[1923 | 1077.29] loss=1.03 avg=0.98\n",
      "[1924 | 1078.88] loss=0.61 avg=0.98\n",
      "[1925 | 1080.48] loss=0.95 avg=0.98\n",
      "[1926 | 1082.08] loss=0.66 avg=0.97\n",
      "[1927 | 1083.67] loss=0.80 avg=0.97\n",
      "[1928 | 1085.27] loss=1.15 avg=0.97\n",
      "[1929 | 1086.86] loss=1.25 avg=0.97\n",
      "[1930 | 1088.47] loss=1.12 avg=0.98\n",
      "[1931 | 1090.06] loss=0.88 avg=0.98\n",
      "[1932 | 1091.66] loss=0.58 avg=0.97\n",
      "[1933 | 1093.26] loss=1.19 avg=0.97\n",
      "[1934 | 1094.87] loss=0.55 avg=0.97\n",
      "[1935 | 1096.47] loss=0.99 avg=0.97\n",
      "[1936 | 1098.07] loss=0.96 avg=0.97\n",
      "[1937 | 1099.66] loss=0.57 avg=0.97\n",
      "[1938 | 1101.26] loss=0.41 avg=0.96\n",
      "[1939 | 1102.87] loss=1.27 avg=0.96\n",
      "[1940 | 1104.47] loss=1.08 avg=0.96\n",
      "[1941 | 1106.07] loss=0.85 avg=0.96\n",
      "[1942 | 1107.66] loss=0.91 avg=0.96\n",
      "[1943 | 1109.26] loss=0.71 avg=0.96\n",
      "[1944 | 1110.86] loss=0.59 avg=0.96\n",
      "[1945 | 1112.46] loss=0.69 avg=0.95\n",
      "[1946 | 1114.06] loss=1.51 avg=0.96\n",
      "[1947 | 1115.65] loss=0.62 avg=0.96\n",
      "[1948 | 1117.25] loss=1.25 avg=0.96\n",
      "[1949 | 1118.85] loss=0.26 avg=0.95\n",
      "[1950 | 1120.44] loss=1.22 avg=0.95\n",
      "[1951 | 1122.04] loss=0.60 avg=0.95\n",
      "[1952 | 1123.63] loss=1.25 avg=0.95\n",
      "[1953 | 1125.23] loss=1.05 avg=0.95\n",
      "[1954 | 1126.82] loss=0.94 avg=0.95\n",
      "[1955 | 1128.42] loss=1.04 avg=0.96\n",
      "[1956 | 1130.01] loss=1.03 avg=0.96\n",
      "[1957 | 1131.61] loss=0.68 avg=0.95\n",
      "[1958 | 1133.20] loss=1.09 avg=0.95\n",
      "[1959 | 1134.80] loss=1.04 avg=0.96\n",
      "[1960 | 1136.39] loss=1.15 avg=0.96\n",
      "[1961 | 1137.98] loss=1.15 avg=0.96\n",
      "[1962 | 1139.58] loss=1.00 avg=0.96\n",
      "[1963 | 1141.17] loss=1.05 avg=0.96\n",
      "[1964 | 1142.77] loss=0.83 avg=0.96\n",
      "[1965 | 1144.36] loss=1.02 avg=0.96\n",
      "[1966 | 1145.95] loss=0.91 avg=0.96\n",
      "[1967 | 1147.55] loss=0.89 avg=0.96\n",
      "[1968 | 1149.14] loss=0.82 avg=0.96\n",
      "[1969 | 1150.73] loss=1.41 avg=0.96\n",
      "[1970 | 1152.32] loss=1.04 avg=0.96\n",
      "[1971 | 1153.91] loss=0.69 avg=0.96\n",
      "[1972 | 1155.50] loss=0.22 avg=0.95\n",
      "[1973 | 1157.09] loss=1.18 avg=0.95\n",
      "[1974 | 1158.68] loss=1.21 avg=0.96\n",
      "[1975 | 1160.27] loss=1.08 avg=0.96\n",
      "[1976 | 1161.87] loss=0.62 avg=0.96\n",
      "[1977 | 1163.46] loss=0.53 avg=0.95\n",
      "[1978 | 1165.06] loss=1.75 avg=0.96\n",
      "[1979 | 1166.65] loss=0.82 avg=0.96\n",
      "[1980 | 1168.24] loss=0.35 avg=0.95\n",
      "[1981 | 1169.84] loss=0.85 avg=0.95\n",
      "[1982 | 1171.43] loss=1.37 avg=0.95\n",
      "[1983 | 1173.01] loss=0.63 avg=0.95\n",
      "[1984 | 1174.60] loss=0.81 avg=0.95\n",
      "[1985 | 1176.19] loss=0.75 avg=0.95\n",
      "[1986 | 1177.78] loss=1.49 avg=0.95\n",
      "[1987 | 1179.37] loss=1.70 avg=0.96\n",
      "[1988 | 1180.96] loss=1.02 avg=0.96\n",
      "[1989 | 1182.56] loss=0.88 avg=0.96\n",
      "[1990 | 1184.15] loss=1.82 avg=0.97\n",
      "[1991 | 1185.74] loss=1.08 avg=0.97\n",
      "[1992 | 1187.34] loss=1.25 avg=0.97\n",
      "[1993 | 1188.94] loss=1.46 avg=0.98\n",
      "[1994 | 1190.53] loss=0.95 avg=0.98\n",
      "[1995 | 1192.13] loss=0.82 avg=0.98\n",
      "[1996 | 1193.72] loss=1.07 avg=0.98\n",
      "[1997 | 1195.32] loss=0.94 avg=0.98\n",
      "[1998 | 1196.91] loss=1.19 avg=0.98\n",
      "[1999 | 1198.50] loss=0.87 avg=0.98\n",
      "Saving checkpoint/run1/model-2000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "             *\n",
      "                  * @deprecated\n",
      "                    Bool value = Integer.valueOf(b);\n",
      "\n",
      "                      if(value == -1)\n",
      "                     {\n",
      "                          throw new IllegalArgumentException(\"Value = \" + value);\n",
      "                      }\n",
      "                        try\n",
      "                       {\n",
      "                              String value = Integer.valueOf(b);\n",
      "                                if(!value)\n",
      "                              {\n",
      "                                                                                            value = Integer.valueOf(1);\n",
      "                                 }\n",
      "                           }\n",
      "                                                                                              value = Integer.valueOf(1);\n",
      "                          }\n",
      "                         }\n",
      "                 }\n",
      "\n",
      "                                                                                                                                                             }\n",
      "\n",
      "                                                                                     }\n",
      "\n",
      "                                                                                                         \n",
      "\n",
      "[2000 | 1234.10] loss=0.38 avg=0.97\n",
      "[2001 | 1235.74] loss=1.18 avg=0.97\n",
      "[2002 | 1237.38] loss=2.00 avg=0.98\n",
      "[2003 | 1239.02] loss=0.79 avg=0.98\n",
      "[2004 | 1240.66] loss=1.08 avg=0.98\n",
      "[2005 | 1242.29] loss=0.65 avg=0.98\n",
      "[2006 | 1243.93] loss=0.97 avg=0.98\n",
      "[2007 | 1245.56] loss=1.10 avg=0.98\n",
      "[2008 | 1247.19] loss=0.65 avg=0.98\n",
      "[2009 | 1248.80] loss=1.14 avg=0.98\n",
      "[2010 | 1250.42] loss=0.70 avg=0.98\n",
      "[2011 | 1252.03] loss=1.10 avg=0.98\n",
      "[2012 | 1253.63] loss=1.19 avg=0.98\n",
      "[2013 | 1255.24] loss=0.67 avg=0.98\n",
      "[2014 | 1256.84] loss=0.13 avg=0.97\n",
      "[2015 | 1258.44] loss=0.74 avg=0.97\n",
      "[2016 | 1260.03] loss=1.32 avg=0.97\n",
      "[2017 | 1261.62] loss=1.31 avg=0.97\n",
      "[2018 | 1263.22] loss=0.99 avg=0.97\n",
      "[2019 | 1264.81] loss=1.16 avg=0.98\n",
      "[2020 | 1266.39] loss=0.49 avg=0.97\n",
      "[2021 | 1267.99] loss=1.06 avg=0.97\n",
      "[2022 | 1269.57] loss=0.92 avg=0.97\n",
      "[2023 | 1271.16] loss=0.94 avg=0.97\n",
      "[2024 | 1272.75] loss=0.53 avg=0.97\n",
      "[2025 | 1274.34] loss=0.77 avg=0.96\n",
      "[2026 | 1275.93] loss=0.88 avg=0.96\n",
      "[2027 | 1277.51] loss=0.87 avg=0.96\n",
      "[2028 | 1279.10] loss=1.34 avg=0.97\n",
      "[2029 | 1280.69] loss=0.70 avg=0.96\n",
      "[2030 | 1282.27] loss=1.11 avg=0.96\n",
      "[2031 | 1283.85] loss=0.85 avg=0.96\n",
      "[2032 | 1285.44] loss=0.24 avg=0.96\n",
      "[2033 | 1287.02] loss=0.91 avg=0.96\n",
      "[2034 | 1288.61] loss=0.58 avg=0.95\n",
      "[2035 | 1290.20] loss=0.77 avg=0.95\n",
      "[2036 | 1291.79] loss=1.36 avg=0.95\n",
      "[2037 | 1293.38] loss=0.89 avg=0.95\n",
      "[2038 | 1294.98] loss=0.72 avg=0.95\n",
      "[2039 | 1296.57] loss=0.77 avg=0.95\n",
      "[2040 | 1298.16] loss=0.81 avg=0.95\n",
      "[2041 | 1299.75] loss=0.82 avg=0.95\n",
      "[2042 | 1301.34] loss=1.08 avg=0.95\n",
      "[2043 | 1302.94] loss=0.77 avg=0.95\n",
      "[2044 | 1304.53] loss=0.82 avg=0.95\n",
      "[2045 | 1306.13] loss=1.00 avg=0.95\n",
      "[2046 | 1307.73] loss=0.55 avg=0.94\n",
      "[2047 | 1309.32] loss=1.03 avg=0.94\n",
      "[2048 | 1310.93] loss=0.58 avg=0.94\n",
      "[2049 | 1312.52] loss=0.81 avg=0.94\n",
      "[2050 | 1314.12] loss=0.93 avg=0.94\n",
      "[2051 | 1315.72] loss=0.26 avg=0.93\n",
      "[2052 | 1317.32] loss=0.90 avg=0.93\n",
      "[2053 | 1318.93] loss=0.98 avg=0.93\n",
      "[2054 | 1320.53] loss=1.80 avg=0.94\n",
      "[2055 | 1322.13] loss=0.89 avg=0.94\n",
      "[2056 | 1323.72] loss=1.04 avg=0.94\n",
      "[2057 | 1325.32] loss=0.77 avg=0.94\n",
      "[2058 | 1326.92] loss=0.36 avg=0.93\n",
      "[2059 | 1328.51] loss=0.98 avg=0.93\n",
      "[2060 | 1330.11] loss=1.15 avg=0.94\n",
      "[2061 | 1331.71] loss=0.57 avg=0.93\n",
      "[2062 | 1333.31] loss=0.94 avg=0.93\n",
      "[2063 | 1334.92] loss=1.26 avg=0.94\n",
      "[2064 | 1336.51] loss=0.51 avg=0.93\n",
      "[2065 | 1338.10] loss=1.25 avg=0.93\n",
      "[2066 | 1339.70] loss=0.41 avg=0.93\n",
      "[2067 | 1341.30] loss=0.51 avg=0.92\n",
      "[2068 | 1342.90] loss=1.20 avg=0.93\n",
      "[2069 | 1344.50] loss=0.42 avg=0.92\n",
      "[2070 | 1346.11] loss=1.25 avg=0.93\n",
      "[2071 | 1347.70] loss=1.19 avg=0.93\n",
      "[2072 | 1349.31] loss=0.98 avg=0.93\n",
      "[2073 | 1350.90] loss=0.62 avg=0.93\n",
      "[2074 | 1352.50] loss=1.13 avg=0.93\n",
      "[2075 | 1354.10] loss=0.78 avg=0.93\n",
      "[2076 | 1355.69] loss=0.99 avg=0.93\n",
      "[2077 | 1357.29] loss=1.05 avg=0.93\n",
      "[2078 | 1358.89] loss=1.05 avg=0.93\n",
      "[2079 | 1360.49] loss=0.99 avg=0.93\n",
      "[2080 | 1362.08] loss=0.50 avg=0.93\n",
      "[2081 | 1363.68] loss=0.44 avg=0.92\n",
      "[2082 | 1365.27] loss=0.81 avg=0.92\n",
      "[2083 | 1366.87] loss=0.85 avg=0.92\n",
      "[2084 | 1368.46] loss=0.78 avg=0.92\n",
      "[2085 | 1370.06] loss=1.12 avg=0.92\n",
      "[2086 | 1371.66] loss=0.41 avg=0.91\n",
      "[2087 | 1373.25] loss=1.63 avg=0.92\n",
      "[2088 | 1374.85] loss=1.10 avg=0.92\n",
      "[2089 | 1376.44] loss=1.56 avg=0.93\n",
      "[2090 | 1378.04] loss=0.49 avg=0.93\n",
      "[2091 | 1379.63] loss=1.10 avg=0.93\n",
      "[2092 | 1381.23] loss=0.60 avg=0.92\n",
      "[2093 | 1382.82] loss=0.97 avg=0.92\n",
      "[2094 | 1384.41] loss=0.93 avg=0.92\n",
      "[2095 | 1386.01] loss=0.98 avg=0.92\n",
      "[2096 | 1387.61] loss=1.02 avg=0.93\n",
      "[2097 | 1389.20] loss=0.49 avg=0.92\n",
      "[2098 | 1390.79] loss=0.92 avg=0.92\n",
      "[2099 | 1392.38] loss=0.98 avg=0.92\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " {    super.onClick(tbody, this);\n",
      "    }\n",
      "    return tbody;\n",
      "   }\n",
      "  }\n",
      "  \n",
      "  public void setActionFilterMode(FilterMode filterMode)\n",
      "   {\n",
      "     this.filterMode = filterMode;\n",
      "   }\n",
      "  \n",
      "  public FilterManager getFilterManager(){\n",
      "     this.filterManager = new FilterManager();\n",
      "   }\n",
      "  \n",
      "  public void setDefaultFilterFilterMode(FilterMode defaultFilterFilterFilterMode)\n",
      "   {\n",
      "     this.defaultFilterFilterFilterMode = defaultFilterFilterFilterMode;\n",
      "  }\n",
      "  \n",
      "  @Override\n",
      "  public Object getFilterOptions() {\n",
      "     return this.filterManager.filterOptions;\n",
      "  }\n",
      "\n",
      "  @Override\n",
      "  public void setFilterFilterOptions(FilterFilterOptions filterFilterOptions)\n",
      "   {\n",
      "     super.filterFilterOptions = filterFilterOptions;\n",
      "  }\n",
      "}\n",
      "<|endoftext|>/*******************************************************************************\n",
      " * Copyright (c) 2002, 2006, Red Hat, Inc.,\n",
      " * All rights reserved.\n",
      " * Red Hat, Inc. and the associated logos are trademarks of\n",
      " * Red Hat, Inc. and are used only to identify the product or\n",
      " * service through which the files or modifications are provided.\n",
      " * \n",
      " * This program is licensed to you under the terms of version 3 of the\n",
      " * BSD License.  You may not use this product except as expressly\n",
      " * permitted under the terms of this license.  The terms and conditions\n",
      " * for this License will be found in the LICENSE file that accompanied this\n",
      " * program.\n",
      " * \n",
      " * This program is distributed in the hope that it will be useful,\n",
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      " * terms and conditions for more details.\n",
      " * See the Red Hat Public License for more details.\n",
      " * \n",
      " * You should have received a copy of the Red Hat Public License\n",
      " * along with this distribution; if not, write to the\n",
      " * Red Hat, Inc. General Counsel, PO Box 7050, Red Hat,\n",
      " *                                                                     ,\n",
      " *                  1F, San Francisco, CA 94103-7050, USA.\n",
      " *   \n",
      " *    This program and the accompanying materials is distributed\n",
      " *    under the terms of Version 3 of the BSD License.  You may not\n",
      " * use this product except as expressly allowed under the\n",
      " * terms of this license.  The terms and conditions for this License will be\n",
      " * found in the LICENSE file that accompanied this program.\n",
      " * \n",
      " * This program is distributed in the hope that it will be useful,\n",
      " * but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      " * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
      " * terms and conditions for more details.\n",
      " * See the Red Hat Public License for more details.\n",
      " * \n",
      " * You should have received a copy of the Red Hat Public License\n",
      " * along with this distribution; if not, write to the\n",
      " * Red Hat, Inc. General Counsel, PO Box 7050, Red Hat,\n",
      " *                                                                                                                                  //  The following class may not be used directly by the\n",
      " //                                          system-id/user:\n",
      "                              \n",
      "\n",
      "[2100 | 1416.10] loss=0.73 avg=0.92\n",
      "[2101 | 1417.69] loss=1.37 avg=0.92\n",
      "[2102 | 1419.28] loss=1.33 avg=0.93\n",
      "[2103 | 1420.88] loss=1.19 avg=0.93\n",
      "[2104 | 1422.47] loss=1.25 avg=0.93\n",
      "[2105 | 1424.06] loss=0.33 avg=0.93\n",
      "[2106 | 1425.65] loss=0.86 avg=0.93\n",
      "[2107 | 1427.24] loss=1.28 avg=0.93\n",
      "[2108 | 1428.83] loss=1.08 avg=0.93\n",
      "[2109 | 1430.43] loss=0.88 avg=0.93\n",
      "[2110 | 1432.01] loss=1.52 avg=0.94\n",
      "[2111 | 1433.60] loss=1.06 avg=0.94\n",
      "[2112 | 1435.19] loss=1.04 avg=0.94\n",
      "[2113 | 1436.78] loss=0.30 avg=0.93\n",
      "[2114 | 1438.37] loss=1.38 avg=0.94\n",
      "[2115 | 1439.97] loss=1.33 avg=0.94\n",
      "[2116 | 1441.56] loss=0.72 avg=0.94\n",
      "[2117 | 1443.15] loss=1.36 avg=0.94\n",
      "[2118 | 1444.75] loss=1.16 avg=0.95\n",
      "[2119 | 1446.34] loss=0.69 avg=0.94\n",
      "[2120 | 1447.93] loss=0.72 avg=0.94\n",
      "[2121 | 1449.52] loss=1.01 avg=0.94\n",
      "[2122 | 1451.11] loss=0.70 avg=0.94\n",
      "[2123 | 1452.70] loss=1.05 avg=0.94\n",
      "[2124 | 1454.30] loss=1.30 avg=0.94\n",
      "[2125 | 1455.89] loss=1.28 avg=0.95\n",
      "[2126 | 1457.48] loss=1.21 avg=0.95\n",
      "[2127 | 1459.07] loss=1.10 avg=0.95\n",
      "[2128 | 1460.66] loss=1.21 avg=0.95\n",
      "[2129 | 1462.26] loss=1.31 avg=0.96\n",
      "[2130 | 1463.85] loss=0.42 avg=0.95\n",
      "[2131 | 1465.44] loss=1.07 avg=0.95\n",
      "[2132 | 1467.03] loss=0.46 avg=0.95\n",
      "[2133 | 1468.63] loss=0.72 avg=0.95\n",
      "[2134 | 1470.22] loss=1.46 avg=0.95\n",
      "[2135 | 1471.81] loss=0.91 avg=0.95\n",
      "[2136 | 1473.39] loss=0.69 avg=0.95\n",
      "[2137 | 1474.98] loss=0.57 avg=0.95\n",
      "[2138 | 1476.57] loss=0.61 avg=0.94\n",
      "[2139 | 1478.16] loss=0.73 avg=0.94\n",
      "[2140 | 1479.75] loss=0.74 avg=0.94\n",
      "[2141 | 1481.34] loss=1.39 avg=0.94\n",
      "[2142 | 1482.93] loss=1.05 avg=0.94\n",
      "[2143 | 1484.53] loss=0.91 avg=0.94\n",
      "[2144 | 1486.12] loss=0.54 avg=0.94\n",
      "[2145 | 1487.71] loss=0.96 avg=0.94\n",
      "[2146 | 1489.31] loss=0.53 avg=0.93\n",
      "[2147 | 1490.90] loss=1.46 avg=0.94\n",
      "[2148 | 1492.50] loss=0.56 avg=0.94\n",
      "[2149 | 1494.10] loss=0.58 avg=0.93\n",
      "[2150 | 1495.69] loss=0.97 avg=0.93\n",
      "[2151 | 1497.29] loss=1.73 avg=0.94\n",
      "[2152 | 1498.89] loss=0.94 avg=0.94\n",
      "[2153 | 1500.49] loss=0.45 avg=0.94\n",
      "[2154 | 1502.08] loss=0.35 avg=0.93\n",
      "[2155 | 1503.68] loss=0.41 avg=0.93\n",
      "[2156 | 1505.28] loss=1.31 avg=0.93\n",
      "[2157 | 1506.88] loss=0.22 avg=0.92\n",
      "[2158 | 1508.48] loss=1.05 avg=0.92\n",
      "[2159 | 1510.08] loss=1.05 avg=0.92\n",
      "[2160 | 1511.68] loss=0.83 avg=0.92\n",
      "[2161 | 1513.27] loss=0.83 avg=0.92\n",
      "[2162 | 1514.87] loss=0.55 avg=0.92\n",
      "[2163 | 1516.46] loss=1.01 avg=0.92\n",
      "[2164 | 1518.05] loss=2.14 avg=0.93\n",
      "[2165 | 1519.65] loss=0.43 avg=0.93\n",
      "[2166 | 1521.24] loss=0.61 avg=0.92\n",
      "[2167 | 1522.84] loss=1.23 avg=0.93\n",
      "[2168 | 1524.44] loss=1.12 avg=0.93\n",
      "[2169 | 1526.04] loss=0.48 avg=0.92\n",
      "[2170 | 1527.64] loss=0.54 avg=0.92\n",
      "[2171 | 1529.23] loss=0.95 avg=0.92\n",
      "[2172 | 1530.82] loss=0.97 avg=0.92\n",
      "[2173 | 1532.42] loss=0.39 avg=0.92\n",
      "[2174 | 1534.01] loss=0.70 avg=0.91\n",
      "[2175 | 1535.61] loss=1.08 avg=0.92\n",
      "[2176 | 1537.20] loss=0.06 avg=0.91\n",
      "[2177 | 1538.80] loss=0.77 avg=0.91\n",
      "[2178 | 1540.39] loss=1.59 avg=0.91\n",
      "[2179 | 1541.98] loss=1.67 avg=0.92\n",
      "[2180 | 1543.58] loss=1.30 avg=0.92\n",
      "[2181 | 1545.18] loss=0.95 avg=0.92\n",
      "[2182 | 1546.77] loss=1.45 avg=0.93\n",
      "[2183 | 1548.36] loss=1.13 avg=0.93\n",
      "[2184 | 1549.95] loss=1.30 avg=0.93\n",
      "[2185 | 1551.54] loss=0.52 avg=0.93\n",
      "[2186 | 1553.14] loss=1.12 avg=0.93\n",
      "[2187 | 1554.73] loss=0.64 avg=0.93\n",
      "[2188 | 1556.33] loss=1.05 avg=0.93\n",
      "[2189 | 1557.92] loss=0.78 avg=0.93\n",
      "[2190 | 1559.51] loss=0.82 avg=0.93\n",
      "[2191 | 1561.11] loss=1.06 avg=0.93\n",
      "[2192 | 1562.70] loss=0.62 avg=0.93\n",
      "[2193 | 1564.30] loss=1.00 avg=0.93\n",
      "[2194 | 1565.89] loss=0.72 avg=0.93\n",
      "[2195 | 1567.49] loss=0.96 avg=0.93\n",
      "[2196 | 1569.08] loss=0.76 avg=0.92\n",
      "[2197 | 1570.68] loss=0.69 avg=0.92\n",
      "[2198 | 1572.28] loss=1.30 avg=0.93\n",
      "[2199 | 1573.88] loss=0.39 avg=0.92\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "0000.0,0,0x0,0x0,0x4,0x0,0x0,0x0,0x0,0x0,0x0,0xc,0,0,0,0,0x0,0x0,0xc,0x0,0x0,0x0,0xc,0,0 ,0x0,0,0 ,0x0,0,0 ,0x0,0,0 ,0x0,0,0x0,0 ,0x0,0,0x0,0 ,0x0,0,0 ,0x0,0,0 ,0x0,0,0,0 ,0x0,0,0 ,0x1,0,0 ,0x1,0,0 ,0x0,0,0 ,0x2,0,0 ,0x2,0,0 ,0x0,0,0,0 ,0x4,0,0 ,0x0,0,0x0 ,0x0,0 ,0x0,0,0 ,0x2,0,0 ,0x0,0,0 ,0x0,0,0 ,0x0,0 ,0x4,0,0 ,0x0,0,0 ,0x0,0,0 ,0x0,0 ,0x0,0,0 ,0x0,0 ,0x1,0,0 ,0x1,0,0 ,0x0,0 ,0x1,0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x1,0,0 ,0x0,0 ,0x1,0,0 ,0x0,0 ,0x1,0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x1,0,0 ,0x1,0,0 ,0x1,0,0 ,0x1,0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x1,0,0 ,0x1,0,0 ,0x1,0,0 ,0x0,0 ,0x0,0 ,0x1,0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x4,0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0x0,0 ,0\n",
      "\n",
      "[2200 | 1597.31] loss=1.41 avg=0.92\n",
      "[2201 | 1598.90] loss=1.06 avg=0.93\n",
      "[2202 | 1600.50] loss=0.50 avg=0.92\n",
      "[2203 | 1602.09] loss=0.80 avg=0.92\n",
      "[2204 | 1603.68] loss=0.75 avg=0.92\n",
      "[2205 | 1605.27] loss=0.93 avg=0.92\n",
      "[2206 | 1606.86] loss=0.42 avg=0.91\n",
      "[2207 | 1608.45] loss=1.18 avg=0.92\n",
      "[2208 | 1610.04] loss=1.50 avg=0.92\n",
      "[2209 | 1611.62] loss=1.32 avg=0.93\n",
      "[2210 | 1613.22] loss=0.97 avg=0.93\n",
      "[2211 | 1614.81] loss=1.23 avg=0.93\n",
      "[2212 | 1616.40] loss=1.14 avg=0.93\n",
      "[2213 | 1617.98] loss=0.92 avg=0.93\n",
      "[2214 | 1619.58] loss=1.18 avg=0.93\n",
      "[2215 | 1621.16] loss=0.82 avg=0.93\n",
      "[2216 | 1622.76] loss=0.69 avg=0.93\n",
      "[2217 | 1624.35] loss=1.07 avg=0.93\n",
      "[2218 | 1625.94] loss=0.92 avg=0.93\n",
      "[2219 | 1627.54] loss=1.11 avg=0.93\n",
      "[2220 | 1629.13] loss=1.32 avg=0.94\n",
      "[2221 | 1630.72] loss=0.75 avg=0.94\n",
      "[2222 | 1632.31] loss=0.55 avg=0.93\n",
      "[2223 | 1633.92] loss=1.20 avg=0.93\n",
      "[2224 | 1635.52] loss=0.94 avg=0.93\n",
      "[2225 | 1637.12] loss=0.95 avg=0.94\n",
      "[2226 | 1638.71] loss=2.29 avg=0.95\n",
      "[2227 | 1640.31] loss=1.35 avg=0.95\n",
      "[2228 | 1641.91] loss=0.94 avg=0.95\n",
      "[2229 | 1643.51] loss=0.94 avg=0.95\n",
      "[2230 | 1645.11] loss=0.92 avg=0.95\n",
      "[2231 | 1646.72] loss=1.62 avg=0.96\n",
      "[2232 | 1648.32] loss=1.24 avg=0.96\n",
      "[2233 | 1649.92] loss=0.87 avg=0.96\n",
      "[2234 | 1651.52] loss=0.66 avg=0.96\n",
      "[2235 | 1653.13] loss=1.18 avg=0.96\n",
      "[2236 | 1654.73] loss=1.02 avg=0.96\n",
      "[2237 | 1656.34] loss=0.91 avg=0.96\n",
      "[2238 | 1657.94] loss=1.10 avg=0.96\n",
      "[2239 | 1659.54] loss=0.61 avg=0.96\n",
      "[2240 | 1661.14] loss=0.75 avg=0.96\n",
      "[2241 | 1662.74] loss=0.35 avg=0.95\n",
      "[2242 | 1664.33] loss=1.78 avg=0.96\n",
      "[2243 | 1665.92] loss=0.58 avg=0.95\n",
      "[2244 | 1667.52] loss=0.65 avg=0.95\n",
      "[2245 | 1669.12] loss=1.21 avg=0.95\n",
      "[2246 | 1670.72] loss=1.13 avg=0.96\n",
      "[2247 | 1672.31] loss=0.40 avg=0.95\n",
      "[2248 | 1673.90] loss=1.48 avg=0.96\n",
      "[2249 | 1675.50] loss=0.92 avg=0.95\n",
      "[2250 | 1677.08] loss=1.64 avg=0.96\n",
      "[2251 | 1678.68] loss=0.56 avg=0.96\n",
      "[2252 | 1680.26] loss=0.79 avg=0.96\n",
      "[2253 | 1681.86] loss=0.28 avg=0.95\n",
      "[2254 | 1683.45] loss=0.89 avg=0.95\n",
      "[2255 | 1685.04] loss=0.42 avg=0.94\n",
      "[2256 | 1686.62] loss=0.79 avg=0.94\n",
      "[2257 | 1688.22] loss=0.73 avg=0.94\n",
      "[2258 | 1689.81] loss=1.30 avg=0.94\n",
      "[2259 | 1691.40] loss=0.71 avg=0.94\n",
      "[2260 | 1692.99] loss=1.08 avg=0.94\n",
      "[2261 | 1694.58] loss=0.40 avg=0.94\n",
      "[2262 | 1696.18] loss=0.59 avg=0.93\n",
      "[2263 | 1697.77] loss=0.62 avg=0.93\n",
      "[2264 | 1699.37] loss=1.09 avg=0.93\n",
      "[2265 | 1700.96] loss=1.17 avg=0.93\n",
      "[2266 | 1702.55] loss=0.90 avg=0.93\n",
      "[2267 | 1704.14] loss=0.37 avg=0.93\n",
      "[2268 | 1705.74] loss=1.02 avg=0.93\n",
      "[2269 | 1707.33] loss=1.04 avg=0.93\n",
      "[2270 | 1708.93] loss=0.85 avg=0.93\n",
      "[2271 | 1710.53] loss=0.82 avg=0.93\n",
      "[2272 | 1712.12] loss=1.18 avg=0.93\n",
      "[2273 | 1713.72] loss=1.34 avg=0.94\n",
      "[2274 | 1715.32] loss=0.50 avg=0.93\n",
      "[2275 | 1716.92] loss=0.97 avg=0.93\n",
      "[2276 | 1718.52] loss=0.80 avg=0.93\n",
      "[2277 | 1720.12] loss=1.14 avg=0.93\n",
      "[2278 | 1721.72] loss=1.04 avg=0.93\n",
      "[2279 | 1723.31] loss=1.09 avg=0.93\n",
      "[2280 | 1724.91] loss=1.10 avg=0.94\n",
      "[2281 | 1726.51] loss=0.91 avg=0.94\n",
      "[2282 | 1728.11] loss=0.45 avg=0.93\n",
      "[2283 | 1729.71] loss=0.64 avg=0.93\n",
      "[2284 | 1731.31] loss=0.72 avg=0.93\n",
      "[2285 | 1732.90] loss=0.78 avg=0.92\n",
      "[2286 | 1734.50] loss=0.52 avg=0.92\n",
      "[2287 | 1736.09] loss=1.05 avg=0.92\n",
      "[2288 | 1737.69] loss=0.82 avg=0.92\n",
      "[2289 | 1739.29] loss=0.69 avg=0.92\n",
      "[2290 | 1740.88] loss=1.05 avg=0.92\n",
      "[2291 | 1742.48] loss=0.88 avg=0.92\n",
      "[2292 | 1744.09] loss=0.96 avg=0.92\n",
      "[2293 | 1745.68] loss=1.26 avg=0.92\n",
      "[2294 | 1747.28] loss=0.64 avg=0.92\n",
      "[2295 | 1748.88] loss=1.13 avg=0.92\n",
      "[2296 | 1750.48] loss=0.67 avg=0.92\n",
      "[2297 | 1752.07] loss=0.57 avg=0.92\n",
      "[2298 | 1753.67] loss=0.75 avg=0.91\n",
      "[2299 | 1755.27] loss=0.66 avg=0.91\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "   * <p>The original version of this document did not specify a `// <em>name</em></p>.</p>\n",
      " *\n",
      " * Licensed to the Apache Software Foundation (as defined in the Apache-2.0 license)\n",
      " *         subject to the terms and conditions set forth in the Apache-2.0 distribution\n",
      " *  http://www.apache.org/licenses/LICENSE-2.0\n",
      " * \n",
      " * Unless required by applicable law or agreed to in writing, software\n",
      " * distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      " * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  See the\n",
      " * Apache-2.0 LICENSE file for additional information or\n",
      " * http://www.apache.org/2.0/\n",
      " */\n",
      "\n",
      "package com.sun.android.internal.util.annotation.annotations;\n",
      "\n",
      "import static javax.annotation.annotations.annotations.Equality;\n",
      "\n",
      "import java.lang.annotation.Convention;\n",
      "import java.lang.annotation.Convention.forClass;\n",
      "import java.lang.annotation.Conventions.Type;\n",
      "import java.lang.annotation.Impl;\n",
      "\n",
      "import com.sun.android.internal.util.annotation.annotation.Equality;\n",
      "import com.sun.android.internal.util.annotation.Equality.Impl;\n",
      "\n",
      "import com.sun.android.internal.util.annotation.EqualityImpl;\n",
      "\n",
      "/**\n",
      " * A convenience class that is used by the Android IDE to construct an array of\n",
      " * Equality annotations to be used in Android projects.\n",
      " */\n",
      "public class EqualityEquality extends AbstractAbstractEquality<Equality> {\n",
      "\n",
      "    /**\n",
      "     * This method is only relevant to the Eclipse, IntelliJ IDEA and\n",
      "     * Microsoft Visual Studio platforms.\n",
      "     */\n",
      "    protected int[] equalityEqualions = new int[1];\n",
      "\n",
      "    /**\n",
      "     * A convenience class that is used to construct an array of equality annotations to\n",
      "     * be used in the Eclipse, IntelliJ IDEA and Microsoft Visual Studio platforms.\n",
      "     */\n",
      "    protected Array<Equality, Equality> equalityEqualions = new Array<Equality, Equality>();\n",
      "\n",
      "    /**\n",
      "     *\n",
      "     * The default implementation of this method on Java programs.\n",
      "     */\n",
      "    /**\n",
      "     * The inferred equality methods if null\n",
      "     */\n",
      "    public void ignoreNull(int[] index) {\n",
      "        getIndexEqualions() .add(1);\n",
      "    }\n",
      "\n",
      "    /**\n",
      "     * @return true if the array is set to equal each of the arrays of equality\n",
      "     * annotations.\n",
      "     */\n",
      "    public boolean[] equal(String annotation);\n",
      "  }\n",
      "\n",
      "}\n",
      "<|endoftext|>/**\n",
      " * This is the <a href=\"http://www.apache.org/2.0/\">Equality</a>.\n",
      " * This is an example for an implementation that implements one inequality, which contains\n",
      " * the same method as the implementation of the <a href=\"http://www.apache.org/2.0/\">Equality</a>.\n",
      " *\n",
      " * @author <a href=\"http://www.sun.android.internal.util.annotation.annotations.annotations.Equality.Equality.Impl\">Gabe Biernagel\n",
      " * @since 2.0\n",
      " */\n",
      "package com.sun.android.internal.util.annotation.annotations;\n",
      "\n",
      "/**\n",
      " * Implementation of a <a href=\"http://www.apache.org/2.0/\">A+</a> inequality for the\n",
      " * Eclipse, IntelliJ IDEA and Microsoft Visual Studio.\n",
      " */\n",
      "public class EqualityEqual {\n",
      "   private static final Object classEqual = new Object();\n",
      "\n",
      "    @Override\n",
      "    public Equality() {\n",
      "        this(Class<?> classes) {\n",
      "             return new Equality(classes);\n",
      "        }\n",
      "\n",
      "    }\n",
      "}\n",
      "<|endoftext|>/**\n",
      " * This is the <a href=\"http://www.apache.org/2.0/\">A+</a> inequality for the\n",
      " * Eclipse, IntelliJ IDEA and Microsoft\n",
      "\n",
      "[2300 | 1778.72] loss=0.95 avg=0.91\n",
      "[2301 | 1780.31] loss=1.17 avg=0.92\n",
      "[2302 | 1781.91] loss=1.03 avg=0.92\n",
      "[2303 | 1783.51] loss=1.74 avg=0.92\n",
      "[2304 | 1785.10] loss=1.07 avg=0.93\n",
      "[2305 | 1786.69] loss=0.76 avg=0.92\n",
      "[2306 | 1788.29] loss=0.79 avg=0.92\n",
      "[2307 | 1789.88] loss=0.77 avg=0.92\n",
      "[2308 | 1791.48] loss=1.29 avg=0.93\n",
      "[2309 | 1793.07] loss=1.09 avg=0.93\n",
      "[2310 | 1794.66] loss=0.91 avg=0.93\n",
      "[2311 | 1796.25] loss=0.94 avg=0.93\n",
      "[2312 | 1797.84] loss=1.85 avg=0.94\n",
      "[2313 | 1799.43] loss=1.29 avg=0.94\n",
      "[2314 | 1801.03] loss=1.32 avg=0.94\n",
      "[2315 | 1802.62] loss=0.98 avg=0.94\n",
      "[2316 | 1804.21] loss=1.03 avg=0.94\n",
      "[2317 | 1805.80] loss=1.09 avg=0.95\n",
      "[2318 | 1807.39] loss=0.31 avg=0.94\n",
      "[2319 | 1808.98] loss=2.54 avg=0.96\n",
      "[2320 | 1810.57] loss=1.74 avg=0.96\n",
      "[2321 | 1812.17] loss=0.37 avg=0.96\n",
      "[2322 | 1813.76] loss=0.90 avg=0.96\n",
      "[2323 | 1815.36] loss=0.77 avg=0.96\n",
      "[2324 | 1816.95] loss=0.51 avg=0.95\n",
      "[2325 | 1818.53] loss=0.79 avg=0.95\n",
      "[2326 | 1820.13] loss=1.19 avg=0.95\n",
      "[2327 | 1821.71] loss=0.44 avg=0.95\n",
      "[2328 | 1823.31] loss=0.78 avg=0.94\n",
      "[2329 | 1824.89] loss=0.49 avg=0.94\n",
      "[2330 | 1826.48] loss=1.26 avg=0.94\n",
      "[2331 | 1828.07] loss=0.95 avg=0.94\n",
      "[2332 | 1829.67] loss=0.98 avg=0.94\n",
      "[2333 | 1831.26] loss=0.87 avg=0.94\n",
      "[2334 | 1832.85] loss=0.78 avg=0.94\n",
      "[2335 | 1834.45] loss=0.88 avg=0.94\n",
      "[2336 | 1836.03] loss=1.00 avg=0.94\n",
      "[2337 | 1837.62] loss=1.13 avg=0.94\n",
      "[2338 | 1839.22] loss=0.58 avg=0.94\n",
      "[2339 | 1840.81] loss=1.32 avg=0.94\n",
      "[2340 | 1842.40] loss=0.91 avg=0.94\n",
      "[2341 | 1843.99] loss=1.22 avg=0.95\n",
      "[2342 | 1845.60] loss=0.90 avg=0.95\n",
      "[2343 | 1847.19] loss=0.78 avg=0.94\n",
      "[2344 | 1848.79] loss=1.14 avg=0.95\n",
      "[2345 | 1850.39] loss=1.03 avg=0.95\n",
      "[2346 | 1851.98] loss=0.63 avg=0.94\n",
      "[2347 | 1853.58] loss=0.67 avg=0.94\n",
      "[2348 | 1855.18] loss=0.84 avg=0.94\n",
      "[2349 | 1856.77] loss=1.19 avg=0.94\n",
      "[2350 | 1858.37] loss=0.88 avg=0.94\n",
      "[2351 | 1859.97] loss=0.89 avg=0.94\n",
      "[2352 | 1861.57] loss=0.83 avg=0.94\n",
      "[2353 | 1863.17] loss=1.47 avg=0.95\n",
      "[2354 | 1864.77] loss=1.35 avg=0.95\n",
      "[2355 | 1866.37] loss=1.23 avg=0.95\n",
      "[2356 | 1867.97] loss=0.63 avg=0.95\n",
      "[2357 | 1869.57] loss=0.86 avg=0.95\n",
      "[2358 | 1871.16] loss=0.91 avg=0.95\n",
      "[2359 | 1872.76] loss=0.85 avg=0.95\n",
      "[2360 | 1874.36] loss=0.73 avg=0.94\n",
      "[2361 | 1875.95] loss=0.91 avg=0.94\n",
      "[2362 | 1877.55] loss=1.11 avg=0.95\n",
      "[2363 | 1879.14] loss=0.64 avg=0.94\n",
      "[2364 | 1880.73] loss=1.12 avg=0.94\n",
      "[2365 | 1882.33] loss=0.20 avg=0.94\n",
      "[2366 | 1883.92] loss=0.83 avg=0.94\n",
      "[2367 | 1885.52] loss=0.83 avg=0.93\n",
      "[2368 | 1887.11] loss=0.62 avg=0.93\n",
      "[2369 | 1888.70] loss=0.87 avg=0.93\n",
      "[2370 | 1890.30] loss=1.23 avg=0.93\n",
      "[2371 | 1891.89] loss=0.43 avg=0.93\n",
      "[2372 | 1893.49] loss=1.69 avg=0.94\n",
      "[2373 | 1895.09] loss=0.87 avg=0.94\n",
      "[2374 | 1896.69] loss=1.41 avg=0.94\n",
      "[2375 | 1898.29] loss=1.05 avg=0.94\n",
      "[2376 | 1899.88] loss=1.20 avg=0.94\n",
      "[2377 | 1901.48] loss=0.57 avg=0.94\n",
      "[2378 | 1903.07] loss=0.80 avg=0.94\n",
      "[2379 | 1904.66] loss=0.96 avg=0.94\n",
      "[2380 | 1906.25] loss=0.29 avg=0.93\n",
      "[2381 | 1907.85] loss=2.22 avg=0.95\n",
      "[2382 | 1909.45] loss=0.96 avg=0.95\n",
      "[2383 | 1911.04] loss=1.49 avg=0.95\n",
      "[2384 | 1912.63] loss=1.23 avg=0.95\n",
      "[2385 | 1914.23] loss=0.79 avg=0.95\n",
      "[2386 | 1915.82] loss=1.38 avg=0.96\n",
      "[2387 | 1917.41] loss=1.09 avg=0.96\n",
      "[2388 | 1919.00] loss=1.59 avg=0.96\n",
      "[2389 | 1920.60] loss=0.20 avg=0.96\n",
      "[2390 | 1922.19] loss=0.53 avg=0.95\n",
      "[2391 | 1923.79] loss=0.85 avg=0.95\n",
      "[2392 | 1925.38] loss=0.92 avg=0.95\n",
      "[2393 | 1926.98] loss=1.16 avg=0.95\n",
      "[2394 | 1928.57] loss=0.39 avg=0.95\n",
      "[2395 | 1930.16] loss=0.58 avg=0.94\n",
      "[2396 | 1931.76] loss=0.92 avg=0.94\n",
      "[2397 | 1933.35] loss=1.06 avg=0.95\n",
      "[2398 | 1934.95] loss=0.98 avg=0.95\n",
      "[2399 | 1936.54] loss=1.29 avg=0.95\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "         return m_p2;\n",
      "    \n",
      "    \n",
      "     public void testSet() throws Exception {\n",
      "        \n",
      "         if (type != ArrayType.String) {\n",
      "                 try {\n",
      "                        m_p2 = m_p3;\n",
      "                   } catch (Exception elem) {\n",
      "                                                                                                                        } catch (LambdaElem                                                                                                                                                                                                                                                                                                    } catch (UnsatisfactoryElem                                                                                                                      !(m_p1 != m_p0))\n",
      "                       {}                                                                                                                                                                                       ;\n",
      "                  /* (LAMBDA                    LAMBDA                 LAMBDA               LAMBDA               LAMBDA            \n",
      "\n",
      "[2400 | 1959.93] loss=0.75 avg=0.95\n",
      "[2401 | 1961.53] loss=1.24 avg=0.95\n",
      "[2402 | 1963.12] loss=1.49 avg=0.96\n",
      "[2403 | 1964.73] loss=1.54 avg=0.96\n",
      "[2404 | 1966.33] loss=0.40 avg=0.96\n",
      "[2405 | 1967.92] loss=0.85 avg=0.95\n",
      "[2406 | 1969.51] loss=0.73 avg=0.95\n",
      "[2407 | 1971.11] loss=0.72 avg=0.95\n",
      "[2408 | 1972.71] loss=1.06 avg=0.95\n",
      "[2409 | 1974.30] loss=0.93 avg=0.95\n",
      "[2410 | 1975.90] loss=1.64 avg=0.96\n",
      "[2411 | 1977.49] loss=1.18 avg=0.96\n",
      "[2412 | 1979.08] loss=0.57 avg=0.96\n",
      "[2413 | 1980.68] loss=0.36 avg=0.95\n",
      "[2414 | 1982.27] loss=0.99 avg=0.95\n",
      "[2415 | 1983.86] loss=0.62 avg=0.95\n",
      "[2416 | 1985.45] loss=1.55 avg=0.95\n",
      "[2417 | 1987.05] loss=0.79 avg=0.95\n",
      "[2418 | 1988.64] loss=0.77 avg=0.95\n",
      "[2419 | 1990.23] loss=0.84 avg=0.95\n",
      "[2420 | 1991.82] loss=0.97 avg=0.95\n",
      "[2421 | 1993.42] loss=1.06 avg=0.95\n",
      "[2422 | 1995.01] loss=1.06 avg=0.95\n",
      "[2423 | 1996.60] loss=0.65 avg=0.95\n",
      "[2424 | 1998.19] loss=0.77 avg=0.95\n",
      "[2425 | 1999.78] loss=1.24 avg=0.95\n",
      "[2426 | 2001.38] loss=1.43 avg=0.95\n",
      "[2427 | 2002.97] loss=0.84 avg=0.95\n",
      "[2428 | 2004.56] loss=1.00 avg=0.95\n",
      "[2429 | 2006.16] loss=1.39 avg=0.96\n",
      "[2430 | 2007.75] loss=0.82 avg=0.96\n",
      "[2431 | 2009.33] loss=0.90 avg=0.96\n",
      "[2432 | 2010.92] loss=1.01 avg=0.96\n",
      "[2433 | 2012.50] loss=0.60 avg=0.95\n",
      "[2434 | 2014.09] loss=1.16 avg=0.95\n",
      "[2435 | 2015.68] loss=1.38 avg=0.96\n",
      "[2436 | 2017.27] loss=0.84 avg=0.96\n",
      "[2437 | 2018.86] loss=0.55 avg=0.95\n",
      "[2438 | 2020.46] loss=0.86 avg=0.95\n",
      "[2439 | 2022.05] loss=1.06 avg=0.95\n",
      "[2440 | 2023.64] loss=0.57 avg=0.95\n",
      "[2441 | 2025.23] loss=1.33 avg=0.95\n",
      "[2442 | 2026.83] loss=1.06 avg=0.95\n",
      "[2443 | 2028.42] loss=0.53 avg=0.95\n",
      "[2444 | 2030.02] loss=0.55 avg=0.95\n",
      "[2445 | 2031.63] loss=1.13 avg=0.95\n",
      "[2446 | 2033.22] loss=1.35 avg=0.95\n",
      "[2447 | 2034.81] loss=0.58 avg=0.95\n",
      "[2448 | 2036.41] loss=0.97 avg=0.95\n",
      "[2449 | 2038.00] loss=1.13 avg=0.95\n",
      "[2450 | 2039.60] loss=1.27 avg=0.95\n",
      "[2451 | 2041.21] loss=0.95 avg=0.95\n",
      "[2452 | 2042.81] loss=0.79 avg=0.95\n",
      "[2453 | 2044.42] loss=0.88 avg=0.95\n",
      "[2454 | 2046.02] loss=0.36 avg=0.95\n",
      "[2455 | 2047.62] loss=1.02 avg=0.95\n",
      "[2456 | 2049.22] loss=0.90 avg=0.95\n",
      "[2457 | 2050.81] loss=1.29 avg=0.95\n",
      "[2458 | 2052.41] loss=1.10 avg=0.95\n",
      "[2459 | 2054.00] loss=0.71 avg=0.95\n",
      "[2460 | 2055.60] loss=0.47 avg=0.94\n",
      "[2461 | 2057.20] loss=0.79 avg=0.94\n",
      "[2462 | 2058.79] loss=1.02 avg=0.94\n",
      "[2463 | 2060.39] loss=0.73 avg=0.94\n",
      "[2464 | 2061.99] loss=0.93 avg=0.94\n",
      "[2465 | 2063.58] loss=1.22 avg=0.94\n",
      "[2466 | 2065.17] loss=1.32 avg=0.95\n",
      "[2467 | 2066.76] loss=0.68 avg=0.94\n",
      "[2468 | 2068.36] loss=0.67 avg=0.94\n",
      "[2469 | 2069.95] loss=0.33 avg=0.94\n",
      "[2470 | 2071.55] loss=0.65 avg=0.93\n",
      "[2471 | 2073.14] loss=0.71 avg=0.93\n",
      "[2472 | 2074.73] loss=0.90 avg=0.93\n",
      "[2473 | 2076.32] loss=0.90 avg=0.93\n",
      "[2474 | 2077.92] loss=1.06 avg=0.93\n",
      "[2475 | 2079.52] loss=1.76 avg=0.94\n",
      "[2476 | 2081.11] loss=0.36 avg=0.93\n",
      "[2477 | 2082.70] loss=1.25 avg=0.94\n",
      "[2478 | 2084.29] loss=0.79 avg=0.94\n",
      "[2479 | 2085.88] loss=1.04 avg=0.94\n",
      "[2480 | 2087.47] loss=1.02 avg=0.94\n",
      "[2481 | 2089.06] loss=0.54 avg=0.93\n",
      "[2482 | 2090.66] loss=0.33 avg=0.93\n",
      "[2483 | 2092.25] loss=0.93 avg=0.93\n",
      "[2484 | 2093.84] loss=0.34 avg=0.92\n",
      "[2485 | 2095.43] loss=0.96 avg=0.92\n",
      "[2486 | 2097.02] loss=0.92 avg=0.92\n",
      "[2487 | 2098.62] loss=0.92 avg=0.92\n",
      "[2488 | 2100.22] loss=1.16 avg=0.92\n",
      "[2489 | 2101.81] loss=1.11 avg=0.93\n",
      "[2490 | 2103.40] loss=1.15 avg=0.93\n",
      "[2491 | 2105.00] loss=0.98 avg=0.93\n",
      "[2492 | 2106.60] loss=0.90 avg=0.93\n",
      "[2493 | 2108.19] loss=1.08 avg=0.93\n",
      "[2494 | 2109.79] loss=0.89 avg=0.93\n",
      "[2495 | 2111.38] loss=1.60 avg=0.94\n",
      "[2496 | 2112.98] loss=0.75 avg=0.93\n",
      "[2497 | 2114.58] loss=0.93 avg=0.93\n",
      "[2498 | 2116.17] loss=0.62 avg=0.93\n",
      "[2499 | 2117.77] loss=1.27 avg=0.93\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      ".1*(n1+n2),(c*sqrt) - c2 + c3 + c+c4;\n",
      "                      \n",
      "                   */\n",
      "                  public int[] sqrt(int n1, int n2, int n3, int n4) {\n",
      "                      int sum = this.add(\"sum\", -(n1-(n2-n3))) / 2;\n",
      "                       if (sum != 0) {\n",
      "                           return this.get(n1);\n",
      "                         }\n",
      "                       int c =(n1+(n2-n3));\n",
      "\n",
      "                       if (sum != 0) {\n",
      "                          return this.get(n1);\n",
      "                         int c =(n1-(n2-n3) - c+c-c4);\n",
      "                        int n0 = c;\n",
      "                        int n1 = n2;\n",
      "                       int n2 = n3;\n",
      "                      int n3 = n4 - n0;\n",
      "                     try {\n",
      "                                int c2 = (n1-2) * sqrt(n1+n2 + c2);\n",
      "                                if (sum != 0) {\n",
      "                                     int c3 = c2 - c0;\n",
      "                                      if (sum != 0) {\n",
      "                                         int c4 = c1 - c0 + c4 + c4 + c4;\n",
      "                                        if (sum != 0) {\n",
      "                                           int c3 = c2 - c4 - c4 + c4 + c5;\n",
      "                                         int c4 = (n1-2 + c1) * sqrt(n1+n3);\n",
      "                                        if (sum != 0) {\n",
      "               \n",
      "\n",
      "[2500 | 2141.36] loss=3.29 avg=0.96\n",
      "[2501 | 2142.95] loss=1.11 avg=0.96\n",
      "[2502 | 2144.54] loss=0.76 avg=0.96\n",
      "[2503 | 2146.13] loss=1.17 avg=0.96\n",
      "[2504 | 2147.72] loss=0.90 avg=0.96\n",
      "[2505 | 2149.31] loss=0.64 avg=0.96\n",
      "[2506 | 2150.91] loss=0.95 avg=0.96\n",
      "[2507 | 2152.50] loss=1.08 avg=0.96\n",
      "[2508 | 2154.09] loss=0.80 avg=0.96\n",
      "[2509 | 2155.68] loss=0.79 avg=0.95\n",
      "[2510 | 2157.26] loss=1.06 avg=0.95\n",
      "[2511 | 2158.85] loss=0.19 avg=0.95\n",
      "[2512 | 2160.44] loss=0.65 avg=0.94\n",
      "[2513 | 2162.03] loss=0.87 avg=0.94\n",
      "[2514 | 2163.63] loss=0.95 avg=0.94\n",
      "[2515 | 2165.21] loss=0.60 avg=0.94\n",
      "[2516 | 2166.80] loss=1.18 avg=0.94\n",
      "[2517 | 2168.39] loss=0.98 avg=0.94\n",
      "[2518 | 2169.98] loss=0.85 avg=0.94\n",
      "[2519 | 2171.58] loss=0.74 avg=0.94\n",
      "[2520 | 2173.17] loss=1.32 avg=0.94\n",
      "[2521 | 2174.77] loss=0.97 avg=0.94\n",
      "[2522 | 2176.36] loss=1.28 avg=0.95\n",
      "[2523 | 2177.95] loss=0.98 avg=0.95\n",
      "[2524 | 2179.54] loss=0.88 avg=0.95\n",
      "[2525 | 2181.13] loss=1.62 avg=0.95\n",
      "[2526 | 2182.73] loss=1.46 avg=0.96\n",
      "[2527 | 2184.32] loss=0.68 avg=0.96\n",
      "[2528 | 2185.92] loss=0.79 avg=0.95\n",
      "[2529 | 2187.52] loss=1.36 avg=0.96\n",
      "[2530 | 2189.12] loss=0.70 avg=0.96\n",
      "[2531 | 2190.71] loss=1.40 avg=0.96\n",
      "[2532 | 2192.31] loss=0.27 avg=0.95\n",
      "[2533 | 2193.90] loss=0.46 avg=0.95\n",
      "[2534 | 2195.49] loss=1.16 avg=0.95\n",
      "[2535 | 2197.09] loss=1.02 avg=0.95\n",
      "[2536 | 2198.69] loss=1.62 avg=0.96\n",
      "[2537 | 2200.28] loss=1.09 avg=0.96\n",
      "[2538 | 2201.88] loss=1.16 avg=0.96\n",
      "[2539 | 2203.47] loss=0.48 avg=0.96\n",
      "[2540 | 2205.06] loss=1.20 avg=0.96\n",
      "[2541 | 2206.66] loss=0.75 avg=0.96\n",
      "[2542 | 2208.25] loss=0.88 avg=0.96\n",
      "[2543 | 2209.86] loss=1.03 avg=0.96\n",
      "[2544 | 2211.45] loss=0.39 avg=0.95\n",
      "[2545 | 2213.05] loss=0.86 avg=0.95\n",
      "[2546 | 2214.64] loss=0.68 avg=0.95\n",
      "[2547 | 2216.24] loss=1.34 avg=0.95\n",
      "[2548 | 2217.84] loss=0.52 avg=0.95\n",
      "[2549 | 2219.44] loss=0.99 avg=0.95\n",
      "[2550 | 2221.04] loss=1.38 avg=0.95\n",
      "[2551 | 2222.63] loss=0.57 avg=0.95\n",
      "[2552 | 2224.23] loss=0.56 avg=0.94\n",
      "[2553 | 2225.82] loss=1.29 avg=0.95\n",
      "[2554 | 2227.43] loss=1.21 avg=0.95\n",
      "[2555 | 2229.03] loss=2.06 avg=0.96\n",
      "[2556 | 2230.63] loss=1.02 avg=0.96\n",
      "[2557 | 2232.23] loss=1.07 avg=0.96\n",
      "[2558 | 2233.83] loss=0.84 avg=0.96\n",
      "[2559 | 2235.43] loss=0.88 avg=0.96\n",
      "[2560 | 2237.03] loss=1.75 avg=0.97\n",
      "[2561 | 2238.62] loss=0.90 avg=0.97\n",
      "[2562 | 2240.22] loss=0.95 avg=0.97\n",
      "[2563 | 2241.81] loss=0.61 avg=0.96\n",
      "[2564 | 2243.41] loss=0.69 avg=0.96\n",
      "[2565 | 2245.00] loss=0.69 avg=0.96\n",
      "[2566 | 2246.60] loss=0.80 avg=0.96\n",
      "[2567 | 2248.20] loss=1.17 avg=0.96\n",
      "[2568 | 2249.80] loss=0.89 avg=0.96\n",
      "[2569 | 2251.40] loss=0.58 avg=0.96\n",
      "[2570 | 2253.00] loss=0.73 avg=0.95\n",
      "[2571 | 2254.60] loss=0.61 avg=0.95\n",
      "[2572 | 2256.20] loss=1.05 avg=0.95\n",
      "[2573 | 2257.80] loss=0.84 avg=0.95\n",
      "[2574 | 2259.40] loss=0.32 avg=0.94\n",
      "[2575 | 2260.99] loss=0.54 avg=0.94\n",
      "[2576 | 2262.59] loss=0.35 avg=0.93\n",
      "[2577 | 2264.19] loss=1.23 avg=0.94\n",
      "[2578 | 2265.78] loss=0.86 avg=0.94\n",
      "[2579 | 2267.37] loss=0.98 avg=0.94\n",
      "[2580 | 2268.97] loss=1.00 avg=0.94\n",
      "[2581 | 2270.57] loss=0.41 avg=0.93\n",
      "[2582 | 2272.16] loss=0.87 avg=0.93\n",
      "[2583 | 2273.76] loss=0.93 avg=0.93\n",
      "[2584 | 2275.36] loss=0.60 avg=0.93\n",
      "[2585 | 2276.96] loss=0.81 avg=0.93\n",
      "[2586 | 2278.55] loss=1.03 avg=0.93\n",
      "[2587 | 2280.15] loss=0.74 avg=0.93\n",
      "[2588 | 2281.75] loss=1.17 avg=0.93\n",
      "[2589 | 2283.34] loss=0.56 avg=0.92\n",
      "[2590 | 2284.94] loss=0.58 avg=0.92\n",
      "[2591 | 2286.53] loss=0.95 avg=0.92\n",
      "[2592 | 2288.13] loss=0.91 avg=0.92\n",
      "[2593 | 2289.73] loss=0.83 avg=0.92\n",
      "[2594 | 2291.32] loss=0.72 avg=0.92\n",
      "[2595 | 2292.92] loss=0.70 avg=0.92\n",
      "[2596 | 2294.51] loss=0.43 avg=0.91\n",
      "[2597 | 2296.11] loss=1.28 avg=0.91\n",
      "[2598 | 2297.71] loss=0.57 avg=0.91\n",
      "[2599 | 2299.30] loss=0.43 avg=0.91\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ----------------------------------------------------------------\n",
      "\n",
      "RAW Paste Data\n",
      "\n",
      "/** ---                                                                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "[2600 | 2322.68] loss=0.42 avg=0.90\n",
      "[2601 | 2324.28] loss=0.85 avg=0.90\n",
      "[2602 | 2325.87] loss=1.67 avg=0.91\n",
      "[2603 | 2327.46] loss=1.12 avg=0.91\n",
      "[2604 | 2329.05] loss=1.32 avg=0.91\n",
      "[2605 | 2330.65] loss=0.83 avg=0.91\n",
      "[2606 | 2332.24] loss=1.19 avg=0.92\n",
      "[2607 | 2333.83] loss=1.20 avg=0.92\n",
      "[2608 | 2335.43] loss=0.63 avg=0.92\n",
      "[2609 | 2337.02] loss=0.88 avg=0.92\n",
      "[2610 | 2338.61] loss=1.25 avg=0.92\n",
      "[2611 | 2340.20] loss=1.25 avg=0.92\n",
      "[2612 | 2341.79] loss=0.81 avg=0.92\n",
      "[2613 | 2343.38] loss=0.90 avg=0.92\n",
      "[2614 | 2344.97] loss=1.06 avg=0.92\n",
      "[2615 | 2346.56] loss=1.09 avg=0.92\n",
      "[2616 | 2348.15] loss=0.91 avg=0.92\n",
      "[2617 | 2349.74] loss=0.81 avg=0.92\n",
      "[2618 | 2351.33] loss=1.01 avg=0.92\n",
      "[2619 | 2352.92] loss=0.80 avg=0.92\n",
      "[2620 | 2354.51] loss=1.16 avg=0.93\n",
      "[2621 | 2356.10] loss=0.73 avg=0.92\n",
      "[2622 | 2357.70] loss=0.96 avg=0.92\n",
      "[2623 | 2359.29] loss=0.77 avg=0.92\n",
      "[2624 | 2360.89] loss=0.88 avg=0.92\n",
      "[2625 | 2362.48] loss=1.31 avg=0.93\n",
      "[2626 | 2364.07] loss=0.61 avg=0.92\n",
      "[2627 | 2365.67] loss=0.96 avg=0.92\n",
      "[2628 | 2367.26] loss=1.02 avg=0.92\n",
      "[2629 | 2368.85] loss=0.90 avg=0.92\n",
      "[2630 | 2370.44] loss=0.69 avg=0.92\n",
      "[2631 | 2372.04] loss=0.96 avg=0.92\n",
      "[2632 | 2373.64] loss=0.69 avg=0.92\n",
      "[2633 | 2375.24] loss=0.64 avg=0.92\n",
      "[2634 | 2376.83] loss=0.51 avg=0.91\n",
      "[2635 | 2378.42] loss=0.71 avg=0.91\n",
      "[2636 | 2380.01] loss=1.13 avg=0.91\n",
      "[2637 | 2381.60] loss=0.82 avg=0.91\n",
      "[2638 | 2383.20] loss=0.94 avg=0.91\n",
      "[2639 | 2384.80] loss=0.54 avg=0.91\n",
      "[2640 | 2386.39] loss=1.29 avg=0.91\n",
      "[2641 | 2387.99] loss=1.16 avg=0.91\n",
      "[2642 | 2389.57] loss=1.03 avg=0.92\n",
      "[2643 | 2391.17] loss=1.52 avg=0.92\n",
      "[2644 | 2392.77] loss=0.70 avg=0.92\n",
      "[2645 | 2394.37] loss=1.53 avg=0.93\n",
      "[2646 | 2395.96] loss=1.03 avg=0.93\n",
      "[2647 | 2397.56] loss=0.51 avg=0.92\n",
      "[2648 | 2399.15] loss=0.95 avg=0.92\n",
      "[2649 | 2400.75] loss=0.76 avg=0.92\n",
      "[2650 | 2402.35] loss=0.99 avg=0.92\n",
      "[2651 | 2403.95] loss=0.89 avg=0.92\n",
      "[2652 | 2405.55] loss=0.76 avg=0.92\n",
      "[2653 | 2407.15] loss=1.31 avg=0.92\n",
      "[2654 | 2408.74] loss=1.06 avg=0.93\n",
      "[2655 | 2410.34] loss=1.36 avg=0.93\n",
      "[2656 | 2411.94] loss=1.14 avg=0.93\n",
      "[2657 | 2413.53] loss=0.82 avg=0.93\n",
      "[2658 | 2415.13] loss=0.86 avg=0.93\n",
      "[2659 | 2416.72] loss=0.90 avg=0.93\n",
      "[2660 | 2418.32] loss=0.85 avg=0.93\n",
      "[2661 | 2419.92] loss=1.79 avg=0.94\n",
      "[2662 | 2421.52] loss=0.50 avg=0.93\n",
      "[2663 | 2423.12] loss=1.19 avg=0.94\n",
      "[2664 | 2424.71] loss=0.65 avg=0.93\n",
      "[2665 | 2426.31] loss=0.20 avg=0.93\n",
      "[2666 | 2427.91] loss=0.72 avg=0.92\n",
      "[2667 | 2429.51] loss=1.17 avg=0.93\n",
      "[2668 | 2431.10] loss=0.77 avg=0.92\n",
      "[2669 | 2432.70] loss=0.85 avg=0.92\n",
      "[2670 | 2434.30] loss=0.77 avg=0.92\n",
      "[2671 | 2435.89] loss=0.15 avg=0.91\n",
      "[2672 | 2437.49] loss=0.48 avg=0.91\n",
      "[2673 | 2439.09] loss=1.09 avg=0.91\n",
      "[2674 | 2440.69] loss=0.37 avg=0.91\n",
      "[2675 | 2442.29] loss=0.94 avg=0.91\n",
      "[2676 | 2443.88] loss=0.58 avg=0.90\n",
      "[2677 | 2445.48] loss=0.48 avg=0.90\n",
      "[2678 | 2447.08] loss=0.90 avg=0.90\n",
      "[2679 | 2448.67] loss=1.10 avg=0.90\n",
      "[2680 | 2450.26] loss=1.19 avg=0.90\n",
      "[2681 | 2451.85] loss=1.03 avg=0.91\n",
      "[2682 | 2453.44] loss=0.87 avg=0.90\n",
      "[2683 | 2455.04] loss=0.64 avg=0.90\n",
      "[2684 | 2456.63] loss=0.95 avg=0.90\n",
      "[2685 | 2458.22] loss=0.62 avg=0.90\n",
      "[2686 | 2459.81] loss=0.79 avg=0.90\n",
      "[2687 | 2461.40] loss=0.26 avg=0.89\n",
      "[2688 | 2463.00] loss=0.78 avg=0.89\n",
      "[2689 | 2464.59] loss=1.56 avg=0.90\n",
      "[2690 | 2466.19] loss=0.82 avg=0.90\n",
      "[2691 | 2467.78] loss=2.30 avg=0.91\n",
      "[2692 | 2469.37] loss=1.05 avg=0.91\n",
      "[2693 | 2470.97] loss=1.30 avg=0.92\n",
      "[2694 | 2472.55] loss=1.61 avg=0.92\n",
      "[2695 | 2474.15] loss=0.38 avg=0.92\n",
      "[2696 | 2475.74] loss=0.67 avg=0.92\n",
      "[2697 | 2477.32] loss=0.71 avg=0.91\n",
      "[2698 | 2478.92] loss=1.25 avg=0.92\n",
      "[2699 | 2480.51] loss=1.13 avg=0.92\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "i's head from behind?\n",
      "     */\n",
      "    public static void\n",
      "    execute() {\n",
      "        if (nrows != 1\n",
      "                && ncolumns != 0\n",
      "                    && tblClass != null && (tblClass.isClass() ? ClassID.getClass()\n",
      "                                                                                               System.out.println(\"\");\n",
      "        break;\n",
      "    }\n",
      "\n",
      "    protected synchronized boolean\n",
      "    executeEx(IntPtr id, int nrows, int ncolumns)\n",
      "       throws Exception\n",
      "       {\n",
      "        String[] idNameX = id + \", \"\n",
      "        String idX = ID.getString(idNameX);\n",
      "\n",
      "       if (nrows < 1) {\n",
      "           return false;\n",
      "        }\n",
      "        if (ncolumns < 0) {\n",
      "           return false;\n",
      "        }\n",
      "        System.out.println(\"EXIT status: \" + id, true);\n",
      "        final List<String> result = new List<String>();\n",
      "        List<String> resultX = new List<String>();\n",
      "        for (int i = 0; i < nrows; i++) {\n",
      "            List<String> rown = (List<String>)(getRowNumber(rown));\n",
      "           if (getColumnNumber(rown) != null && getColumnString(rown) != null) {\n",
      "             System.log.println(\"CREATE TABLE \" + getColumnName(rown) + \", id.type\"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
      "\n",
      "[2700 | 2503.91] loss=0.90 avg=0.92\n",
      "[2701 | 2505.51] loss=0.70 avg=0.92\n",
      "[2702 | 2507.10] loss=0.84 avg=0.92\n",
      "[2703 | 2508.70] loss=0.95 avg=0.92\n",
      "[2704 | 2510.30] loss=1.12 avg=0.92\n",
      "[2705 | 2511.90] loss=0.63 avg=0.92\n",
      "[2706 | 2513.50] loss=2.04 avg=0.93\n",
      "[2707 | 2515.09] loss=0.85 avg=0.93\n",
      "[2708 | 2516.69] loss=0.83 avg=0.92\n",
      "[2709 | 2518.28] loss=0.92 avg=0.92\n",
      "[2710 | 2519.88] loss=0.49 avg=0.92\n",
      "[2711 | 2521.47] loss=0.68 avg=0.92\n",
      "[2712 | 2523.06] loss=0.78 avg=0.92\n",
      "[2713 | 2524.66] loss=0.85 avg=0.92\n",
      "[2714 | 2526.25] loss=1.08 avg=0.92\n",
      "[2715 | 2527.84] loss=0.37 avg=0.91\n",
      "[2716 | 2529.44] loss=0.86 avg=0.91\n",
      "[2717 | 2531.03] loss=0.85 avg=0.91\n",
      "[2718 | 2532.63] loss=1.02 avg=0.91\n",
      "[2719 | 2534.23] loss=0.34 avg=0.91\n",
      "[2720 | 2535.82] loss=1.05 avg=0.91\n",
      "[2721 | 2537.42] loss=1.62 avg=0.91\n",
      "[2722 | 2539.01] loss=1.40 avg=0.92\n",
      "[2723 | 2540.60] loss=0.71 avg=0.92\n",
      "[2724 | 2542.20] loss=0.36 avg=0.91\n",
      "[2725 | 2543.79] loss=1.32 avg=0.92\n",
      "[2726 | 2545.39] loss=0.21 avg=0.91\n",
      "[2727 | 2546.98] loss=1.32 avg=0.91\n",
      "[2728 | 2548.58] loss=0.31 avg=0.91\n",
      "[2729 | 2550.17] loss=1.35 avg=0.91\n",
      "[2730 | 2551.76] loss=1.30 avg=0.92\n",
      "[2731 | 2553.36] loss=0.72 avg=0.91\n",
      "[2732 | 2554.95] loss=1.09 avg=0.92\n",
      "[2733 | 2556.54] loss=1.33 avg=0.92\n",
      "[2734 | 2558.13] loss=0.73 avg=0.92\n",
      "[2735 | 2559.72] loss=0.79 avg=0.92\n",
      "[2736 | 2561.31] loss=1.23 avg=0.92\n",
      "[2737 | 2562.90] loss=1.02 avg=0.92\n",
      "[2738 | 2564.50] loss=1.48 avg=0.93\n",
      "[2739 | 2566.09] loss=0.85 avg=0.93\n",
      "[2740 | 2567.68] loss=0.55 avg=0.92\n",
      "[2741 | 2569.27] loss=1.45 avg=0.93\n",
      "[2742 | 2570.87] loss=1.25 avg=0.93\n",
      "[2743 | 2572.46] loss=0.63 avg=0.93\n",
      "[2744 | 2574.06] loss=0.71 avg=0.92\n",
      "[2745 | 2575.65] loss=0.75 avg=0.92\n",
      "[2746 | 2577.24] loss=0.51 avg=0.92\n",
      "[2747 | 2578.84] loss=0.77 avg=0.92\n",
      "[2748 | 2580.44] loss=1.13 avg=0.92\n",
      "[2749 | 2582.04] loss=1.09 avg=0.92\n",
      "[2750 | 2583.64] loss=0.91 avg=0.92\n",
      "[2751 | 2585.24] loss=1.09 avg=0.92\n",
      "[2752 | 2586.84] loss=0.68 avg=0.92\n",
      "[2753 | 2588.45] loss=1.33 avg=0.92\n",
      "[2754 | 2590.04] loss=0.68 avg=0.92\n",
      "[2755 | 2591.64] loss=0.44 avg=0.92\n",
      "[2756 | 2593.24] loss=0.95 avg=0.92\n",
      "[2757 | 2594.83] loss=0.91 avg=0.92\n",
      "[2758 | 2596.42] loss=1.44 avg=0.92\n",
      "[2759 | 2598.01] loss=0.93 avg=0.92\n",
      "[2760 | 2599.60] loss=0.83 avg=0.92\n",
      "[2761 | 2601.19] loss=0.77 avg=0.92\n",
      "[2762 | 2602.78] loss=1.09 avg=0.92\n",
      "[2763 | 2604.37] loss=0.61 avg=0.92\n",
      "[2764 | 2605.97] loss=0.87 avg=0.92\n",
      "[2765 | 2607.56] loss=1.39 avg=0.92\n",
      "[2766 | 2609.15] loss=0.60 avg=0.92\n",
      "[2767 | 2610.75] loss=0.82 avg=0.92\n",
      "[2768 | 2612.35] loss=0.90 avg=0.92\n",
      "[2769 | 2613.94] loss=0.86 avg=0.92\n",
      "[2770 | 2615.53] loss=0.46 avg=0.91\n",
      "[2771 | 2617.12] loss=1.27 avg=0.92\n",
      "[2772 | 2618.72] loss=0.48 avg=0.91\n",
      "[2773 | 2620.31] loss=0.74 avg=0.91\n",
      "[2774 | 2621.90] loss=0.91 avg=0.91\n",
      "[2775 | 2623.50] loss=1.35 avg=0.92\n",
      "[2776 | 2625.09] loss=0.83 avg=0.91\n",
      "[2777 | 2626.68] loss=0.79 avg=0.91\n",
      "[2778 | 2628.28] loss=0.78 avg=0.91\n",
      "[2779 | 2629.87] loss=1.61 avg=0.92\n",
      "[2780 | 2631.46] loss=0.41 avg=0.91\n",
      "[2781 | 2633.05] loss=0.40 avg=0.91\n",
      "[2782 | 2634.64] loss=1.12 avg=0.91\n",
      "[2783 | 2636.24] loss=0.77 avg=0.91\n",
      "[2784 | 2637.83] loss=0.89 avg=0.91\n",
      "[2785 | 2639.43] loss=0.63 avg=0.91\n",
      "[2786 | 2641.01] loss=1.45 avg=0.91\n",
      "[2787 | 2642.61] loss=0.94 avg=0.91\n",
      "[2788 | 2644.20] loss=0.86 avg=0.91\n",
      "[2789 | 2645.80] loss=0.85 avg=0.91\n",
      "[2790 | 2647.39] loss=1.07 avg=0.91\n",
      "[2791 | 2648.99] loss=0.25 avg=0.91\n",
      "[2792 | 2650.58] loss=1.59 avg=0.91\n",
      "[2793 | 2652.18] loss=1.24 avg=0.92\n",
      "[2794 | 2653.77] loss=0.98 avg=0.92\n",
      "[2795 | 2655.37] loss=1.65 avg=0.92\n",
      "[2796 | 2656.96] loss=1.11 avg=0.93\n",
      "[2797 | 2658.56] loss=0.74 avg=0.92\n",
      "[2798 | 2660.16] loss=1.70 avg=0.93\n",
      "[2799 | 2661.75] loss=0.63 avg=0.93\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " }\n",
      "    private System.Event.ActionEvent event;\n",
      "\n",
      "    /* \n",
      "     static void resetState() {\n",
      "       \n",
      "        if (this.isStateSet())\n",
      "          {\n",
      "\n",
      "        }\n",
      "     }\n",
      "\n",
      "    /* \n",
      "     static System.Object newState = newState()\n",
      "           \n",
      "        if (getState() != null)\n",
      "         {\n",
      "          if (newState != null)\n",
      "         {\n",
      "           if (newState.isChanged())\n",
      "           {\n",
      "            if (newState.isChanged())\n",
      "            getState().resetState();\n",
      "            getEventCount();\n",
      "          }\n",
      "\n",
      "\n",
      "        } else {\n",
      "        getState().resetState();\n",
      "        getEventCount();\n",
      "       }\n",
      "\n",
      "\n",
      "     }\n",
      "}\n",
      "<|endoftext|>package org.lwjgl.glew.surface.draw.draw;\n",
      "\n",
      "import java.util.Collections;\n",
      "\n",
      "public class DrawScene extends BaseScene {\n",
      "     /* \n",
      "      static void resetState() {\n",
      "       \n",
      "       if (this.isStateSet())\n",
      "         {\n",
      "          System.exit();\n",
      "       }\n",
      "\n",
      "       getState() = null;\n",
      "       setState(GetState(), \"draw\", \"draw\");\n",
      "     }\n",
      "    }\n",
      "\n",
      "    /* \n",
      "     static void setState(GetState());\n",
      "     *                 \n",
      "     */\n",
      "    public void setState(GetState());\n",
      "\n",
      "    /**\n",
      "      * @throws IllegalStateException if getState() is not set\n",
      "      */\n",
      "    public void setState(SetState());\n",
      "}\n",
      "<|endoftext|>package org.lwjgl.glew.surface.draw.draw.draw;\n",
      "\n",
      "import java.util.Collections;\n",
      "\n",
      "import org.lwjgl.glew.draw.draw.draw.Draw;\n",
      "\n",
      "import java.awt.image.Image;\n",
      "import java.awt.event.ActionEvent;\n",
      "import java.awt.event.ActionListener;\n",
      "import java.awt.event.ActionListenerEvent;\n",
      "import java.awt.event.ActionListener;\n",
      "\n",
      "import org.lwjgl.glew.glew.surface.surface.surface.Draw;\n",
      "import org.lwjgl.glew.glew.surface.surface.surface.surface.Drawable;\n",
      "import org.lwjgl.glew.glew.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface.surface\n",
      "\n",
      "[2800 | 2685.15] loss=0.67 avg=0.93\n",
      "[2801 | 2686.74] loss=0.44 avg=0.92\n",
      "[2802 | 2688.33] loss=0.90 avg=0.92\n",
      "[2803 | 2689.93] loss=0.66 avg=0.92\n",
      "[2804 | 2691.52] loss=1.04 avg=0.92\n",
      "[2805 | 2693.11] loss=0.26 avg=0.91\n",
      "[2806 | 2694.71] loss=0.62 avg=0.91\n",
      "[2807 | 2696.30] loss=0.96 avg=0.91\n",
      "[2808 | 2697.88] loss=0.80 avg=0.91\n",
      "[2809 | 2699.48] loss=1.26 avg=0.91\n",
      "[2810 | 2701.07] loss=0.88 avg=0.91\n",
      "[2811 | 2702.66] loss=0.79 avg=0.91\n",
      "[2812 | 2704.25] loss=1.07 avg=0.91\n",
      "[2813 | 2705.84] loss=1.24 avg=0.92\n",
      "[2814 | 2707.43] loss=0.72 avg=0.91\n",
      "[2815 | 2709.03] loss=0.95 avg=0.91\n",
      "[2816 | 2710.62] loss=1.11 avg=0.92\n",
      "[2817 | 2712.22] loss=1.05 avg=0.92\n",
      "[2818 | 2713.81] loss=0.58 avg=0.91\n",
      "[2819 | 2715.40] loss=0.42 avg=0.91\n",
      "[2820 | 2716.99] loss=0.42 avg=0.90\n",
      "[2821 | 2718.59] loss=0.99 avg=0.91\n",
      "[2822 | 2720.18] loss=0.36 avg=0.90\n",
      "[2823 | 2721.78] loss=0.61 avg=0.90\n",
      "[2824 | 2723.38] loss=0.90 avg=0.90\n",
      "[2825 | 2724.97] loss=0.69 avg=0.89\n",
      "[2826 | 2726.57] loss=0.82 avg=0.89\n",
      "[2827 | 2728.17] loss=0.65 avg=0.89\n",
      "[2828 | 2729.77] loss=0.91 avg=0.89\n",
      "[2829 | 2731.37] loss=0.78 avg=0.89\n",
      "[2830 | 2732.97] loss=1.04 avg=0.89\n",
      "[2831 | 2734.57] loss=1.20 avg=0.90\n",
      "[2832 | 2736.17] loss=1.31 avg=0.90\n",
      "[2833 | 2737.77] loss=0.70 avg=0.90\n",
      "[2834 | 2739.36] loss=1.08 avg=0.90\n",
      "[2835 | 2740.96] loss=0.62 avg=0.90\n",
      "[2836 | 2742.56] loss=0.90 avg=0.90\n",
      "[2837 | 2744.16] loss=0.92 avg=0.90\n",
      "[2838 | 2745.76] loss=0.89 avg=0.90\n",
      "[2839 | 2747.36] loss=1.18 avg=0.90\n",
      "[2840 | 2748.95] loss=0.77 avg=0.90\n",
      "[2841 | 2750.56] loss=0.97 avg=0.90\n",
      "[2842 | 2752.15] loss=1.09 avg=0.90\n",
      "[2843 | 2753.75] loss=1.16 avg=0.90\n",
      "[2844 | 2755.35] loss=1.09 avg=0.91\n",
      "[2845 | 2756.94] loss=0.88 avg=0.90\n",
      "[2846 | 2758.54] loss=0.41 avg=0.90\n",
      "[2847 | 2760.14] loss=0.97 avg=0.90\n",
      "[2848 | 2761.74] loss=0.43 avg=0.90\n",
      "[2849 | 2763.33] loss=1.13 avg=0.90\n",
      "[2850 | 2764.93] loss=0.82 avg=0.90\n",
      "[2851 | 2766.52] loss=0.55 avg=0.89\n",
      "[2852 | 2768.11] loss=1.13 avg=0.90\n",
      "[2853 | 2769.70] loss=0.98 avg=0.90\n",
      "[2854 | 2771.30] loss=0.83 avg=0.90\n",
      "[2855 | 2772.89] loss=1.30 avg=0.90\n",
      "[2856 | 2774.49] loss=1.00 avg=0.90\n",
      "[2857 | 2776.09] loss=0.16 avg=0.89\n",
      "[2858 | 2777.68] loss=0.88 avg=0.89\n",
      "[2859 | 2779.27] loss=0.68 avg=0.89\n",
      "[2860 | 2780.86] loss=0.65 avg=0.89\n",
      "[2861 | 2782.46] loss=0.47 avg=0.89\n",
      "[2862 | 2784.06] loss=0.57 avg=0.88\n",
      "[2863 | 2785.65] loss=0.84 avg=0.88\n",
      "[2864 | 2787.25] loss=1.34 avg=0.89\n",
      "[2865 | 2788.84] loss=0.70 avg=0.88\n",
      "[2866 | 2790.43] loss=0.27 avg=0.88\n",
      "[2867 | 2792.02] loss=1.18 avg=0.88\n",
      "[2868 | 2793.61] loss=0.82 avg=0.88\n",
      "[2869 | 2795.20] loss=0.77 avg=0.88\n",
      "[2870 | 2796.79] loss=0.31 avg=0.87\n",
      "[2871 | 2798.38] loss=1.50 avg=0.88\n",
      "[2872 | 2799.97] loss=0.56 avg=0.88\n",
      "[2873 | 2801.56] loss=1.05 avg=0.88\n",
      "[2874 | 2803.15] loss=0.75 avg=0.88\n",
      "[2875 | 2804.74] loss=0.84 avg=0.88\n",
      "[2876 | 2806.33] loss=1.01 avg=0.88\n",
      "[2877 | 2807.93] loss=0.65 avg=0.88\n",
      "[2878 | 2809.53] loss=1.77 avg=0.89\n",
      "[2879 | 2811.11] loss=1.07 avg=0.89\n",
      "[2880 | 2812.70] loss=1.08 avg=0.89\n",
      "[2881 | 2814.29] loss=1.33 avg=0.89\n",
      "[2882 | 2815.88] loss=1.28 avg=0.90\n",
      "[2883 | 2817.48] loss=1.16 avg=0.90\n",
      "[2884 | 2819.07] loss=0.93 avg=0.90\n",
      "[2885 | 2820.66] loss=0.95 avg=0.90\n",
      "[2886 | 2822.26] loss=0.70 avg=0.90\n",
      "[2887 | 2823.85] loss=1.17 avg=0.90\n",
      "[2888 | 2825.44] loss=0.80 avg=0.90\n",
      "[2889 | 2827.04] loss=0.44 avg=0.90\n",
      "[2890 | 2828.63] loss=0.85 avg=0.90\n",
      "[2891 | 2830.21] loss=0.86 avg=0.89\n",
      "[2892 | 2831.81] loss=0.84 avg=0.89\n",
      "[2893 | 2833.40] loss=1.11 avg=0.90\n",
      "[2894 | 2835.00] loss=0.62 avg=0.89\n",
      "[2895 | 2836.59] loss=0.78 avg=0.89\n",
      "[2896 | 2838.19] loss=0.60 avg=0.89\n",
      "[2897 | 2839.77] loss=0.78 avg=0.89\n",
      "[2898 | 2841.36] loss=1.05 avg=0.89\n",
      "[2899 | 2842.96] loss=0.68 avg=0.89\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " - *\n",
      "     * @throws IllegalStateException\n",
      "     *           if another function (or method) in this code or\n",
      "     *               this class or interface is using non-Java type\n",
      "     *                              methods or non-Java objects (classes) with\n",
      "     *                            non-Java method body)\n",
      "     *                            non-Java construct.\n",
      "     * @see #getInstance()\n",
      "     * @see #getClassMembers\n",
      "     * @see #getTypeNames ()\n",
      "     * @see #getMethodNames()\n",
      "     * @see #getParameterTypes()\n",
      "     * @see #getClassName()\n",
      "     *@see #getConstructors()\n",
      "     * @see #getInitializers()\n",
      "     * @see #getConstructorParameters()\n",
      "     * @see #getMethods()\n",
      "     * @see #getConstructorsParameters\n",
      "     * @see #getConstructorParameterNames()\n",
      "     * @see #setConstructorParameters(boolean)\n",
      "     * @see #setConstructorParameters(boolean)\n",
      "     * @see #setConstructorParameterNames(char)\n",
      "     * @see #setConstructorsParameters(char, boolean)\n",
      "     * @see #setConstructorParameterNames(char[])\n",
      "     * @see #setConstructorsParameters(ArrayList)\n",
      "     * @see #setConstructorParameterNames(ArrayList, boolean)\n",
      "     * @see #setConstructorParameterNames(ArrayList[])\n",
      "     * @see #setConstructorParameterNames(ArrayList[, boolean)\n",
      "     * @see #setConstructorParameterName(String)\n",
      "     * @see #setConstructorParameterNames(String, boolean)\n",
      "     * @see #setConstructorParameterName(String, void)\n",
      "     * @see #setConstructorParameterName(String, boolean)\n",
      "     * @see #setConstructorParameterName(String, ClassName)\n",
      "     * @see #setConstructorParameterName(String, boolean)\n",
      "     * @see #setConstructorParameterName(String, String)\n",
      "     * @see #setConstructorParameterName(String, Object)\n",
      "     * @see #setConstructorParameterName(String, Object, String)\n",
      "     * @see #setCachedConstructorName(boolean)\n",
      "     * @see #setCachedConstructorName(boolean)\n",
      "     * @see #setCachedConstructorName(boolean, boolean)\n",
      "     * @see #setCachedConstructorName(boolean, void)\n",
      "     * @see #setCachedConstructorName(boolean, void, String)\n",
      "     * @see #setCachedConstructorName(void)\n",
      "     * @see #setCachedConstructorName(void, Boolean)\n",
      "     * @see #setCachedConstructorName(void, Boolean, String)\n",
      "     * @see #setCachedConstructorName(void, String, Boolean)\n",
      "     * @see #setCachedConstructorName(String, Boolean, String)\n",
      "     * @see #setCachedConstructorName(String, Boolean, String[])\n",
      "     * @see #setCachedConstructorName(String, Boolean, String[, Object[])\n",
      "     * @see #setCachedConstructorName(String, Object[, Object[, String]), boolean)\n",
      "     * @see #setCachedConstructorName(String, Object[, String]), class)\n",
      "     * @see #setCachedConstructorName(String, String, class)\n",
      "     *\n",
      "\n",
      "[2900 | 2866.48] loss=1.60 avg=0.90\n",
      "[2901 | 2868.07] loss=1.06 avg=0.90\n",
      "[2902 | 2869.66] loss=0.60 avg=0.89\n",
      "[2903 | 2871.26] loss=1.50 avg=0.90\n",
      "[2904 | 2872.85] loss=0.95 avg=0.90\n",
      "[2905 | 2874.45] loss=0.86 avg=0.90\n",
      "[2906 | 2876.04] loss=0.58 avg=0.90\n",
      "[2907 | 2877.63] loss=1.25 avg=0.90\n",
      "[2908 | 2879.22] loss=0.69 avg=0.90\n",
      "[2909 | 2880.81] loss=0.69 avg=0.90\n",
      "[2910 | 2882.40] loss=1.13 avg=0.90\n",
      "[2911 | 2883.99] loss=2.00 avg=0.91\n",
      "[2912 | 2885.58] loss=1.25 avg=0.91\n",
      "[2913 | 2887.17] loss=0.73 avg=0.91\n",
      "[2914 | 2888.76] loss=0.80 avg=0.91\n",
      "[2915 | 2890.35] loss=0.81 avg=0.91\n",
      "[2916 | 2891.94] loss=0.88 avg=0.91\n",
      "[2917 | 2893.54] loss=0.90 avg=0.91\n",
      "[2918 | 2895.13] loss=1.02 avg=0.91\n",
      "[2919 | 2896.73] loss=0.86 avg=0.91\n",
      "[2920 | 2898.32] loss=0.89 avg=0.91\n",
      "[2921 | 2899.92] loss=1.00 avg=0.91\n",
      "[2922 | 2901.51] loss=0.99 avg=0.91\n",
      "[2923 | 2903.10] loss=0.76 avg=0.91\n",
      "[2924 | 2904.70] loss=1.24 avg=0.91\n",
      "[2925 | 2906.29] loss=0.99 avg=0.91\n",
      "[2926 | 2907.89] loss=1.08 avg=0.91\n",
      "[2927 | 2909.48] loss=0.84 avg=0.91\n",
      "[2928 | 2911.07] loss=0.48 avg=0.91\n",
      "[2929 | 2912.66] loss=0.80 avg=0.91\n",
      "[2930 | 2914.26] loss=1.90 avg=0.92\n",
      "[2931 | 2915.85] loss=0.73 avg=0.92\n",
      "[2932 | 2917.44] loss=0.52 avg=0.91\n",
      "[2933 | 2919.03] loss=0.97 avg=0.91\n",
      "[2934 | 2920.63] loss=0.65 avg=0.91\n",
      "[2935 | 2922.22] loss=0.76 avg=0.91\n",
      "[2936 | 2923.81] loss=0.85 avg=0.91\n",
      "[2937 | 2925.40] loss=1.12 avg=0.91\n",
      "[2938 | 2927.00] loss=0.91 avg=0.91\n",
      "[2939 | 2928.59] loss=1.08 avg=0.91\n",
      "[2940 | 2930.19] loss=1.50 avg=0.92\n",
      "[2941 | 2931.78] loss=0.98 avg=0.92\n",
      "[2942 | 2933.38] loss=1.04 avg=0.92\n",
      "[2943 | 2934.98] loss=1.52 avg=0.93\n",
      "[2944 | 2936.57] loss=1.07 avg=0.93\n",
      "[2945 | 2938.17] loss=0.74 avg=0.93\n",
      "[2946 | 2939.77] loss=0.69 avg=0.92\n",
      "[2947 | 2941.36] loss=1.20 avg=0.93\n",
      "[2948 | 2942.96] loss=0.96 avg=0.93\n",
      "[2949 | 2944.55] loss=0.60 avg=0.92\n",
      "[2950 | 2946.14] loss=1.18 avg=0.93\n",
      "[2951 | 2947.73] loss=0.56 avg=0.92\n",
      "[2952 | 2949.33] loss=1.21 avg=0.93\n",
      "[2953 | 2950.92] loss=0.79 avg=0.92\n",
      "[2954 | 2952.52] loss=0.18 avg=0.92\n",
      "[2955 | 2954.11] loss=0.58 avg=0.91\n",
      "[2956 | 2955.70] loss=0.64 avg=0.91\n",
      "[2957 | 2957.29] loss=0.67 avg=0.91\n",
      "[2958 | 2958.89] loss=0.38 avg=0.90\n",
      "[2959 | 2960.48] loss=0.82 avg=0.90\n",
      "[2960 | 2962.07] loss=1.36 avg=0.91\n",
      "[2961 | 2963.67] loss=0.49 avg=0.90\n",
      "[2962 | 2965.26] loss=0.83 avg=0.90\n",
      "[2963 | 2966.85] loss=1.28 avg=0.91\n",
      "[2964 | 2968.45] loss=0.74 avg=0.90\n",
      "[2965 | 2970.04] loss=0.88 avg=0.90\n",
      "[2966 | 2971.63] loss=0.46 avg=0.90\n",
      "[2967 | 2973.23] loss=1.04 avg=0.90\n",
      "[2968 | 2974.83] loss=1.31 avg=0.90\n",
      "[2969 | 2976.42] loss=0.60 avg=0.90\n",
      "[2970 | 2978.01] loss=0.54 avg=0.90\n",
      "[2971 | 2979.61] loss=1.15 avg=0.90\n",
      "[2972 | 2981.21] loss=0.39 avg=0.90\n",
      "[2973 | 2982.81] loss=1.22 avg=0.90\n",
      "[2974 | 2984.40] loss=0.59 avg=0.90\n",
      "[2975 | 2986.00] loss=0.60 avg=0.89\n",
      "[2976 | 2987.60] loss=0.76 avg=0.89\n",
      "[2977 | 2989.19] loss=0.58 avg=0.89\n",
      "[2978 | 2990.79] loss=1.26 avg=0.89\n",
      "[2979 | 2992.38] loss=0.69 avg=0.89\n",
      "[2980 | 2993.97] loss=1.93 avg=0.90\n",
      "[2981 | 2995.57] loss=1.20 avg=0.90\n",
      "[2982 | 2997.17] loss=0.94 avg=0.90\n",
      "[2983 | 2998.76] loss=1.00 avg=0.90\n",
      "[2984 | 3000.35] loss=1.24 avg=0.91\n",
      "[2985 | 3001.95] loss=0.66 avg=0.91\n",
      "[2986 | 3003.54] loss=0.89 avg=0.91\n",
      "[2987 | 3005.14] loss=1.01 avg=0.91\n",
      "[2988 | 3006.74] loss=0.61 avg=0.90\n",
      "[2989 | 3008.33] loss=1.23 avg=0.91\n",
      "[2990 | 3009.93] loss=1.09 avg=0.91\n",
      "[2991 | 3011.52] loss=0.52 avg=0.90\n",
      "[2992 | 3013.12] loss=0.62 avg=0.90\n",
      "[2993 | 3014.71] loss=1.05 avg=0.90\n",
      "[2994 | 3016.31] loss=0.79 avg=0.90\n",
      "[2995 | 3017.90] loss=0.95 avg=0.90\n",
      "[2996 | 3019.50] loss=1.15 avg=0.90\n",
      "[2997 | 3021.10] loss=1.17 avg=0.91\n",
      "[2998 | 3022.69] loss=1.65 avg=0.92\n",
      "[2999 | 3024.29] loss=0.99 avg=0.92\n",
      "Saving checkpoint/run1/model-3000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " return\n",
      "                                                                                                                                                                                                                    \"\",\"displayOrder\"\n",
      "                                                                                                                                                                                                                                                                                                                       \"\"),\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
      "\n",
      "[3000 | 3059.07] loss=1.08 avg=0.92\n",
      "[3001 | 3060.72] loss=1.33 avg=0.92\n",
      "[3002 | 3062.36] loss=0.98 avg=0.92\n",
      "[3003 | 3063.99] loss=1.24 avg=0.93\n",
      "[3004 | 3065.63] loss=0.72 avg=0.92\n",
      "[3005 | 3067.27] loss=1.02 avg=0.92\n",
      "[3006 | 3068.89] loss=1.41 avg=0.93\n",
      "[3007 | 3070.51] loss=0.72 avg=0.93\n",
      "[3008 | 3072.13] loss=1.13 avg=0.93\n",
      "[3009 | 3073.74] loss=1.05 avg=0.93\n",
      "[3010 | 3075.35] loss=1.49 avg=0.94\n",
      "[3011 | 3076.96] loss=1.45 avg=0.94\n",
      "[3012 | 3078.56] loss=0.60 avg=0.94\n",
      "[3013 | 3080.16] loss=0.80 avg=0.94\n",
      "[3014 | 3081.76] loss=0.48 avg=0.93\n",
      "[3015 | 3083.36] loss=1.37 avg=0.94\n",
      "[3016 | 3084.96] loss=0.41 avg=0.93\n",
      "[3017 | 3086.55] loss=0.75 avg=0.93\n",
      "[3018 | 3088.15] loss=1.01 avg=0.93\n",
      "[3019 | 3089.74] loss=1.03 avg=0.93\n",
      "[3020 | 3091.33] loss=0.99 avg=0.93\n",
      "[3021 | 3092.92] loss=0.74 avg=0.93\n",
      "[3022 | 3094.50] loss=1.15 avg=0.93\n",
      "[3023 | 3096.09] loss=0.94 avg=0.93\n",
      "[3024 | 3097.68] loss=0.77 avg=0.93\n",
      "[3025 | 3099.26] loss=0.64 avg=0.93\n",
      "[3026 | 3100.84] loss=1.05 avg=0.93\n",
      "[3027 | 3102.43] loss=1.26 avg=0.93\n",
      "[3028 | 3104.01] loss=1.11 avg=0.93\n",
      "[3029 | 3105.59] loss=0.74 avg=0.93\n",
      "[3030 | 3107.18] loss=0.55 avg=0.93\n",
      "[3031 | 3108.77] loss=0.69 avg=0.93\n",
      "[3032 | 3110.35] loss=0.76 avg=0.92\n",
      "[3033 | 3111.93] loss=0.53 avg=0.92\n",
      "[3034 | 3113.52] loss=1.25 avg=0.92\n",
      "[3035 | 3115.10] loss=0.46 avg=0.92\n",
      "[3036 | 3116.69] loss=0.81 avg=0.92\n",
      "[3037 | 3118.28] loss=0.76 avg=0.92\n",
      "[3038 | 3119.87] loss=1.06 avg=0.92\n",
      "[3039 | 3121.46] loss=0.92 avg=0.92\n",
      "[3040 | 3123.05] loss=0.72 avg=0.92\n",
      "[3041 | 3124.64] loss=0.64 avg=0.91\n",
      "[3042 | 3126.24] loss=1.03 avg=0.91\n",
      "[3043 | 3127.83] loss=1.11 avg=0.92\n",
      "[3044 | 3129.42] loss=0.49 avg=0.91\n",
      "[3045 | 3131.02] loss=1.52 avg=0.92\n",
      "[3046 | 3132.62] loss=1.69 avg=0.93\n",
      "[3047 | 3134.22] loss=0.82 avg=0.92\n",
      "[3048 | 3135.82] loss=0.65 avg=0.92\n",
      "[3049 | 3137.42] loss=0.57 avg=0.92\n",
      "[3050 | 3139.02] loss=1.10 avg=0.92\n",
      "[3051 | 3140.62] loss=1.19 avg=0.92\n",
      "[3052 | 3142.22] loss=0.80 avg=0.92\n",
      "[3053 | 3143.82] loss=1.20 avg=0.92\n",
      "[3054 | 3145.42] loss=0.93 avg=0.92\n",
      "[3055 | 3147.02] loss=0.70 avg=0.92\n",
      "[3056 | 3148.63] loss=0.87 avg=0.92\n",
      "[3057 | 3150.22] loss=1.17 avg=0.92\n",
      "[3058 | 3151.82] loss=0.97 avg=0.92\n",
      "[3059 | 3153.42] loss=0.73 avg=0.92\n",
      "[3060 | 3155.02] loss=0.39 avg=0.92\n",
      "[3061 | 3156.62] loss=0.86 avg=0.92\n",
      "[3062 | 3158.22] loss=2.33 avg=0.93\n",
      "[3063 | 3159.82] loss=0.42 avg=0.93\n",
      "[3064 | 3161.42] loss=0.75 avg=0.92\n",
      "[3065 | 3163.01] loss=1.06 avg=0.93\n",
      "[3066 | 3164.60] loss=1.05 avg=0.93\n",
      "[3067 | 3166.20] loss=0.96 avg=0.93\n",
      "[3068 | 3167.80] loss=1.16 avg=0.93\n",
      "[3069 | 3169.40] loss=1.23 avg=0.93\n",
      "[3070 | 3170.99] loss=0.41 avg=0.93\n",
      "[3071 | 3172.59] loss=1.09 avg=0.93\n",
      "[3072 | 3174.19] loss=0.66 avg=0.93\n",
      "[3073 | 3175.79] loss=0.43 avg=0.92\n",
      "[3074 | 3177.38] loss=1.18 avg=0.92\n",
      "[3075 | 3178.98] loss=0.54 avg=0.92\n",
      "[3076 | 3180.58] loss=0.64 avg=0.92\n",
      "[3077 | 3182.17] loss=0.62 avg=0.91\n",
      "[3078 | 3183.77] loss=1.25 avg=0.92\n",
      "[3079 | 3185.37] loss=0.37 avg=0.91\n",
      "[3080 | 3186.96] loss=1.68 avg=0.92\n",
      "[3081 | 3188.56] loss=1.02 avg=0.92\n",
      "[3082 | 3190.16] loss=0.63 avg=0.92\n",
      "[3083 | 3191.76] loss=1.00 avg=0.92\n",
      "[3084 | 3193.35] loss=0.95 avg=0.92\n",
      "[3085 | 3194.95] loss=1.76 avg=0.93\n",
      "[3086 | 3196.54] loss=1.13 avg=0.93\n",
      "[3087 | 3198.14] loss=1.12 avg=0.93\n",
      "[3088 | 3199.74] loss=1.00 avg=0.93\n",
      "[3089 | 3201.33] loss=1.08 avg=0.93\n",
      "[3090 | 3202.93] loss=1.11 avg=0.93\n",
      "[3091 | 3204.52] loss=0.85 avg=0.93\n",
      "[3092 | 3206.12] loss=0.94 avg=0.93\n",
      "[3093 | 3207.72] loss=0.81 avg=0.93\n",
      "[3094 | 3209.31] loss=0.69 avg=0.93\n",
      "[3095 | 3210.91] loss=0.68 avg=0.93\n",
      "[3096 | 3212.50] loss=0.45 avg=0.92\n",
      "[3097 | 3214.09] loss=0.75 avg=0.92\n",
      "[3098 | 3215.69] loss=0.77 avg=0.92\n",
      "[3099 | 3217.29] loss=0.43 avg=0.92\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "\t@class}\\u0000@\\u00a0\\u0000@\\u0000@\\u0000@\\u0000@\\u00b2\\u00b7\\u00bd\\u00bf\\u0005\\u00c1\\u0006\\u0007\\u0000@\\u001a\\u000j\\u001b\\u000c\\u000d\\u000e\\u000f\\u0005\\u0000@\\u0000\\u0000@\\u0000@\\u0000@\\u000c\\u000f\\u0003\\u0000\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000\n",
      "\n",
      "\\u0000\\u0000@\\u000c\\u000f\\u0003\\u0000\\u0000@\\u0000@\\u0000\\u0000@\\u0000@\\u0000@\\u0000@\\u0000@\\u0000\\u0000\n",
      "\\u0000\\u0000@\\u000f\\u000f\\u000c\\u000f\\u000b\\u000c\\u000c\\u000f\\u000b\\u000e\\u000f\n",
      "\\u0000\\u0000@\\u0000\n",
      "\\u0000\\u0000\\u0000\\u0000\\u0000\n",
      "\n",
      "\n",
      "\\u000f\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\n",
      "\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\n",
      "\n",
      "\n",
      "\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0001\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u000f\\u000f\n",
      "\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000 \\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u1000\\u0000\\u0000\\u000f\\u0000\\u0000 \\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000 \\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u000f\\u000f\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u1000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u000f\\u00c1\\u000f\\u000f\\u000f\\u0000\\u0000 \\u0000\\u0000\\u0000\\u0000\\u0000\\u8000\\u0000\\u0000\\u0000\\u7c7\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u00b3\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u00b3\\u0000\\u0000\\u0000\\u0000\\u0000\\u0000\\u000f\\u000f \\u0000\\u0000\\u0000\n",
      "\n",
      "[3100 | 3240.66] loss=0.68 avg=0.91\n",
      "[3101 | 3242.25] loss=1.13 avg=0.91\n",
      "[3102 | 3243.84] loss=0.81 avg=0.91\n",
      "[3103 | 3245.44] loss=0.79 avg=0.91\n",
      "[3104 | 3247.03] loss=0.76 avg=0.91\n",
      "[3105 | 3248.62] loss=1.08 avg=0.91\n",
      "[3106 | 3250.21] loss=1.03 avg=0.91\n",
      "[3107 | 3251.80] loss=0.81 avg=0.91\n",
      "[3108 | 3253.40] loss=0.89 avg=0.91\n",
      "[3109 | 3255.01] loss=0.83 avg=0.91\n",
      "[3110 | 3256.60] loss=0.67 avg=0.91\n",
      "[3111 | 3258.20] loss=0.82 avg=0.91\n",
      "[3112 | 3259.79] loss=1.40 avg=0.91\n",
      "[3113 | 3261.39] loss=0.44 avg=0.91\n",
      "[3114 | 3262.98] loss=0.56 avg=0.91\n",
      "[3115 | 3264.58] loss=0.95 avg=0.91\n",
      "[3116 | 3266.17] loss=0.81 avg=0.90\n",
      "[3117 | 3267.76] loss=1.59 avg=0.91\n",
      "[3118 | 3269.35] loss=0.82 avg=0.91\n",
      "[3119 | 3270.93] loss=1.07 avg=0.91\n",
      "[3120 | 3272.52] loss=0.98 avg=0.91\n",
      "[3121 | 3274.11] loss=1.30 avg=0.92\n",
      "[3122 | 3275.70] loss=0.67 avg=0.91\n",
      "[3123 | 3277.29] loss=0.80 avg=0.91\n",
      "[3124 | 3278.88] loss=0.45 avg=0.91\n",
      "[3125 | 3280.47] loss=0.80 avg=0.91\n",
      "[3126 | 3282.07] loss=1.34 avg=0.91\n",
      "[3127 | 3283.66] loss=0.81 avg=0.91\n",
      "[3128 | 3285.25] loss=0.48 avg=0.91\n",
      "[3129 | 3286.84] loss=0.80 avg=0.91\n",
      "[3130 | 3288.43] loss=0.84 avg=0.90\n",
      "[3131 | 3290.02] loss=0.58 avg=0.90\n",
      "[3132 | 3291.62] loss=1.45 avg=0.91\n",
      "[3133 | 3293.21] loss=0.90 avg=0.91\n",
      "[3134 | 3294.80] loss=0.72 avg=0.91\n",
      "[3135 | 3296.40] loss=1.19 avg=0.91\n",
      "[3136 | 3297.98] loss=1.61 avg=0.92\n",
      "[3137 | 3299.58] loss=1.07 avg=0.92\n",
      "[3138 | 3301.18] loss=0.43 avg=0.91\n",
      "[3139 | 3302.77] loss=1.16 avg=0.91\n",
      "[3140 | 3304.37] loss=0.84 avg=0.91\n",
      "[3141 | 3305.96] loss=0.91 avg=0.91\n",
      "[3142 | 3307.56] loss=0.94 avg=0.91\n",
      "[3143 | 3309.15] loss=0.84 avg=0.91\n",
      "[3144 | 3310.75] loss=1.10 avg=0.91\n",
      "[3145 | 3312.35] loss=0.92 avg=0.91\n",
      "[3146 | 3313.94] loss=1.56 avg=0.92\n",
      "[3147 | 3315.53] loss=0.54 avg=0.92\n",
      "[3148 | 3317.13] loss=0.37 avg=0.91\n",
      "[3149 | 3318.72] loss=0.41 avg=0.91\n",
      "[3150 | 3320.32] loss=1.01 avg=0.91\n",
      "[3151 | 3321.92] loss=1.28 avg=0.91\n",
      "[3152 | 3323.51] loss=1.31 avg=0.92\n",
      "[3153 | 3325.11] loss=0.87 avg=0.92\n",
      "[3154 | 3326.71] loss=0.35 avg=0.91\n",
      "[3155 | 3328.31] loss=1.25 avg=0.91\n",
      "[3156 | 3329.90] loss=0.81 avg=0.91\n",
      "[3157 | 3331.49] loss=0.55 avg=0.91\n",
      "[3158 | 3333.09] loss=0.46 avg=0.90\n",
      "[3159 | 3334.68] loss=0.82 avg=0.90\n",
      "[3160 | 3336.28] loss=1.22 avg=0.91\n",
      "[3161 | 3337.88] loss=1.08 avg=0.91\n",
      "[3162 | 3339.48] loss=0.85 avg=0.91\n",
      "[3163 | 3341.07] loss=0.93 avg=0.91\n",
      "[3164 | 3342.68] loss=0.64 avg=0.91\n",
      "[3165 | 3344.27] loss=0.78 avg=0.90\n",
      "[3166 | 3345.87] loss=0.96 avg=0.90\n",
      "[3167 | 3347.47] loss=1.31 avg=0.91\n",
      "[3168 | 3349.07] loss=0.74 avg=0.91\n",
      "[3169 | 3350.67] loss=1.05 avg=0.91\n",
      "[3170 | 3352.27] loss=1.01 avg=0.91\n",
      "[3171 | 3353.86] loss=1.08 avg=0.91\n",
      "[3172 | 3355.46] loss=0.97 avg=0.91\n",
      "[3173 | 3357.05] loss=0.34 avg=0.91\n",
      "[3174 | 3358.65] loss=0.73 avg=0.90\n",
      "[3175 | 3360.24] loss=0.32 avg=0.90\n",
      "[3176 | 3361.84] loss=0.80 avg=0.90\n",
      "[3177 | 3363.44] loss=0.80 avg=0.90\n",
      "[3178 | 3365.04] loss=1.05 avg=0.90\n",
      "[3179 | 3366.63] loss=0.59 avg=0.89\n",
      "[3180 | 3368.24] loss=0.46 avg=0.89\n",
      "[3181 | 3369.83] loss=0.98 avg=0.89\n",
      "[3182 | 3371.43] loss=1.17 avg=0.89\n",
      "[3183 | 3373.03] loss=0.94 avg=0.89\n",
      "[3184 | 3374.63] loss=1.67 avg=0.90\n",
      "[3185 | 3376.23] loss=0.78 avg=0.90\n",
      "[3186 | 3377.83] loss=1.09 avg=0.90\n",
      "[3187 | 3379.43] loss=0.86 avg=0.90\n",
      "[3188 | 3381.03] loss=0.73 avg=0.90\n",
      "[3189 | 3382.62] loss=0.48 avg=0.90\n",
      "[3190 | 3384.22] loss=0.52 avg=0.89\n",
      "[3191 | 3385.82] loss=1.20 avg=0.90\n",
      "[3192 | 3387.42] loss=0.48 avg=0.89\n",
      "[3193 | 3389.02] loss=1.66 avg=0.90\n",
      "[3194 | 3390.62] loss=0.76 avg=0.90\n",
      "[3195 | 3392.22] loss=0.38 avg=0.89\n",
      "[3196 | 3393.82] loss=1.05 avg=0.89\n",
      "[3197 | 3395.42] loss=1.01 avg=0.90\n",
      "[3198 | 3397.01] loss=0.59 avg=0.89\n",
      "[3199 | 3398.61] loss=0.56 avg=0.89\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "             if !$ret.exists()) {\n",
      "                                                    ret = new XMLValue(new JXDataParser(data.getStringValue(), data.getXMLValue().getExtentDescription()),\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     } else {\n",
      "                                                                  return data.getXMLValue().getExtentDescription();\n",
      "                                                   }\n",
      "\n",
      "                                          }\n",
      "                          }\n",
      "                 }\n",
      "              }\n",
      "         }\n",
      "         else {\n",
      "              // if there is no XML from source, convert to an empty string that will be\n",
      "              // ignored.\n",
      "             // (a) \n",
      "             if ($xmlValue is null) {\n",
      "                  return null;\n",
      "             } \n",
      "        }\n",
      "        if ($xmlValue instance\n",
      "\n",
      "[3200 | 3422.48] loss=0.57 avg=0.89\n",
      "[3201 | 3424.08] loss=0.81 avg=0.89\n",
      "[3202 | 3425.67] loss=0.37 avg=0.88\n",
      "[3203 | 3427.25] loss=0.80 avg=0.88\n",
      "[3204 | 3428.84] loss=0.35 avg=0.87\n",
      "[3205 | 3430.44] loss=1.44 avg=0.88\n",
      "[3206 | 3432.03] loss=0.60 avg=0.88\n",
      "[3207 | 3433.62] loss=0.85 avg=0.88\n",
      "[3208 | 3435.21] loss=1.38 avg=0.88\n",
      "[3209 | 3436.81] loss=0.21 avg=0.87\n",
      "[3210 | 3438.40] loss=1.01 avg=0.88\n",
      "[3211 | 3439.99] loss=0.51 avg=0.87\n",
      "[3212 | 3441.57] loss=0.43 avg=0.87\n",
      "[3213 | 3443.16] loss=0.33 avg=0.86\n",
      "[3214 | 3444.76] loss=0.67 avg=0.86\n",
      "[3215 | 3446.35] loss=0.85 avg=0.86\n",
      "[3216 | 3447.94] loss=0.56 avg=0.86\n",
      "[3217 | 3449.53] loss=0.89 avg=0.86\n",
      "[3218 | 3451.13] loss=0.62 avg=0.86\n",
      "[3219 | 3452.72] loss=1.13 avg=0.86\n",
      "[3220 | 3454.31] loss=0.81 avg=0.86\n",
      "[3221 | 3455.90] loss=0.99 avg=0.86\n",
      "[3222 | 3457.49] loss=0.84 avg=0.86\n",
      "[3223 | 3459.08] loss=0.80 avg=0.86\n",
      "[3224 | 3460.67] loss=0.59 avg=0.86\n",
      "[3225 | 3462.26] loss=0.79 avg=0.86\n",
      "[3226 | 3463.85] loss=0.39 avg=0.85\n",
      "[3227 | 3465.44] loss=1.05 avg=0.85\n",
      "[3228 | 3467.03] loss=1.03 avg=0.85\n",
      "[3229 | 3468.62] loss=0.31 avg=0.85\n",
      "[3230 | 3470.22] loss=1.11 avg=0.85\n",
      "[3231 | 3471.82] loss=0.88 avg=0.85\n",
      "[3232 | 3473.41] loss=0.71 avg=0.85\n",
      "[3233 | 3475.00] loss=0.20 avg=0.84\n",
      "[3234 | 3476.60] loss=1.29 avg=0.85\n",
      "[3235 | 3478.20] loss=0.61 avg=0.85\n",
      "[3236 | 3479.79] loss=0.16 avg=0.84\n",
      "[3237 | 3481.39] loss=0.97 avg=0.84\n",
      "[3238 | 3482.98] loss=0.67 avg=0.84\n",
      "[3239 | 3484.58] loss=1.10 avg=0.84\n",
      "[3240 | 3486.17] loss=0.68 avg=0.84\n",
      "[3241 | 3487.76] loss=0.46 avg=0.84\n",
      "[3242 | 3489.36] loss=1.50 avg=0.84\n",
      "[3243 | 3490.96] loss=0.99 avg=0.84\n",
      "[3244 | 3492.55] loss=0.80 avg=0.84\n",
      "[3245 | 3494.15] loss=0.65 avg=0.84\n",
      "[3246 | 3495.74] loss=0.56 avg=0.84\n",
      "[3247 | 3497.35] loss=0.69 avg=0.84\n",
      "[3248 | 3498.94] loss=0.81 avg=0.84\n",
      "[3249 | 3500.54] loss=0.51 avg=0.83\n",
      "[3250 | 3502.13] loss=1.51 avg=0.84\n",
      "[3251 | 3503.73] loss=0.89 avg=0.84\n",
      "[3252 | 3505.32] loss=1.24 avg=0.84\n",
      "[3253 | 3506.92] loss=1.59 avg=0.85\n",
      "[3254 | 3508.52] loss=0.92 avg=0.85\n",
      "[3255 | 3510.11] loss=1.01 avg=0.85\n",
      "[3256 | 3511.71] loss=1.30 avg=0.86\n",
      "[3257 | 3513.31] loss=0.53 avg=0.86\n",
      "[3258 | 3514.91] loss=0.85 avg=0.86\n",
      "[3259 | 3516.51] loss=0.32 avg=0.85\n",
      "[3260 | 3518.11] loss=1.03 avg=0.85\n",
      "[3261 | 3519.72] loss=0.44 avg=0.85\n",
      "[3262 | 3521.32] loss=0.84 avg=0.85\n",
      "[3263 | 3522.92] loss=0.69 avg=0.85\n",
      "[3264 | 3524.52] loss=1.05 avg=0.85\n",
      "[3265 | 3526.11] loss=0.95 avg=0.85\n",
      "[3266 | 3527.71] loss=1.20 avg=0.85\n",
      "[3267 | 3529.31] loss=0.83 avg=0.85\n",
      "[3268 | 3530.91] loss=0.59 avg=0.85\n",
      "[3269 | 3532.51] loss=0.84 avg=0.85\n",
      "[3270 | 3534.11] loss=0.54 avg=0.85\n",
      "[3271 | 3535.71] loss=0.97 avg=0.85\n",
      "[3272 | 3537.30] loss=0.49 avg=0.84\n",
      "[3273 | 3538.90] loss=0.36 avg=0.84\n",
      "[3274 | 3540.50] loss=0.97 avg=0.84\n",
      "[3275 | 3542.09] loss=0.47 avg=0.84\n",
      "[3276 | 3543.69] loss=1.73 avg=0.85\n",
      "[3277 | 3545.28] loss=0.40 avg=0.84\n",
      "[3278 | 3546.88] loss=0.87 avg=0.84\n",
      "[3279 | 3548.47] loss=0.75 avg=0.84\n",
      "[3280 | 3550.07] loss=0.96 avg=0.84\n",
      "[3281 | 3551.66] loss=1.09 avg=0.84\n",
      "[3282 | 3553.26] loss=0.81 avg=0.84\n",
      "[3283 | 3554.86] loss=0.73 avg=0.84\n",
      "[3284 | 3556.45] loss=0.80 avg=0.84\n",
      "[3285 | 3558.05] loss=1.25 avg=0.85\n",
      "[3286 | 3559.64] loss=0.69 avg=0.85\n",
      "[3287 | 3561.24] loss=0.66 avg=0.84\n",
      "[3288 | 3562.84] loss=1.12 avg=0.85\n",
      "[3289 | 3564.43] loss=0.85 avg=0.85\n",
      "[3290 | 3566.03] loss=0.79 avg=0.85\n",
      "[3291 | 3567.62] loss=1.66 avg=0.85\n",
      "[3292 | 3569.22] loss=0.57 avg=0.85\n",
      "[3293 | 3570.81] loss=1.05 avg=0.85\n",
      "[3294 | 3572.41] loss=0.66 avg=0.85\n",
      "[3295 | 3574.01] loss=1.42 avg=0.86\n",
      "[3296 | 3575.60] loss=0.82 avg=0.86\n",
      "[3297 | 3577.20] loss=0.97 avg=0.86\n",
      "[3298 | 3578.79] loss=0.97 avg=0.86\n",
      "[3299 | 3580.39] loss=0.97 avg=0.86\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " org\n",
      "     public static final void addLogicContext(final LogicContext logicContext)\n",
      "                         {\n",
      "        int current = logicContext.getCurrent();\n",
      "        int new = logicContext.getNew();\n",
      "        int length = logicContext.getLength();\n",
      "\n",
      "        // If we're using this, the return value is an array of\n",
      "        // int s, so don't do anything\n",
      "        // and just return the new value\n",
      "        // If we're using this again, only return\n",
      "        // the value of the array.\n",
      "        // Note that in the second call, we've not initialized it\n",
      "        // to get a \"return\" value\n",
      "        # ifndef LOGIC_CONTEXT_FULL\n",
      "        # include \"libjooq.h\"\n",
      "        # endif\n",
      "        // if we just have a \"return\" value, then this is ok\n",
      "        // since that's what we'll get back here too\n",
      "        if (current != -1 && logicContext.getBool() == 0) {\n",
      "             logicContext.getBool(\"return\", current == -1 ? LOGIC_REQUEST_FORWARD : LOGIC_NOT_REQUESTED);              \n",
      "         } else {\n",
      "             // TODO: If this returns an array of ints, we only need to change the returned\n",
      "             // int length, so it should use nulls as indexers.\n",
      "             getBoolean(\"return\", current); }\n",
      "             // If we're using this again, only return\n",
      "             // because that's what we'll get back here too\n",
      "         }\n",
      "\n",
      "        // In order to keep things consistent, there should be only one\n",
      "        // array at the start of each loop.\n",
      "   }\n",
      "}\n",
      "<|endoftext|>/*****************************************************\n",
      " * Copyright (C), 2004, 2007, Oracle and/or its affiliates. All rights reserved.\n",
      " * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.\n",
      " *\n",
      " * This code is free software; you can redistribute it and/or modify it\n",
      " * under the terms of the GNU General Public License version 2 only, as\n",
      " * published by the Free Software Foundation.  Oracle designates this\n",
      " * particular file as \"exception 3\".\n",
      " *\n",
      " * This code is distributed in the hope that it will be useful, but WITHOUT\n",
      " * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n",
      " * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n",
      " * version 2 for more details (a copy is included in the LICENSE file accompanying\n",
      " * this file).\n",
      " *\n",
      " * You should have received a copy of the GNU General Public License version\n",
      " * 2 along with this work; if not, write to the Free Software\n",
      " * Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.\n",
      " *\n",
      " * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA\n",
      " * or visit www.oracle.com if you need additional information or have any\n",
      " * questions.\n",
      " */\n",
      "\n",
      "package sun.xml.mock.api.client.client;\n",
      "\n",
      "import java.util.ArrayList;\n",
      "import java.util.Collections;\n",
      "import java.util.List;\n",
      "import java.util.Map;\n",
      "import sun.xml.mock.api.client.clientContext;\n",
      "import sun.xml.mock.api.client.context.Context;\n",
      "import sun.xml.mock.api.client.clientFactory;\n",
      "\n",
      "import org.apache.commons.lang.annotation.Preprocessor;\n",
      "import org.apache.commons.lang\n",
      "\n",
      "[3300 | 3603.97] loss=1.11 avg=0.86\n",
      "[3301 | 3605.56] loss=0.57 avg=0.86\n",
      "[3302 | 3607.15] loss=1.06 avg=0.86\n",
      "[3303 | 3608.75] loss=1.24 avg=0.87\n",
      "[3304 | 3610.33] loss=0.58 avg=0.86\n",
      "[3305 | 3611.92] loss=1.07 avg=0.86\n",
      "[3306 | 3613.51] loss=0.53 avg=0.86\n",
      "[3307 | 3615.10] loss=0.97 avg=0.86\n",
      "[3308 | 3616.69] loss=0.89 avg=0.86\n",
      "[3309 | 3618.28] loss=1.74 avg=0.87\n",
      "[3310 | 3619.87] loss=0.94 avg=0.87\n",
      "[3311 | 3621.47] loss=0.85 avg=0.87\n",
      "[3312 | 3623.06] loss=0.78 avg=0.87\n",
      "[3313 | 3624.65] loss=0.87 avg=0.87\n",
      "[3314 | 3626.24] loss=0.81 avg=0.87\n",
      "[3315 | 3627.83] loss=1.52 avg=0.88\n",
      "[3316 | 3629.43] loss=0.88 avg=0.88\n",
      "[3317 | 3631.02] loss=1.43 avg=0.88\n",
      "[3318 | 3632.62] loss=0.58 avg=0.88\n",
      "[3319 | 3634.21] loss=0.85 avg=0.88\n",
      "[3320 | 3635.81] loss=0.35 avg=0.87\n",
      "[3321 | 3637.40] loss=1.72 avg=0.88\n",
      "[3322 | 3639.00] loss=1.66 avg=0.89\n",
      "[3323 | 3640.60] loss=0.85 avg=0.89\n",
      "[3324 | 3642.19] loss=1.28 avg=0.89\n",
      "[3325 | 3643.78] loss=0.78 avg=0.89\n",
      "[3326 | 3645.38] loss=0.80 avg=0.89\n",
      "[3327 | 3646.98] loss=0.80 avg=0.89\n",
      "[3328 | 3648.57] loss=0.35 avg=0.89\n",
      "[3329 | 3650.17] loss=0.72 avg=0.88\n",
      "[3330 | 3651.76] loss=0.43 avg=0.88\n",
      "[3331 | 3653.36] loss=1.28 avg=0.88\n",
      "[3332 | 3654.96] loss=0.39 avg=0.88\n",
      "[3333 | 3656.56] loss=0.93 avg=0.88\n",
      "[3334 | 3658.15] loss=1.38 avg=0.88\n",
      "[3335 | 3659.75] loss=2.18 avg=0.90\n",
      "[3336 | 3661.34] loss=0.36 avg=0.89\n",
      "[3337 | 3662.94] loss=0.49 avg=0.89\n",
      "[3338 | 3664.53] loss=1.27 avg=0.89\n",
      "[3339 | 3666.13] loss=1.19 avg=0.89\n",
      "[3340 | 3667.73] loss=0.58 avg=0.89\n",
      "[3341 | 3669.33] loss=0.89 avg=0.89\n",
      "[3342 | 3670.93] loss=1.64 avg=0.90\n",
      "[3343 | 3672.53] loss=1.15 avg=0.90\n",
      "[3344 | 3674.13] loss=1.14 avg=0.90\n",
      "[3345 | 3675.73] loss=1.59 avg=0.91\n",
      "[3346 | 3677.33] loss=1.01 avg=0.91\n",
      "[3347 | 3678.92] loss=0.80 avg=0.91\n",
      "[3348 | 3680.52] loss=0.95 avg=0.91\n",
      "[3349 | 3682.12] loss=0.87 avg=0.91\n",
      "[3350 | 3683.72] loss=0.65 avg=0.91\n",
      "[3351 | 3685.31] loss=1.29 avg=0.91\n",
      "[3352 | 3686.91] loss=0.34 avg=0.91\n",
      "[3353 | 3688.51] loss=0.83 avg=0.90\n",
      "[3354 | 3690.10] loss=0.84 avg=0.90\n",
      "[3355 | 3691.70] loss=0.85 avg=0.90\n",
      "[3356 | 3693.30] loss=0.90 avg=0.90\n",
      "[3357 | 3694.90] loss=0.84 avg=0.90\n",
      "[3358 | 3696.49] loss=0.76 avg=0.90\n",
      "[3359 | 3698.09] loss=0.68 avg=0.90\n",
      "[3360 | 3699.69] loss=1.15 avg=0.90\n",
      "[3361 | 3701.29] loss=1.03 avg=0.90\n",
      "[3362 | 3702.89] loss=1.29 avg=0.91\n",
      "[3363 | 3704.49] loss=0.33 avg=0.90\n",
      "[3364 | 3706.09] loss=1.01 avg=0.90\n",
      "[3365 | 3707.69] loss=0.67 avg=0.90\n",
      "[3366 | 3709.29] loss=1.31 avg=0.90\n",
      "[3367 | 3710.90] loss=0.86 avg=0.90\n",
      "[3368 | 3712.49] loss=0.83 avg=0.90\n",
      "[3369 | 3714.08] loss=0.85 avg=0.90\n",
      "[3370 | 3715.68] loss=0.72 avg=0.90\n",
      "[3371 | 3717.28] loss=1.13 avg=0.90\n",
      "[3372 | 3718.88] loss=1.31 avg=0.91\n",
      "[3373 | 3720.47] loss=1.01 avg=0.91\n",
      "[3374 | 3722.07] loss=0.90 avg=0.91\n",
      "[3375 | 3723.67] loss=0.78 avg=0.91\n",
      "[3376 | 3725.26] loss=0.55 avg=0.90\n",
      "[3377 | 3726.86] loss=1.26 avg=0.91\n",
      "[3378 | 3728.45] loss=1.11 avg=0.91\n",
      "[3379 | 3730.05] loss=0.74 avg=0.91\n",
      "[3380 | 3731.65] loss=1.05 avg=0.91\n",
      "[3381 | 3733.24] loss=1.38 avg=0.91\n",
      "[3382 | 3734.84] loss=1.38 avg=0.92\n",
      "[3383 | 3736.44] loss=1.15 avg=0.92\n",
      "[3384 | 3738.04] loss=1.18 avg=0.92\n",
      "[3385 | 3739.64] loss=1.16 avg=0.93\n",
      "[3386 | 3741.23] loss=0.93 avg=0.93\n",
      "[3387 | 3742.83] loss=0.55 avg=0.92\n",
      "[3388 | 3744.43] loss=1.04 avg=0.92\n",
      "[3389 | 3746.02] loss=0.87 avg=0.92\n",
      "[3390 | 3747.62] loss=0.73 avg=0.92\n",
      "[3391 | 3749.22] loss=1.17 avg=0.92\n",
      "[3392 | 3750.81] loss=0.72 avg=0.92\n",
      "[3393 | 3752.40] loss=1.51 avg=0.93\n",
      "[3394 | 3754.00] loss=1.27 avg=0.93\n",
      "[3395 | 3755.59] loss=0.46 avg=0.93\n",
      "[3396 | 3757.19] loss=0.85 avg=0.92\n",
      "[3397 | 3758.78] loss=0.99 avg=0.93\n",
      "[3398 | 3760.38] loss=0.60 avg=0.92\n",
      "[3399 | 3761.98] loss=0.74 avg=0.92\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "                 }\n",
      "                                     }\n",
      "                                       if (nMb_b == 9) {\n",
      "                                          // We are on Mb, check to be sure\n",
      "                                         if (nMx >= nMb_b && nMx <= nMx + 3) {\n",
      "                                              // not Mb, because Mb + 2 is too short (3 - 1)\n",
      "                                             /*\n",
      "                                    /* + 2 <= nMb */\n",
      "                                          */\n",
      "                                        nMx = nMb;\n",
      "                                      } else {\n",
      "                                       break ;\n",
      "                                     }\n",
      "                                    break ;\n",
      "                                  case '0':\n",
      "                                      /*\n",
      "                                    /* + 2 <= nMb */\n",
      "                                      break ;\n",
      "                                  case '+':\n",
      "                                      /*\n",
      "                                     /* + 2 <= nMb */\n",
      "                                      break ;\n",
      "                                    case '.':\n",
      "                                     /*\n",
      "\n",
      "\n",
      "[3400 | 3785.42] loss=0.99 avg=0.92\n",
      "[3401 | 3787.01] loss=0.84 avg=0.92\n",
      "[3402 | 3788.60] loss=0.90 avg=0.92\n",
      "[3403 | 3790.19] loss=1.24 avg=0.92\n",
      "[3404 | 3791.79] loss=0.75 avg=0.92\n",
      "[3405 | 3793.37] loss=0.55 avg=0.92\n",
      "[3406 | 3794.96] loss=1.09 avg=0.92\n",
      "[3407 | 3796.54] loss=0.84 avg=0.92\n",
      "[3408 | 3798.13] loss=1.26 avg=0.92\n",
      "[3409 | 3799.72] loss=1.27 avg=0.93\n",
      "[3410 | 3801.32] loss=1.05 avg=0.93\n",
      "[3411 | 3802.91] loss=1.01 avg=0.93\n",
      "[3412 | 3804.50] loss=0.14 avg=0.92\n",
      "[3413 | 3806.09] loss=0.81 avg=0.92\n",
      "[3414 | 3807.69] loss=1.53 avg=0.92\n",
      "[3415 | 3809.28] loss=1.01 avg=0.93\n",
      "[3416 | 3810.87] loss=0.66 avg=0.92\n",
      "[3417 | 3812.46] loss=0.89 avg=0.92\n",
      "[3418 | 3814.05] loss=0.85 avg=0.92\n",
      "[3419 | 3815.65] loss=0.37 avg=0.92\n",
      "[3420 | 3817.24] loss=0.48 avg=0.91\n",
      "[3421 | 3818.83] loss=0.39 avg=0.91\n",
      "[3422 | 3820.43] loss=0.93 avg=0.91\n",
      "[3423 | 3822.03] loss=1.43 avg=0.91\n",
      "[3424 | 3823.63] loss=1.23 avg=0.92\n",
      "[3425 | 3825.23] loss=0.70 avg=0.91\n",
      "[3426 | 3826.82] loss=0.91 avg=0.91\n",
      "[3427 | 3828.42] loss=0.48 avg=0.91\n",
      "[3428 | 3830.02] loss=0.82 avg=0.91\n",
      "[3429 | 3831.63] loss=1.32 avg=0.91\n",
      "[3430 | 3833.23] loss=1.15 avg=0.91\n",
      "[3431 | 3834.82] loss=0.47 avg=0.91\n",
      "[3432 | 3836.42] loss=0.98 avg=0.91\n",
      "[3433 | 3838.02] loss=0.82 avg=0.91\n",
      "[3434 | 3839.62] loss=1.14 avg=0.91\n",
      "[3435 | 3841.21] loss=0.49 avg=0.91\n",
      "[3436 | 3842.82] loss=0.81 avg=0.91\n",
      "[3437 | 3844.41] loss=1.13 avg=0.91\n",
      "[3438 | 3846.01] loss=1.18 avg=0.91\n",
      "[3439 | 3847.61] loss=1.22 avg=0.92\n",
      "[3440 | 3849.21] loss=0.96 avg=0.92\n",
      "[3441 | 3850.81] loss=1.18 avg=0.92\n",
      "[3442 | 3852.41] loss=1.20 avg=0.92\n",
      "[3443 | 3854.01] loss=0.76 avg=0.92\n",
      "[3444 | 3855.61] loss=0.44 avg=0.91\n",
      "[3445 | 3857.20] loss=0.51 avg=0.91\n",
      "[3446 | 3858.80] loss=0.97 avg=0.91\n",
      "[3447 | 3860.40] loss=0.96 avg=0.91\n",
      "[3448 | 3861.99] loss=0.95 avg=0.91\n",
      "[3449 | 3863.59] loss=1.05 avg=0.91\n",
      "[3450 | 3865.18] loss=1.64 avg=0.92\n",
      "[3451 | 3866.77] loss=0.87 avg=0.92\n",
      "[3452 | 3868.36] loss=1.02 avg=0.92\n",
      "[3453 | 3869.95] loss=0.78 avg=0.92\n",
      "[3454 | 3871.54] loss=0.93 avg=0.92\n",
      "[3455 | 3873.13] loss=1.03 avg=0.92\n",
      "[3456 | 3874.73] loss=1.42 avg=0.93\n",
      "[3457 | 3876.32] loss=0.91 avg=0.93\n",
      "[3458 | 3877.91] loss=0.82 avg=0.92\n",
      "[3459 | 3879.50] loss=0.55 avg=0.92\n",
      "[3460 | 3881.08] loss=0.54 avg=0.92\n",
      "[3461 | 3882.67] loss=0.48 avg=0.91\n",
      "[3462 | 3884.26] loss=0.93 avg=0.91\n",
      "[3463 | 3885.85] loss=0.81 avg=0.91\n",
      "[3464 | 3887.44] loss=0.46 avg=0.91\n",
      "[3465 | 3889.03] loss=1.12 avg=0.91\n",
      "[3466 | 3890.62] loss=0.60 avg=0.91\n",
      "[3467 | 3892.21] loss=1.57 avg=0.91\n",
      "[3468 | 3893.79] loss=0.30 avg=0.91\n",
      "[3469 | 3895.39] loss=0.70 avg=0.90\n",
      "[3470 | 3896.98] loss=0.75 avg=0.90\n",
      "[3471 | 3898.57] loss=0.60 avg=0.90\n",
      "[3472 | 3900.16] loss=0.88 avg=0.90\n",
      "[3473 | 3901.75] loss=1.20 avg=0.90\n",
      "[3474 | 3903.35] loss=0.87 avg=0.90\n",
      "[3475 | 3904.94] loss=0.43 avg=0.90\n",
      "[3476 | 3906.53] loss=0.99 avg=0.90\n",
      "[3477 | 3908.13] loss=0.83 avg=0.90\n",
      "[3478 | 3909.72] loss=0.85 avg=0.90\n",
      "[3479 | 3911.32] loss=1.04 avg=0.90\n",
      "[3480 | 3912.91] loss=0.88 avg=0.90\n",
      "[3481 | 3914.51] loss=1.69 avg=0.91\n",
      "[3482 | 3916.10] loss=0.90 avg=0.91\n",
      "[3483 | 3917.70] loss=0.79 avg=0.91\n",
      "[3484 | 3919.29] loss=0.35 avg=0.90\n",
      "[3485 | 3920.89] loss=0.69 avg=0.90\n",
      "[3486 | 3922.48] loss=0.80 avg=0.90\n",
      "[3487 | 3924.07] loss=0.77 avg=0.90\n",
      "[3488 | 3925.66] loss=0.82 avg=0.89\n",
      "[3489 | 3927.25] loss=0.73 avg=0.89\n",
      "[3490 | 3928.84] loss=1.03 avg=0.89\n",
      "[3491 | 3930.43] loss=0.83 avg=0.89\n",
      "[3492 | 3932.03] loss=0.54 avg=0.89\n",
      "[3493 | 3933.62] loss=0.79 avg=0.89\n",
      "[3494 | 3935.21] loss=0.35 avg=0.88\n",
      "[3495 | 3936.81] loss=1.15 avg=0.89\n",
      "[3496 | 3938.40] loss=1.33 avg=0.89\n",
      "[3497 | 3939.99] loss=1.03 avg=0.89\n",
      "[3498 | 3941.59] loss=1.42 avg=0.90\n",
      "[3499 | 3943.18] loss=1.17 avg=0.90\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "     * @param path the location of the file\n",
      "     */\n",
      "    @Override\n",
      "    public File(String path) {\n",
      "        //\n",
      "        // Check if we will accept it\n",
      "        if(path == null) {\n",
      "            return null;\n",
      "        }\n",
      "        // If we will accept it\n",
      "        if(path.contains(\":\")) {\n",
      "            this.path = \"http://localhost/m4.js\", null;\n",
      "        } else if(path.startswith(\":\").isAbsolute()) {\n",
      "            this.path = \"http://localhost/m4.js\", null;\n",
      "        } else if((path.contains(\"\\\\\"&path) != null) && !(path.endingAppended(\\\"$\\\")) && path.startswith(\\\"\\\") && path.endswith(\\\"\\\\\\\")) {\n",
      "            this.path = \"http://localhost/m4.js\", null;\n",
      "        } else {\n",
      "            return new JSR-4.1.1.jsp.Jsp.Element;\n",
      "        }\n",
      "    }\n",
      "   \n",
      "    /**\n",
      "     * Checks if the file was found\n",
      "     */\n",
      "    @Override\n",
      "    public List<File> findFiles() {\n",
      "        return new File(null, null, null, null, null);\n",
      "    }\n",
      "   \n",
      "    /**\n",
      "     * Return a JFrame to be used in an editor\n",
      "     */\n",
      "    @Override\n",
      "    public String getJFrame() {\n",
      "        return this.frame;\n",
      "    }\n",
      "   \n",
      "    /**\n",
      "     *\n",
      "     * @deprecated Use JSParser instead.\n",
      "     */\n",
      "    @Override\n",
      "    public String getJsparser() {\n",
      "        return setJsparser(getJsparser());\n",
      "    }\n",
      "   \n",
      "    /**\n",
      "     * @since 4.0 - Added JSParser.\n",
      "     */\n",
      "    @Override\n",
      "    public void setJsparser(Jsparser jsprapper) {\n",
      "        if (jsprapper.getEnabled()) {\n",
      "            setEnabled(true);\n",
      "        } else {\n",
      "            setEnabled(false);\n",
      "        }\n",
      "   \n",
      "    /**\n",
      "     * Check whether the file exists before parsing the file\n",
      "     */\n",
      "    @Deprecated\n",
      "    public boolean exists() {\n",
      "        return jsprapper.isEnabled();\n",
      "    }\n",
      "   \n",
      "    /**\n",
      "     * If there is a file, print it to the terminal\n",
      "     */\n",
      "    @Deprecated\n",
      "    public void printToTerminal(int x, int y) {\n",
      "        // Check if there is a buffer if (canReadFromTerminal) {\n",
      "         StringBuffer sb = new StringBuffer(this.path,\n",
      "                              \"http://localhost/m4.js/jsp.asp\",\n",
      "                             \"m4/jsp.asp\", null);\n",
      "        //\n",
      "        if (sb.isEmpty()) {\n",
      "         // print to the terminal (if there is one)\n",
      "         sb.printOut(\"\\n\n",
      "\n",
      "[3500 | 3966.64] loss=0.60 avg=0.90\n",
      "[3501 | 3968.24] loss=0.58 avg=0.89\n",
      "[3502 | 3969.83] loss=1.10 avg=0.90\n",
      "[3503 | 3971.43] loss=0.65 avg=0.89\n",
      "[3504 | 3973.02] loss=0.71 avg=0.89\n",
      "[3505 | 3974.62] loss=1.09 avg=0.89\n",
      "[3506 | 3976.21] loss=1.29 avg=0.90\n",
      "[3507 | 3977.81] loss=1.39 avg=0.90\n",
      "[3508 | 3979.40] loss=0.49 avg=0.90\n",
      "[3509 | 3981.00] loss=0.90 avg=0.90\n",
      "[3510 | 3982.60] loss=1.22 avg=0.90\n",
      "[3511 | 3984.19] loss=0.80 avg=0.90\n",
      "[3512 | 3985.79] loss=0.89 avg=0.90\n",
      "[3513 | 3987.38] loss=0.97 avg=0.90\n",
      "[3514 | 3988.98] loss=1.16 avg=0.90\n",
      "[3515 | 3990.57] loss=0.81 avg=0.90\n",
      "[3516 | 3992.17] loss=0.51 avg=0.90\n",
      "[3517 | 3993.76] loss=0.36 avg=0.89\n",
      "[3518 | 3995.35] loss=1.48 avg=0.90\n",
      "[3519 | 3996.94] loss=1.57 avg=0.91\n",
      "[3520 | 3998.54] loss=0.64 avg=0.90\n",
      "[3521 | 4000.13] loss=0.77 avg=0.90\n",
      "[3522 | 4001.72] loss=1.03 avg=0.90\n",
      "[3523 | 4003.32] loss=1.08 avg=0.91\n",
      "[3524 | 4004.91] loss=0.70 avg=0.90\n",
      "[3525 | 4006.50] loss=2.20 avg=0.92\n",
      "[3526 | 4008.09] loss=0.76 avg=0.91\n",
      "[3527 | 4009.69] loss=1.32 avg=0.92\n",
      "[3528 | 4011.28] loss=0.87 avg=0.92\n",
      "[3529 | 4012.87] loss=0.82 avg=0.92\n",
      "[3530 | 4014.46] loss=1.05 avg=0.92\n",
      "[3531 | 4016.05] loss=0.72 avg=0.92\n",
      "[3532 | 4017.64] loss=0.59 avg=0.91\n",
      "[3533 | 4019.23] loss=0.40 avg=0.91\n",
      "[3534 | 4020.82] loss=1.52 avg=0.91\n",
      "[3535 | 4022.41] loss=0.57 avg=0.91\n",
      "[3536 | 4024.01] loss=0.75 avg=0.91\n",
      "[3537 | 4025.60] loss=1.02 avg=0.91\n",
      "[3538 | 4027.20] loss=1.22 avg=0.91\n",
      "[3539 | 4028.79] loss=0.52 avg=0.91\n",
      "[3540 | 4030.38] loss=1.43 avg=0.91\n",
      "[3541 | 4031.98] loss=1.32 avg=0.92\n",
      "[3542 | 4033.57] loss=0.70 avg=0.92\n",
      "[3543 | 4035.17] loss=1.29 avg=0.92\n",
      "[3544 | 4036.76] loss=0.34 avg=0.91\n",
      "[3545 | 4038.36] loss=0.92 avg=0.91\n",
      "[3546 | 4039.95] loss=0.90 avg=0.91\n",
      "[3547 | 4041.54] loss=0.86 avg=0.91\n",
      "[3548 | 4043.14] loss=1.77 avg=0.92\n",
      "[3549 | 4044.73] loss=1.41 avg=0.93\n",
      "[3550 | 4046.32] loss=0.65 avg=0.92\n",
      "[3551 | 4047.92] loss=1.05 avg=0.93\n",
      "[3552 | 4049.51] loss=1.04 avg=0.93\n",
      "[3553 | 4051.11] loss=1.34 avg=0.93\n",
      "[3554 | 4052.70] loss=0.88 avg=0.93\n",
      "[3555 | 4054.29] loss=1.29 avg=0.93\n",
      "[3556 | 4055.89] loss=1.22 avg=0.94\n",
      "[3557 | 4057.48] loss=1.15 avg=0.94\n",
      "[3558 | 4059.08] loss=1.02 avg=0.94\n",
      "[3559 | 4060.67] loss=1.11 avg=0.94\n",
      "[3560 | 4062.26] loss=1.03 avg=0.94\n",
      "[3561 | 4063.85] loss=0.71 avg=0.94\n",
      "[3562 | 4065.44] loss=1.47 avg=0.95\n",
      "[3563 | 4067.04] loss=0.87 avg=0.95\n",
      "[3564 | 4068.63] loss=1.55 avg=0.95\n",
      "[3565 | 4070.22] loss=0.65 avg=0.95\n",
      "[3566 | 4071.82] loss=0.54 avg=0.94\n",
      "[3567 | 4073.41] loss=0.61 avg=0.94\n",
      "[3568 | 4075.01] loss=1.77 avg=0.95\n",
      "[3569 | 4076.60] loss=0.42 avg=0.94\n",
      "[3570 | 4078.19] loss=1.02 avg=0.94\n",
      "[3571 | 4079.79] loss=0.59 avg=0.94\n",
      "[3572 | 4081.39] loss=0.76 avg=0.94\n",
      "[3573 | 4082.98] loss=0.78 avg=0.94\n",
      "[3574 | 4084.57] loss=0.47 avg=0.93\n",
      "[3575 | 4086.17] loss=0.99 avg=0.93\n",
      "[3576 | 4087.76] loss=1.94 avg=0.94\n",
      "[3577 | 4089.35] loss=1.08 avg=0.95\n",
      "[3578 | 4090.95] loss=0.73 avg=0.94\n",
      "[3579 | 4092.54] loss=0.83 avg=0.94\n",
      "[3580 | 4094.14] loss=0.78 avg=0.94\n",
      "[3581 | 4095.73] loss=0.95 avg=0.94\n",
      "[3582 | 4097.32] loss=0.96 avg=0.94\n",
      "[3583 | 4098.92] loss=1.17 avg=0.94\n",
      "[3584 | 4100.51] loss=1.05 avg=0.94\n",
      "[3585 | 4102.11] loss=1.06 avg=0.94\n",
      "[3586 | 4103.70] loss=0.91 avg=0.94\n",
      "[3587 | 4105.30] loss=0.93 avg=0.94\n",
      "[3588 | 4106.89] loss=0.88 avg=0.94\n",
      "[3589 | 4108.49] loss=0.35 avg=0.94\n",
      "[3590 | 4110.09] loss=0.86 avg=0.94\n",
      "[3591 | 4111.69] loss=1.96 avg=0.95\n",
      "[3592 | 4113.28] loss=0.77 avg=0.95\n",
      "[3593 | 4114.87] loss=0.91 avg=0.95\n",
      "[3594 | 4116.46] loss=0.78 avg=0.94\n",
      "[3595 | 4118.06] loss=1.18 avg=0.95\n",
      "[3596 | 4119.65] loss=1.29 avg=0.95\n",
      "[3597 | 4121.25] loss=1.37 avg=0.95\n",
      "[3598 | 4122.84] loss=1.57 avg=0.96\n",
      "[3599 | 4124.43] loss=1.17 avg=0.96\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "let's call these to get the value.\n",
      "     *\n",
      "     * @return string the value\n",
      "     * @throws Exception if an error occurs\n",
      "     * @throws Exception if this Method calls another Method\n",
      "     */\n",
      "    protected final String getValue() throws Exception;\n",
      "\n",
      "    /*\n",
      "     * @param x The type of the result\n",
      "     * @return the type of the result\n",
      "     * @throws TypeError if the argument has type X;\n",
      "     * @throws System.out.println(this);\n",
      "     */\n",
      "    // $ANTLR/doc/*.java\n",
      "\n",
      "    protected TypeError getTypeError() throws TypeError;\n",
      "     *\n",
      "     * @param x The type of the result\n",
      "     * @return the type of the result\n",
      "     */\n",
      "    private TypeError getTypeError() throws TypeError;\n",
      "\n",
      "    /**\n",
      "     * Returns the value of this Method by using the <code>type-level</code> property.\n",
      "     */\n",
      "    private int getValue() throws TypeError;\n",
      "\n",
      "    /**\n",
      "     * Returns the value as double\n",
      "     */\n",
      "    private Double getValue() throws TypeError;\n",
      "\n",
      "    // $ANTLR/doc/*.properties\n",
      "    protected byte[] byte[] getByteArray() throws Exception;\n",
      "    \n",
      "    /**\n",
      "     * Get the value at position 1 of this Method's return sequence to be passed as the\n",
      "     * argument to another method in this Method's return sequence\n",
      "     */\n",
      "    String getByteArray() throws TypeError;\n",
      "\n",
      "    //\n",
      "    /**\n",
      "     * Returns a method call signature and signature list of all their signatures. These\n",
      "     * are the signatures. The values from the signature are set and the remaining\n",
      "     * values are not.\n",
      "     * <p/>\n",
      "     * Note that if no signatures are given for this Method, then the signature\n",
      "     * of this Method should be a single signature, a collection of signature\n",
      "     * pairs, or a pair of SignatureSet1 and SignatureSet2. The collection\n",
      "     * of signature pairs is used to construct an AbstractMethod, because\n",
      "     * the <code>AbstractMethod</code> is constructed from a single signature.\n",
      "     * <p/>\n",
      "     * Note: in the case of a single signature pair, each signature is assigned to a\n",
      "     * associated property of the method.\n",
      "     * <p/>\n",
      "\n",
      "     */\n",
      "    public byte[] getSignature() throws Exception;\n",
      "\n",
      "    /**\n",
      "     * Return the value of this Method.\n",
      "     */\n",
      "    public void setValue(Byte[] value) throws Exception;\n",
      "\n",
      "    /**\n",
      "     * Constructs a {@link Object} from the value.\n",
      "     */\n",
      "    public Object[] get(int x) throws Exception;\n",
      "\n",
      "    /**\n",
      "     * Sets the value of the this Method.\n",
      "     */\n",
      "    void setDefaultValue(byte[] value) {\n",
      "        this.value = value;\n",
      "    }\n",
      "}\n",
      "<|endoftext|>/*\n",
      "    Copyright (C) 2009, 2011 JBoss Group, Red Hat\n",
      "        Inc. All rights reserved.\n",
      "        Red Hat Red Hat Red Hat Red Hat Red Hat is a registered trademark of\n",
      "        Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat.\n",
      "        Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat is an\n",
      "        trademark of Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat Red Hat\n",
      "\n",
      "[3600 | 4148.59] loss=1.01 avg=0.96\n",
      "[3601 | 4150.19] loss=0.75 avg=0.96\n",
      "[3602 | 4151.78] loss=1.20 avg=0.96\n",
      "[3603 | 4153.37] loss=0.85 avg=0.96\n",
      "[3604 | 4154.96] loss=1.45 avg=0.97\n",
      "[3605 | 4156.55] loss=1.30 avg=0.97\n",
      "[3606 | 4158.15] loss=0.88 avg=0.97\n",
      "[3607 | 4159.74] loss=1.20 avg=0.97\n",
      "[3608 | 4161.33] loss=0.61 avg=0.97\n",
      "[3609 | 4162.92] loss=0.83 avg=0.97\n",
      "[3610 | 4164.51] loss=0.81 avg=0.96\n",
      "[3611 | 4166.10] loss=0.84 avg=0.96\n",
      "[3612 | 4167.68] loss=1.28 avg=0.97\n",
      "[3613 | 4169.27] loss=0.48 avg=0.96\n",
      "[3614 | 4170.87] loss=0.33 avg=0.96\n",
      "[3615 | 4172.45] loss=1.00 avg=0.96\n",
      "[3616 | 4174.05] loss=0.67 avg=0.95\n",
      "[3617 | 4175.64] loss=1.21 avg=0.96\n",
      "[3618 | 4177.24] loss=0.96 avg=0.96\n",
      "[3619 | 4178.83] loss=1.14 avg=0.96\n",
      "[3620 | 4180.42] loss=0.86 avg=0.96\n",
      "[3621 | 4182.02] loss=0.33 avg=0.95\n",
      "[3622 | 4183.61] loss=1.14 avg=0.95\n",
      "[3623 | 4185.20] loss=1.72 avg=0.96\n",
      "[3624 | 4186.79] loss=1.04 avg=0.96\n",
      "[3625 | 4188.39] loss=0.76 avg=0.96\n",
      "[3626 | 4189.98] loss=1.71 avg=0.97\n",
      "[3627 | 4191.58] loss=0.59 avg=0.96\n",
      "[3628 | 4193.17] loss=0.73 avg=0.96\n",
      "[3629 | 4194.77] loss=1.13 avg=0.96\n",
      "[3630 | 4196.36] loss=0.85 avg=0.96\n",
      "[3631 | 4197.95] loss=0.89 avg=0.96\n",
      "[3632 | 4199.54] loss=0.79 avg=0.96\n",
      "[3633 | 4201.14] loss=1.29 avg=0.96\n",
      "[3634 | 4202.73] loss=0.92 avg=0.96\n",
      "[3635 | 4204.32] loss=0.34 avg=0.95\n",
      "[3636 | 4205.92] loss=0.56 avg=0.95\n",
      "[3637 | 4207.51] loss=0.59 avg=0.95\n",
      "[3638 | 4209.10] loss=0.53 avg=0.94\n",
      "[3639 | 4210.70] loss=0.76 avg=0.94\n",
      "[3640 | 4212.29] loss=0.13 avg=0.93\n",
      "[3641 | 4213.89] loss=0.96 avg=0.93\n",
      "[3642 | 4215.48] loss=0.80 avg=0.93\n",
      "[3643 | 4217.08] loss=1.04 avg=0.93\n",
      "[3644 | 4218.68] loss=1.09 avg=0.93\n",
      "[3645 | 4220.27] loss=0.97 avg=0.94\n",
      "[3646 | 4221.87] loss=0.97 avg=0.94\n",
      "[3647 | 4223.46] loss=0.56 avg=0.93\n",
      "[3648 | 4225.06] loss=0.99 avg=0.93\n",
      "[3649 | 4226.66] loss=0.98 avg=0.93\n",
      "[3650 | 4228.25] loss=0.82 avg=0.93\n",
      "[3651 | 4229.85] loss=1.21 avg=0.93\n",
      "[3652 | 4231.45] loss=0.68 avg=0.93\n",
      "[3653 | 4233.04] loss=0.84 avg=0.93\n",
      "[3654 | 4234.64] loss=0.82 avg=0.93\n",
      "[3655 | 4236.24] loss=1.15 avg=0.93\n",
      "[3656 | 4237.84] loss=0.75 avg=0.93\n",
      "[3657 | 4239.43] loss=0.90 avg=0.93\n",
      "[3658 | 4241.03] loss=0.52 avg=0.93\n",
      "[3659 | 4242.64] loss=0.97 avg=0.93\n",
      "[3660 | 4244.23] loss=0.71 avg=0.92\n",
      "[3661 | 4245.83] loss=0.59 avg=0.92\n",
      "[3662 | 4247.43] loss=0.84 avg=0.92\n",
      "[3663 | 4249.03] loss=0.56 avg=0.92\n",
      "[3664 | 4250.62] loss=1.23 avg=0.92\n",
      "[3665 | 4252.22] loss=0.38 avg=0.91\n",
      "[3666 | 4253.82] loss=1.53 avg=0.92\n",
      "[3667 | 4255.42] loss=0.94 avg=0.92\n",
      "[3668 | 4257.02] loss=0.81 avg=0.92\n",
      "[3669 | 4258.61] loss=1.23 avg=0.92\n",
      "[3670 | 4260.21] loss=0.64 avg=0.92\n",
      "[3671 | 4261.81] loss=0.89 avg=0.92\n",
      "[3672 | 4263.40] loss=0.76 avg=0.92\n",
      "[3673 | 4265.00] loss=1.60 avg=0.92\n",
      "[3674 | 4266.60] loss=1.12 avg=0.93\n",
      "[3675 | 4268.20] loss=0.99 avg=0.93\n",
      "[3676 | 4269.80] loss=0.53 avg=0.92\n",
      "[3677 | 4271.39] loss=0.67 avg=0.92\n",
      "[3678 | 4272.99] loss=1.22 avg=0.92\n",
      "[3679 | 4274.59] loss=0.90 avg=0.92\n",
      "[3680 | 4276.19] loss=0.81 avg=0.92\n",
      "[3681 | 4277.79] loss=1.00 avg=0.92\n",
      "[3682 | 4279.38] loss=1.39 avg=0.93\n",
      "[3683 | 4280.97] loss=1.03 avg=0.93\n",
      "[3684 | 4282.58] loss=0.48 avg=0.92\n",
      "[3685 | 4284.17] loss=0.81 avg=0.92\n",
      "[3686 | 4285.76] loss=0.52 avg=0.92\n",
      "[3687 | 4287.36] loss=0.87 avg=0.92\n",
      "[3688 | 4288.95] loss=1.17 avg=0.92\n",
      "[3689 | 4290.55] loss=0.30 avg=0.91\n",
      "[3690 | 4292.14] loss=0.74 avg=0.91\n",
      "[3691 | 4293.74] loss=1.02 avg=0.91\n",
      "[3692 | 4295.34] loss=1.16 avg=0.92\n",
      "[3693 | 4296.93] loss=0.87 avg=0.92\n",
      "[3694 | 4298.53] loss=0.75 avg=0.91\n",
      "[3695 | 4300.13] loss=1.10 avg=0.92\n",
      "[3696 | 4301.72] loss=0.85 avg=0.92\n",
      "[3697 | 4303.32] loss=0.85 avg=0.92\n",
      "[3698 | 4304.92] loss=0.84 avg=0.91\n",
      "[3699 | 4306.52] loss=0.74 avg=0.91\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " (I);\n",
      "\n",
      "\tfinal int m_MaxSuspend =\n",
      "\t\t\tnew int[]{\"-1\", \"-1\", \"-1\",\n",
      "\t\t\tnew int[10]{\"+12\", \"+12\",\n",
      "\t\t\tnew int[6]{\"+6\", \"+6\", \"+6\",\n",
      "\t\t\tnew int[12]{\"+10\", \"+10\",\n",
      "\t\t\tfinal int][]{\"+10\", \"+10\", \"+10\",\n",
      "\t\t\tfinal int[]{\"+10\", \"+10\", \"+10\",\n",
      "\t\t\tfinal int[]{\"+10\", \"+10\",\n",
      "\t\t\tfinal int[]{\"+10\", \"+10\", \"+10\",\n",
      "\t\t\tfinal int[3]{\"+6\", \"+6\", \"+6\",\n",
      "\t\t\tfinal int[]{\"+6\", \"+6\", \"+6\",\n",
      "\t\t\tfinal int[]{\"+6\", \"+6\", \"+6\",\n",
      "\t\t\tfinal int[4]{\"+4\", \"+4\", \"+4\", \n",
      "\t\t\tfinal int[5]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[]{\"+4\", \"+4\", \"+4\",\n",
      "\t\t\tfinal int[6]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[1]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[10]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[12]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[3]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[4]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"+8\", \"+8\", \"+8\",\n",
      "\t\t\tfinal int[]{\"\n",
      "\n",
      "[3700 | 4330.30] loss=0.78 avg=0.91\n",
      "[3701 | 4331.90] loss=0.41 avg=0.91\n",
      "[3702 | 4333.49] loss=0.76 avg=0.90\n",
      "[3703 | 4335.08] loss=1.15 avg=0.91\n",
      "[3704 | 4336.67] loss=0.55 avg=0.90\n",
      "[3705 | 4338.26] loss=0.83 avg=0.90\n",
      "[3706 | 4339.85] loss=0.27 avg=0.90\n",
      "[3707 | 4341.44] loss=0.67 avg=0.89\n",
      "[3708 | 4343.03] loss=0.96 avg=0.89\n",
      "[3709 | 4344.61] loss=0.73 avg=0.89\n",
      "[3710 | 4346.20] loss=1.07 avg=0.90\n",
      "[3711 | 4347.79] loss=1.26 avg=0.90\n",
      "[3712 | 4349.39] loss=1.17 avg=0.90\n",
      "[3713 | 4350.98] loss=0.41 avg=0.90\n",
      "[3714 | 4352.57] loss=0.17 avg=0.89\n",
      "[3715 | 4354.16] loss=0.63 avg=0.89\n",
      "[3716 | 4355.75] loss=0.68 avg=0.88\n",
      "[3717 | 4357.34] loss=0.78 avg=0.88\n",
      "[3718 | 4358.93] loss=1.01 avg=0.88\n",
      "[3719 | 4360.53] loss=0.72 avg=0.88\n",
      "[3720 | 4362.12] loss=1.34 avg=0.89\n",
      "[3721 | 4363.72] loss=0.76 avg=0.89\n",
      "[3722 | 4365.32] loss=0.98 avg=0.89\n",
      "[3723 | 4366.91] loss=0.85 avg=0.89\n",
      "[3724 | 4368.50] loss=0.20 avg=0.88\n",
      "[3725 | 4370.10] loss=1.48 avg=0.89\n",
      "[3726 | 4371.70] loss=0.71 avg=0.88\n",
      "[3727 | 4373.30] loss=0.92 avg=0.88\n",
      "[3728 | 4374.89] loss=0.30 avg=0.88\n",
      "[3729 | 4376.49] loss=0.54 avg=0.88\n",
      "[3730 | 4378.09] loss=0.84 avg=0.88\n",
      "[3731 | 4379.69] loss=0.70 avg=0.87\n",
      "[3732 | 4381.29] loss=0.78 avg=0.87\n",
      "[3733 | 4382.89] loss=0.94 avg=0.87\n",
      "[3734 | 4384.49] loss=0.77 avg=0.87\n",
      "[3735 | 4386.10] loss=0.94 avg=0.87\n",
      "[3736 | 4387.70] loss=1.01 avg=0.87\n",
      "[3737 | 4389.29] loss=0.69 avg=0.87\n",
      "[3738 | 4390.89] loss=0.67 avg=0.87\n",
      "[3739 | 4392.49] loss=0.43 avg=0.87\n",
      "[3740 | 4394.09] loss=0.67 avg=0.86\n",
      "[3741 | 4395.69] loss=0.59 avg=0.86\n",
      "[3742 | 4397.28] loss=1.30 avg=0.87\n",
      "[3743 | 4398.88] loss=0.75 avg=0.86\n",
      "[3744 | 4400.48] loss=1.49 avg=0.87\n",
      "[3745 | 4402.08] loss=1.34 avg=0.88\n",
      "[3746 | 4403.68] loss=0.50 avg=0.87\n",
      "[3747 | 4405.28] loss=0.89 avg=0.87\n",
      "[3748 | 4406.88] loss=0.66 avg=0.87\n",
      "[3749 | 4408.48] loss=1.19 avg=0.87\n",
      "[3750 | 4410.08] loss=0.79 avg=0.87\n",
      "[3751 | 4411.69] loss=1.34 avg=0.88\n",
      "[3752 | 4413.28] loss=1.93 avg=0.89\n",
      "[3753 | 4414.87] loss=0.48 avg=0.88\n",
      "[3754 | 4416.47] loss=0.52 avg=0.88\n",
      "[3755 | 4418.06] loss=0.52 avg=0.88\n",
      "[3756 | 4419.66] loss=0.66 avg=0.87\n",
      "[3757 | 4421.26] loss=0.66 avg=0.87\n",
      "[3758 | 4422.86] loss=1.30 avg=0.88\n",
      "[3759 | 4424.45] loss=1.19 avg=0.88\n",
      "[3760 | 4426.05] loss=0.93 avg=0.88\n",
      "[3761 | 4427.65] loss=0.46 avg=0.88\n",
      "[3762 | 4429.24] loss=0.80 avg=0.87\n",
      "[3763 | 4430.84] loss=1.31 avg=0.88\n",
      "[3764 | 4432.43] loss=0.55 avg=0.88\n",
      "[3765 | 4434.03] loss=1.02 avg=0.88\n",
      "[3766 | 4435.63] loss=0.86 avg=0.88\n",
      "[3767 | 4437.23] loss=1.28 avg=0.88\n",
      "[3768 | 4438.83] loss=0.90 avg=0.88\n",
      "[3769 | 4440.43] loss=0.92 avg=0.88\n",
      "[3770 | 4442.03] loss=1.18 avg=0.88\n",
      "[3771 | 4443.63] loss=0.75 avg=0.88\n",
      "[3772 | 4445.23] loss=0.91 avg=0.88\n",
      "[3773 | 4446.82] loss=0.58 avg=0.88\n",
      "[3774 | 4448.43] loss=0.57 avg=0.88\n",
      "[3775 | 4450.02] loss=1.03 avg=0.88\n",
      "[3776 | 4451.63] loss=0.35 avg=0.87\n",
      "[3777 | 4453.22] loss=0.82 avg=0.87\n",
      "[3778 | 4454.83] loss=0.58 avg=0.87\n",
      "[3779 | 4456.42] loss=0.52 avg=0.87\n",
      "[3780 | 4458.02] loss=0.61 avg=0.86\n",
      "[3781 | 4459.61] loss=0.17 avg=0.86\n",
      "[3782 | 4461.22] loss=0.70 avg=0.86\n",
      "[3783 | 4462.81] loss=0.68 avg=0.85\n",
      "[3784 | 4464.41] loss=0.86 avg=0.85\n",
      "[3785 | 4466.01] loss=0.47 avg=0.85\n",
      "[3786 | 4467.60] loss=1.11 avg=0.85\n",
      "[3787 | 4469.20] loss=0.92 avg=0.85\n",
      "[3788 | 4470.80] loss=0.26 avg=0.85\n",
      "[3789 | 4472.39] loss=1.55 avg=0.85\n",
      "[3790 | 4473.99] loss=0.34 avg=0.85\n",
      "[3791 | 4475.58] loss=0.90 avg=0.85\n",
      "[3792 | 4477.18] loss=0.84 avg=0.85\n",
      "[3793 | 4478.78] loss=0.64 avg=0.85\n",
      "[3794 | 4480.38] loss=1.35 avg=0.85\n",
      "[3795 | 4481.97] loss=0.69 avg=0.85\n",
      "[3796 | 4483.57] loss=0.82 avg=0.85\n",
      "[3797 | 4485.17] loss=0.73 avg=0.85\n",
      "[3798 | 4486.76] loss=1.02 avg=0.85\n",
      "[3799 | 4488.36] loss=0.52 avg=0.85\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " u = new Integer((x[i + 3] - x[i + 1] + 5);\n",
      "            break;\n",
      "          } //\n",
      "           // this is the maximum value for i\n",
      "           // to avoid overflow\n",
      "           // if the maximum value reaches 0, we should use an integer\n",
      "           // instead if the first two digits are equal to\n",
      "           // the max value\n",
      "           for (int i = 0; i < x[3 - 3]; i+=1) {\n",
      "             // i + 1 = (int)(2^i^2);\n",
      "             // i + 4 = (int)(5);\n",
      "             // i + 7 = (int)(2^i^2);\n",
      "            } //\n",
      "           \n",
      "           break;\n",
      "           } //\n",
      "           /*\n",
      "           * We need to avoid overflow in this case if the two\n",
      "           * digits are equal, because if an overflow occurs we know\n",
      "           * that the max value was never exceeded.\n",
      "           */\n",
      "\n",
      "           int c = 0;\n",
      "           for (int i = 3; i < 5; i++) {\n",
      "               int j = 3 + x[i + 3];\n",
      "              if ( (j == 1) || (j == 6) ) {\n",
      "                   break;\n",
      "            }\n",
      "               // i + 4 = (int)(2^i^2);\n",
      "              // the first digit is equal to 4;\n",
      "              int j = 2^i^2;\n",
      "              for (int j = 1; j < 4; j++) {\n",
      "                  int x = x[j]);\n",
      "\n",
      "                 if ( (j == 7) && (j == 3) ) {\n",
      "                    (j += 2); // if j == 2 then the maximum\n",
      "                       = 0;\n",
      "                } else {\n",
      "                    int x;\n",
      "                    if ( (j == 3) && (j == 3) ) {\n",
      "                       for (int j = 1; j < 4; j++) {\n",
      "                        if ( (j == 1) || (j == 6) ) {\n",
      "                           ((j ^ 2) + 4) + 2;\n",
      "                      }\n",
      "                   }\n",
      "                   if ( (j == 7) && (j == 3) ) {\n",
      "              \n",
      "\n",
      "[3800 | 4511.78] loss=1.11 avg=0.85\n",
      "[3801 | 4513.37] loss=0.30 avg=0.84\n",
      "[3802 | 4514.97] loss=1.17 avg=0.85\n",
      "[3803 | 4516.56] loss=0.88 avg=0.85\n",
      "[3804 | 4518.15] loss=0.91 avg=0.85\n",
      "[3805 | 4519.74] loss=0.60 avg=0.85\n",
      "[3806 | 4521.33] loss=0.88 avg=0.85\n",
      "[3807 | 4522.92] loss=1.49 avg=0.85\n",
      "[3808 | 4524.51] loss=0.84 avg=0.85\n",
      "[3809 | 4526.10] loss=0.57 avg=0.85\n",
      "[3810 | 4527.69] loss=0.83 avg=0.85\n",
      "[3811 | 4529.28] loss=0.51 avg=0.85\n",
      "[3812 | 4530.87] loss=0.92 avg=0.85\n",
      "[3813 | 4532.46] loss=0.75 avg=0.85\n",
      "[3814 | 4534.05] loss=0.81 avg=0.85\n",
      "[3815 | 4535.64] loss=0.81 avg=0.85\n",
      "[3816 | 4537.23] loss=0.82 avg=0.85\n",
      "[3817 | 4538.82] loss=1.59 avg=0.85\n",
      "[3818 | 4540.41] loss=0.49 avg=0.85\n",
      "[3819 | 4542.01] loss=1.02 avg=0.85\n",
      "[3820 | 4543.59] loss=0.77 avg=0.85\n",
      "[3821 | 4545.19] loss=0.97 avg=0.85\n",
      "[3822 | 4546.78] loss=0.64 avg=0.85\n",
      "[3823 | 4548.38] loss=0.70 avg=0.85\n",
      "[3824 | 4549.97] loss=0.52 avg=0.84\n",
      "[3825 | 4551.57] loss=1.16 avg=0.85\n",
      "[3826 | 4553.17] loss=0.63 avg=0.85\n",
      "[3827 | 4554.81] loss=0.97 avg=0.85\n",
      "[3828 | 4556.41] loss=0.92 avg=0.85\n",
      "[3829 | 4558.00] loss=0.99 avg=0.85\n",
      "[3830 | 4559.60] loss=0.65 avg=0.85\n",
      "[3831 | 4561.20] loss=0.58 avg=0.84\n",
      "[3832 | 4562.80] loss=0.53 avg=0.84\n",
      "[3833 | 4564.40] loss=1.05 avg=0.84\n",
      "[3834 | 4566.00] loss=1.31 avg=0.85\n",
      "[3835 | 4567.59] loss=1.40 avg=0.85\n",
      "[3836 | 4569.19] loss=1.18 avg=0.86\n",
      "[3837 | 4570.79] loss=0.85 avg=0.86\n",
      "[3838 | 4572.39] loss=0.65 avg=0.85\n",
      "[3839 | 4573.99] loss=1.33 avg=0.86\n",
      "[3840 | 4575.59] loss=0.84 avg=0.86\n",
      "[3841 | 4577.19] loss=1.13 avg=0.86\n",
      "[3842 | 4578.79] loss=1.04 avg=0.86\n",
      "[3843 | 4580.39] loss=0.78 avg=0.86\n",
      "[3844 | 4581.99] loss=0.70 avg=0.86\n",
      "[3845 | 4583.58] loss=1.18 avg=0.86\n",
      "[3846 | 4585.18] loss=1.40 avg=0.87\n",
      "[3847 | 4586.78] loss=0.77 avg=0.87\n",
      "[3848 | 4588.38] loss=1.26 avg=0.87\n",
      "[3849 | 4589.99] loss=0.56 avg=0.87\n",
      "[3850 | 4591.58] loss=0.71 avg=0.87\n",
      "[3851 | 4593.18] loss=0.88 avg=0.87\n",
      "[3852 | 4594.78] loss=0.76 avg=0.87\n",
      "[3853 | 4596.38] loss=0.97 avg=0.87\n",
      "[3854 | 4597.98] loss=1.01 avg=0.87\n",
      "[3855 | 4599.58] loss=0.84 avg=0.87\n",
      "[3856 | 4601.18] loss=0.47 avg=0.87\n",
      "[3857 | 4602.77] loss=1.11 avg=0.87\n",
      "[3858 | 4604.37] loss=1.27 avg=0.87\n",
      "[3859 | 4605.97] loss=1.34 avg=0.88\n",
      "[3860 | 4607.57] loss=0.87 avg=0.88\n",
      "[3861 | 4609.17] loss=0.66 avg=0.87\n",
      "[3862 | 4610.76] loss=0.97 avg=0.88\n",
      "[3863 | 4612.36] loss=0.86 avg=0.87\n",
      "[3864 | 4613.97] loss=1.26 avg=0.88\n",
      "[3865 | 4615.57] loss=1.05 avg=0.88\n",
      "[3866 | 4617.17] loss=0.31 avg=0.87\n",
      "[3867 | 4618.76] loss=1.00 avg=0.88\n",
      "[3868 | 4620.36] loss=0.92 avg=0.88\n",
      "[3869 | 4621.96] loss=0.80 avg=0.88\n",
      "[3870 | 4623.56] loss=0.93 avg=0.88\n",
      "[3871 | 4625.16] loss=0.49 avg=0.87\n",
      "[3872 | 4626.76] loss=0.66 avg=0.87\n",
      "[3873 | 4628.36] loss=0.81 avg=0.87\n",
      "[3874 | 4629.96] loss=1.50 avg=0.88\n",
      "[3875 | 4631.56] loss=0.55 avg=0.87\n",
      "[3876 | 4633.16] loss=0.65 avg=0.87\n",
      "[3877 | 4634.76] loss=0.58 avg=0.87\n",
      "[3878 | 4636.35] loss=0.64 avg=0.87\n",
      "[3879 | 4637.95] loss=1.01 avg=0.87\n",
      "[3880 | 4639.55] loss=1.24 avg=0.87\n",
      "[3881 | 4641.15] loss=1.16 avg=0.87\n",
      "[3882 | 4642.75] loss=0.90 avg=0.87\n",
      "[3883 | 4644.35] loss=1.82 avg=0.88\n",
      "[3884 | 4645.95] loss=0.90 avg=0.88\n",
      "[3885 | 4647.54] loss=0.84 avg=0.88\n",
      "[3886 | 4649.15] loss=0.36 avg=0.88\n",
      "[3887 | 4650.74] loss=1.03 avg=0.88\n",
      "[3888 | 4652.34] loss=0.74 avg=0.88\n",
      "[3889 | 4653.93] loss=1.24 avg=0.88\n",
      "[3890 | 4655.53] loss=1.54 avg=0.89\n",
      "[3891 | 4657.12] loss=0.43 avg=0.88\n",
      "[3892 | 4658.72] loss=0.46 avg=0.88\n",
      "[3893 | 4660.32] loss=0.73 avg=0.88\n",
      "[3894 | 4661.92] loss=0.76 avg=0.88\n",
      "[3895 | 4663.51] loss=1.26 avg=0.88\n",
      "[3896 | 4665.10] loss=1.49 avg=0.89\n",
      "[3897 | 4666.70] loss=0.90 avg=0.89\n",
      "[3898 | 4668.30] loss=1.13 avg=0.89\n",
      "[3899 | 4669.90] loss=0.79 avg=0.89\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "                  } else {\n",
      "                         m = null;\n",
      "                    }\n",
      "               }\n",
      "               for ( int i = 0; i < m.numInputs(); i++) {\n",
      "                     while (i < m.numInputs() - 1) {\n",
      "                         m.mute(); }\n",
      "                 } else {\n",
      "                       if (m == null) {\n",
      "                           m = null;\n",
      "                          break;\n",
      "                     }\n",
      "                 }\n",
      "               }\n",
      "              for (int i = 0; i < m.numInputs(); i++) {\n",
      "                   m.mute(); }\n",
      "               break;\n",
      "            }\n",
      "            if (m.chord == Chord.NORMAL) {\n",
      "                m = null;\n",
      "             } else {\n",
      "                m = default;\n",
      "            }\n",
      "              return m;\n",
      "           }\n",
      "\n",
      "           return m.next = m;\n",
      "       }\n",
      "       //\n",
      "       // return a new Chord, always on the same position if its not already\n",
      "       // there.\n",
      "       return m.next = m;\n",
      "    }\n",
      "\n",
      "    /**\n",
      "     * This method determines if the given Chord is currently on the\n",
      "     * current position. Returns true if its on a start.\n",
      "     *\n",
      "     * @return Whether the given Chord is currently on the current space.\n",
      "     *\n",
      "     * @see KeyFrame<|endoftext|>\"The end of the world is only months away,\" is the catch phrase of the apocalypse. Or at least, that's all it says.\n",
      "\n",
      "For those not familiar, apocalypse is a phrase coined by one Mark Twain in his famous fictional novel \"Travels at the Mountains of Madness\" (1872). As the title suggests, Twain's book was loosely based on the life and writings of famous inventor John Von Neumann.\n",
      "\n",
      "What's most interesting about \"travels at the mountains of madness\"? If you consider those famous novel's plot twists, this doesn't surprise. Not only does this film's title imply it's being set in \"the land of the living dead,\" it also implies there's only the short window of months left. Of course, the filmmakers didn't have to make any such \"logical conclusions.\"\n",
      "\n",
      "I've read many accounts of the book's plot twists, and the one I can think of is in Mark Twain's third book \"To the Moon.\" In this short story, set in 1876 and told in a humorous and poetic twist, Twain's character, John, has a \"discovery\" that \"it was time enough for an end\"; then, \"at last.\"\n",
      "\n",
      "Mark Twain's \"Travels at the Mountains of Madness\"\n",
      "\n",
      "If you read \"To the Moon\" and look at the ending of \"travels at the mountains of madness,\" you'll read a somewhat disturbing story\n",
      "\n",
      "[3900 | 4693.39] loss=0.83 avg=0.89\n",
      "[3901 | 4694.99] loss=1.34 avg=0.89\n",
      "[3902 | 4696.58] loss=1.10 avg=0.89\n",
      "[3903 | 4698.17] loss=0.91 avg=0.89\n",
      "[3904 | 4699.77] loss=0.76 avg=0.89\n",
      "[3905 | 4701.37] loss=0.48 avg=0.89\n",
      "[3906 | 4702.96] loss=0.74 avg=0.89\n",
      "[3907 | 4704.56] loss=0.76 avg=0.89\n",
      "[3908 | 4706.15] loss=0.76 avg=0.88\n",
      "[3909 | 4707.74] loss=0.97 avg=0.89\n",
      "[3910 | 4709.34] loss=0.71 avg=0.88\n",
      "[3911 | 4710.93] loss=0.76 avg=0.88\n",
      "[3912 | 4712.51] loss=0.74 avg=0.88\n",
      "[3913 | 4714.11] loss=0.44 avg=0.88\n",
      "[3914 | 4715.69] loss=0.42 avg=0.87\n",
      "[3915 | 4717.28] loss=0.50 avg=0.87\n",
      "[3916 | 4718.87] loss=0.89 avg=0.87\n",
      "[3917 | 4720.46] loss=0.63 avg=0.87\n",
      "[3918 | 4722.05] loss=1.61 avg=0.87\n",
      "[3919 | 4723.64] loss=0.76 avg=0.87\n",
      "[3920 | 4725.24] loss=0.99 avg=0.87\n",
      "[3921 | 4726.83] loss=1.06 avg=0.88\n",
      "[3922 | 4728.42] loss=1.36 avg=0.88\n",
      "[3923 | 4730.01] loss=0.73 avg=0.88\n",
      "[3924 | 4731.61] loss=0.60 avg=0.88\n",
      "[3925 | 4733.21] loss=0.99 avg=0.88\n",
      "[3926 | 4734.80] loss=0.83 avg=0.88\n",
      "[3927 | 4736.39] loss=0.42 avg=0.87\n",
      "[3928 | 4737.99] loss=1.68 avg=0.88\n",
      "[3929 | 4739.58] loss=0.67 avg=0.88\n",
      "[3930 | 4741.17] loss=0.72 avg=0.88\n",
      "[3931 | 4742.77] loss=1.68 avg=0.88\n",
      "[3932 | 4744.36] loss=0.48 avg=0.88\n",
      "[3933 | 4745.95] loss=0.88 avg=0.88\n",
      "[3934 | 4747.55] loss=0.64 avg=0.88\n",
      "[3935 | 4749.14] loss=1.38 avg=0.88\n",
      "[3936 | 4750.73] loss=1.30 avg=0.89\n",
      "[3937 | 4752.33] loss=0.78 avg=0.89\n",
      "[3938 | 4753.92] loss=0.49 avg=0.88\n",
      "[3939 | 4755.53] loss=0.70 avg=0.88\n",
      "[3940 | 4757.13] loss=0.82 avg=0.88\n",
      "[3941 | 4758.72] loss=0.63 avg=0.88\n",
      "[3942 | 4760.33] loss=0.67 avg=0.88\n",
      "[3943 | 4761.93] loss=1.11 avg=0.88\n",
      "[3944 | 4763.52] loss=0.57 avg=0.87\n",
      "[3945 | 4765.11] loss=0.71 avg=0.87\n",
      "[3946 | 4766.71] loss=0.71 avg=0.87\n",
      "[3947 | 4768.31] loss=1.77 avg=0.88\n",
      "[3948 | 4769.91] loss=1.37 avg=0.89\n",
      "[3949 | 4771.50] loss=0.39 avg=0.88\n",
      "[3950 | 4773.10] loss=1.04 avg=0.88\n",
      "[3951 | 4774.70] loss=1.09 avg=0.88\n",
      "[3952 | 4776.29] loss=1.31 avg=0.89\n",
      "[3953 | 4777.89] loss=1.12 avg=0.89\n",
      "[3954 | 4779.49] loss=0.41 avg=0.89\n",
      "[3955 | 4781.09] loss=0.72 avg=0.88\n",
      "[3956 | 4782.68] loss=0.86 avg=0.88\n",
      "[3957 | 4784.28] loss=0.09 avg=0.88\n",
      "[3958 | 4785.88] loss=0.34 avg=0.87\n",
      "[3959 | 4787.49] loss=0.74 avg=0.87\n",
      "[3960 | 4789.09] loss=1.14 avg=0.87\n",
      "[3961 | 4790.69] loss=0.49 avg=0.87\n",
      "[3962 | 4792.28] loss=0.86 avg=0.87\n",
      "[3963 | 4793.88] loss=1.81 avg=0.88\n",
      "[3964 | 4795.48] loss=0.97 avg=0.88\n",
      "[3965 | 4797.07] loss=1.02 avg=0.88\n",
      "[3966 | 4798.67] loss=1.07 avg=0.88\n",
      "[3967 | 4800.27] loss=1.01 avg=0.88\n",
      "[3968 | 4801.87] loss=0.82 avg=0.88\n",
      "[3969 | 4803.47] loss=1.04 avg=0.88\n",
      "[3970 | 4805.07] loss=0.84 avg=0.88\n",
      "[3971 | 4806.66] loss=1.08 avg=0.89\n",
      "[3972 | 4808.26] loss=0.56 avg=0.88\n",
      "[3973 | 4809.85] loss=0.91 avg=0.88\n",
      "[3974 | 4811.45] loss=0.66 avg=0.88\n",
      "[3975 | 4813.05] loss=0.75 avg=0.88\n",
      "[3976 | 4814.64] loss=0.54 avg=0.88\n",
      "[3977 | 4816.24] loss=0.53 avg=0.87\n",
      "[3978 | 4817.83] loss=0.99 avg=0.87\n",
      "[3979 | 4819.43] loss=0.52 avg=0.87\n",
      "[3980 | 4821.02] loss=0.49 avg=0.87\n",
      "[3981 | 4822.61] loss=1.07 avg=0.87\n",
      "[3982 | 4824.20] loss=1.43 avg=0.87\n",
      "[3983 | 4825.81] loss=1.05 avg=0.88\n",
      "[3984 | 4827.40] loss=0.90 avg=0.88\n",
      "[3985 | 4829.00] loss=0.73 avg=0.87\n",
      "[3986 | 4830.60] loss=0.63 avg=0.87\n",
      "[3987 | 4832.20] loss=0.59 avg=0.87\n",
      "[3988 | 4833.79] loss=0.31 avg=0.86\n",
      "[3989 | 4835.39] loss=1.85 avg=0.87\n",
      "[3990 | 4836.98] loss=1.25 avg=0.88\n",
      "[3991 | 4838.58] loss=0.42 avg=0.87\n",
      "[3992 | 4840.18] loss=0.65 avg=0.87\n",
      "[3993 | 4841.78] loss=0.72 avg=0.87\n",
      "[3994 | 4843.38] loss=1.40 avg=0.87\n",
      "[3995 | 4844.97] loss=0.65 avg=0.87\n",
      "[3996 | 4846.57] loss=1.26 avg=0.88\n",
      "[3997 | 4848.16] loss=1.05 avg=0.88\n",
      "[3998 | 4849.76] loss=0.80 avg=0.88\n",
      "[3999 | 4851.36] loss=0.06 avg=0.87\n",
      "Saving checkpoint/run1/model-4000\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "v.getOutputStream(outStream);\n",
      "            if (this.outputThread.next()) {\n",
      "                  if (!isDebug) {\n",
      "                     debugLog(this, \"InputStream.getOutputStream() has \" +\n",
      "                               \" outputThread, trying to get its output\" );\n",
      "                } else {\n",
      "                      logger.warn(this, \"Inputstream.getOutputStream() has \"\n",
      "                                             \"outputThread, trying to get its output\")\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "\n",
      "[4000 | 4886.60] loss=0.95 avg=0.87\n",
      "[4001 | 4888.25] loss=0.97 avg=0.87\n",
      "[4002 | 4889.89] loss=0.72 avg=0.87\n",
      "[4003 | 4891.52] loss=2.85 avg=0.89\n",
      "[4004 | 4893.16] loss=0.50 avg=0.88\n",
      "[4005 | 4894.80] loss=0.80 avg=0.88\n",
      "[4006 | 4896.42] loss=0.94 avg=0.88\n",
      "[4007 | 4898.05] loss=0.80 avg=0.88\n",
      "[4008 | 4899.66] loss=0.73 avg=0.88\n",
      "[4009 | 4901.28] loss=0.50 avg=0.88\n",
      "[4010 | 4902.89] loss=0.85 avg=0.88\n",
      "[4011 | 4904.50] loss=0.92 avg=0.88\n",
      "[4012 | 4906.11] loss=0.92 avg=0.88\n",
      "[4013 | 4907.71] loss=1.03 avg=0.88\n",
      "[4014 | 4909.31] loss=1.33 avg=0.88\n",
      "[4015 | 4910.91] loss=1.01 avg=0.89\n",
      "[4016 | 4912.51] loss=1.23 avg=0.89\n",
      "[4017 | 4914.10] loss=0.47 avg=0.89\n",
      "[4018 | 4915.70] loss=1.00 avg=0.89\n",
      "[4019 | 4917.29] loss=0.99 avg=0.89\n",
      "[4020 | 4918.88] loss=0.96 avg=0.89\n",
      "[4021 | 4920.48] loss=0.94 avg=0.89\n",
      "[4022 | 4922.06] loss=0.59 avg=0.89\n",
      "[4023 | 4923.65] loss=1.32 avg=0.89\n",
      "[4024 | 4925.24] loss=0.95 avg=0.89\n",
      "[4025 | 4926.82] loss=0.84 avg=0.89\n",
      "[4026 | 4928.41] loss=1.01 avg=0.89\n",
      "[4027 | 4930.00] loss=2.68 avg=0.91\n",
      "[4028 | 4931.58] loss=0.89 avg=0.91\n",
      "[4029 | 4933.16] loss=0.79 avg=0.91\n",
      "[4030 | 4934.75] loss=0.79 avg=0.91\n",
      "[4031 | 4936.33] loss=0.98 avg=0.91\n",
      "[4032 | 4937.92] loss=1.00 avg=0.91\n",
      "[4033 | 4939.50] loss=1.22 avg=0.91\n",
      "[4034 | 4941.09] loss=0.61 avg=0.91\n",
      "[4035 | 4942.68] loss=1.16 avg=0.91\n",
      "[4036 | 4944.27] loss=0.39 avg=0.91\n",
      "[4037 | 4945.86] loss=1.17 avg=0.91\n",
      "[4038 | 4947.44] loss=0.75 avg=0.91\n",
      "[4039 | 4949.03] loss=0.76 avg=0.91\n",
      "[4040 | 4950.62] loss=0.90 avg=0.91\n",
      "[4041 | 4952.21] loss=0.68 avg=0.90\n",
      "[4042 | 4953.80] loss=0.48 avg=0.90\n",
      "[4043 | 4955.40] loss=1.49 avg=0.90\n",
      "[4044 | 4956.99] loss=0.67 avg=0.90\n",
      "[4045 | 4958.58] loss=0.80 avg=0.90\n",
      "[4046 | 4960.17] loss=0.35 avg=0.90\n",
      "[4047 | 4961.77] loss=0.59 avg=0.89\n",
      "[4048 | 4963.36] loss=0.95 avg=0.89\n",
      "[4049 | 4964.96] loss=0.91 avg=0.89\n",
      "[4050 | 4966.56] loss=0.26 avg=0.89\n",
      "[4051 | 4968.16] loss=0.65 avg=0.88\n",
      "[4052 | 4969.76] loss=1.12 avg=0.89\n",
      "[4053 | 4971.36] loss=0.53 avg=0.88\n",
      "[4054 | 4972.96] loss=0.72 avg=0.88\n",
      "[4055 | 4974.55] loss=0.74 avg=0.88\n",
      "[4056 | 4976.16] loss=0.38 avg=0.88\n",
      "[4057 | 4977.75] loss=1.16 avg=0.88\n",
      "[4058 | 4979.35] loss=0.59 avg=0.88\n",
      "[4059 | 4980.95] loss=1.13 avg=0.88\n",
      "[4060 | 4982.55] loss=0.91 avg=0.88\n",
      "[4061 | 4984.15] loss=0.53 avg=0.87\n",
      "[4062 | 4985.74] loss=0.53 avg=0.87\n",
      "[4063 | 4987.33] loss=0.73 avg=0.87\n",
      "[4064 | 4988.93] loss=1.15 avg=0.87\n",
      "[4065 | 4990.53] loss=0.66 avg=0.87\n",
      "[4066 | 4992.12] loss=0.98 avg=0.87\n",
      "[4067 | 4993.72] loss=0.89 avg=0.87\n",
      "[4068 | 4995.32] loss=0.49 avg=0.87\n",
      "[4069 | 4996.92] loss=0.43 avg=0.86\n",
      "[4070 | 4998.52] loss=0.92 avg=0.86\n",
      "[4071 | 5000.12] loss=0.74 avg=0.86\n",
      "[4072 | 5001.72] loss=0.91 avg=0.86\n",
      "[4073 | 5003.32] loss=0.77 avg=0.86\n",
      "[4074 | 5004.92] loss=1.17 avg=0.87\n",
      "[4075 | 5006.52] loss=0.70 avg=0.86\n",
      "[4076 | 5008.12] loss=0.54 avg=0.86\n",
      "[4077 | 5009.72] loss=1.09 avg=0.86\n",
      "[4078 | 5011.32] loss=0.45 avg=0.86\n",
      "[4079 | 5012.92] loss=0.53 avg=0.86\n",
      "[4080 | 5014.51] loss=1.07 avg=0.86\n",
      "[4081 | 5016.10] loss=0.31 avg=0.85\n",
      "[4082 | 5017.70] loss=1.36 avg=0.86\n",
      "[4083 | 5019.30] loss=1.11 avg=0.86\n",
      "[4084 | 5020.89] loss=0.49 avg=0.86\n",
      "[4085 | 5022.48] loss=0.75 avg=0.86\n",
      "[4086 | 5024.08] loss=0.72 avg=0.85\n",
      "[4087 | 5025.68] loss=1.62 avg=0.86\n",
      "[4088 | 5027.27] loss=0.32 avg=0.86\n",
      "[4089 | 5028.87] loss=0.78 avg=0.86\n",
      "[4090 | 5030.46] loss=1.23 avg=0.86\n",
      "[4091 | 5032.06] loss=0.79 avg=0.86\n",
      "[4092 | 5033.65] loss=1.48 avg=0.86\n",
      "[4093 | 5035.24] loss=0.59 avg=0.86\n",
      "[4094 | 5036.83] loss=0.60 avg=0.86\n",
      "[4095 | 5038.43] loss=0.68 avg=0.86\n",
      "[4096 | 5040.02] loss=0.10 avg=0.85\n",
      "[4097 | 5041.61] loss=0.92 avg=0.85\n",
      "[4098 | 5043.21] loss=0.60 avg=0.85\n",
      "[4099 | 5044.80] loss=0.66 avg=0.85\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " a new class.\n",
      "             }\n",
      "             return p;\n",
      "        }\n",
      "    }\n",
      "\n",
      "    /**\n",
      "     * @param p         the object that has information\n",
      "     * being processed by this class.\n",
      "     * \n",
      "     */\n",
      "    private class ByteTest extends BaseTestWrapper {\n",
      "\n",
      "             // This class was originally meant to read and write\n",
      "            // any data it wanted by reading or writing data to or\n",
      "            // from the target object. Unfortunately, that class\n",
      "            // was never meant to be used. Any reference to that\n",
      "            // object was just a simple, plain string. Unfortunately,\n",
      "            any data that came in from any other object\n",
      "            was read and written to the target object, too.\n",
      "         }\n",
      "\n",
      "        {\n",
      "         }\n",
      "    { // this case\n",
      "         case ByteDigits:\n",
      "         {\n",
      "            switch (x) {\n",
      "                Case 0:\n",
      "                  // we're going to read the same type of bytes that we read\n",
      "                  // in the other case, the code would not stop for one bit.\n",
      "                    {\n",
      "\n",
      "                      // we will also receive a data that comes from both sides\n",
      "                               // it would be more natural for us to read that bit\n",
      "                                to begin with, which makes sense.\n",
      "                                   int bit = data.get(0);\n",
      "\n",
      "                                  if (!(bit & 0xFFFF))\n",
      "                                   {\n",
      "                                    throw new UnsupportedOperationException(\"Cannot read Bit %d of %s in byte %s\n",
      "                                                                                                           !(Bit.MASK);\n",
      "                                             break;\n",
      "                                     }\n",
      "                                      return new ByteDigits.byteArray(x);\n",
      "                     \n",
      "\n",
      "[4100 | 5068.59] loss=0.19 avg=0.84\n",
      "[4101 | 5070.19] loss=1.04 avg=0.84\n",
      "[4102 | 5071.79] loss=0.80 avg=0.84\n",
      "[4103 | 5073.38] loss=0.41 avg=0.84\n",
      "[4104 | 5074.97] loss=0.54 avg=0.83\n",
      "[4105 | 5076.56] loss=0.84 avg=0.83\n",
      "[4106 | 5078.14] loss=1.58 avg=0.84\n",
      "[4107 | 5079.73] loss=0.15 avg=0.83\n",
      "[4108 | 5081.32] loss=1.19 avg=0.84\n",
      "[4109 | 5082.91] loss=0.84 avg=0.84\n",
      "[4110 | 5084.51] loss=0.62 avg=0.84\n",
      "[4111 | 5086.10] loss=0.27 avg=0.83\n",
      "[4112 | 5087.69] loss=0.67 avg=0.83\n",
      "[4113 | 5089.29] loss=0.99 avg=0.83\n",
      "[4114 | 5090.87] loss=0.61 avg=0.83\n",
      "[4115 | 5092.47] loss=1.19 avg=0.83\n",
      "[4116 | 5094.06] loss=1.15 avg=0.83\n",
      "[4117 | 5095.66] loss=1.87 avg=0.85\n",
      "[4118 | 5097.25] loss=0.56 avg=0.84\n",
      "[4119 | 5098.84] loss=0.40 avg=0.84\n",
      "[4120 | 5100.43] loss=1.01 avg=0.84\n",
      "[4121 | 5102.03] loss=1.01 avg=0.84\n",
      "[4122 | 5103.62] loss=0.75 avg=0.84\n",
      "[4123 | 5105.22] loss=1.28 avg=0.84\n",
      "[4124 | 5106.82] loss=0.76 avg=0.84\n",
      "[4125 | 5108.42] loss=0.71 avg=0.84\n",
      "[4126 | 5110.02] loss=0.94 avg=0.84\n",
      "[4127 | 5111.62] loss=0.80 avg=0.84\n",
      "[4128 | 5113.22] loss=0.88 avg=0.84\n",
      "[4129 | 5114.82] loss=0.83 avg=0.84\n",
      "[4130 | 5116.42] loss=0.93 avg=0.84\n",
      "[4131 | 5118.02] loss=1.06 avg=0.85\n",
      "[4132 | 5119.62] loss=1.08 avg=0.85\n",
      "[4133 | 5121.21] loss=0.81 avg=0.85\n",
      "[4134 | 5122.81] loss=1.02 avg=0.85\n",
      "[4135 | 5124.41] loss=1.30 avg=0.85\n",
      "[4136 | 5126.00] loss=0.82 avg=0.85\n",
      "[4137 | 5127.60] loss=0.67 avg=0.85\n",
      "[4138 | 5129.20] loss=0.96 avg=0.85\n",
      "[4139 | 5130.79] loss=0.80 avg=0.85\n",
      "[4140 | 5132.38] loss=0.44 avg=0.85\n",
      "[4141 | 5133.98] loss=0.09 avg=0.84\n",
      "[4142 | 5135.57] loss=0.82 avg=0.84\n",
      "[4143 | 5137.17] loss=1.18 avg=0.84\n",
      "[4144 | 5138.77] loss=0.95 avg=0.85\n",
      "[4145 | 5140.37] loss=0.47 avg=0.84\n",
      "[4146 | 5141.97] loss=1.28 avg=0.85\n",
      "[4147 | 5143.56] loss=1.37 avg=0.85\n",
      "[4148 | 5145.16] loss=0.93 avg=0.85\n",
      "[4149 | 5146.76] loss=1.27 avg=0.86\n",
      "[4150 | 5148.35] loss=0.45 avg=0.85\n",
      "[4151 | 5149.96] loss=0.80 avg=0.85\n",
      "[4152 | 5151.55] loss=1.01 avg=0.85\n",
      "[4153 | 5153.15] loss=0.33 avg=0.85\n",
      "[4154 | 5154.75] loss=0.80 avg=0.85\n",
      "[4155 | 5156.35] loss=1.18 avg=0.85\n",
      "[4156 | 5157.95] loss=0.62 avg=0.85\n",
      "[4157 | 5159.55] loss=0.74 avg=0.85\n",
      "[4158 | 5161.15] loss=1.42 avg=0.85\n",
      "[4159 | 5162.75] loss=1.10 avg=0.86\n",
      "[4160 | 5164.34] loss=0.71 avg=0.85\n",
      "[4161 | 5165.94] loss=1.01 avg=0.86\n",
      "[4162 | 5167.54] loss=0.75 avg=0.85\n",
      "[4163 | 5169.13] loss=0.99 avg=0.86\n",
      "[4164 | 5170.73] loss=1.00 avg=0.86\n",
      "[4165 | 5172.32] loss=0.72 avg=0.86\n",
      "[4166 | 5173.92] loss=0.99 avg=0.86\n",
      "[4167 | 5175.52] loss=1.34 avg=0.86\n",
      "[4168 | 5177.12] loss=1.40 avg=0.87\n",
      "[4169 | 5178.72] loss=0.80 avg=0.87\n",
      "[4170 | 5180.31] loss=0.48 avg=0.86\n",
      "[4171 | 5181.90] loss=0.65 avg=0.86\n",
      "[4172 | 5183.50] loss=0.79 avg=0.86\n",
      "[4173 | 5185.10] loss=1.32 avg=0.87\n",
      "[4174 | 5186.69] loss=0.95 avg=0.87\n",
      "[4175 | 5188.30] loss=0.44 avg=0.86\n",
      "[4176 | 5189.89] loss=0.91 avg=0.86\n",
      "[4177 | 5191.49] loss=1.09 avg=0.86\n",
      "[4178 | 5193.08] loss=0.29 avg=0.86\n",
      "[4179 | 5194.67] loss=1.06 avg=0.86\n",
      "[4180 | 5196.27] loss=0.74 avg=0.86\n",
      "[4181 | 5197.87] loss=1.24 avg=0.86\n",
      "[4182 | 5199.47] loss=0.27 avg=0.86\n",
      "[4183 | 5201.06] loss=0.65 avg=0.86\n",
      "[4184 | 5202.65] loss=0.45 avg=0.85\n",
      "[4185 | 5204.25] loss=1.02 avg=0.85\n",
      "[4186 | 5205.84] loss=1.09 avg=0.86\n",
      "[4187 | 5207.44] loss=0.43 avg=0.85\n",
      "[4188 | 5209.03] loss=0.92 avg=0.85\n",
      "[4189 | 5210.63] loss=0.95 avg=0.85\n",
      "[4190 | 5212.22] loss=0.57 avg=0.85\n",
      "[4191 | 5213.81] loss=1.16 avg=0.85\n",
      "[4192 | 5215.40] loss=0.54 avg=0.85\n",
      "[4193 | 5217.00] loss=0.63 avg=0.85\n",
      "[4194 | 5218.59] loss=0.84 avg=0.85\n",
      "[4195 | 5220.19] loss=1.47 avg=0.85\n",
      "[4196 | 5221.79] loss=0.88 avg=0.85\n",
      "[4197 | 5223.39] loss=1.03 avg=0.86\n",
      "[4198 | 5224.99] loss=0.71 avg=0.85\n",
      "[4199 | 5226.58] loss=0.68 avg=0.85\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " as the other.\n",
      "\t\t*\n",
      "\t\tstatic JFrame * newFrame = null;\n",
      "\t\t// Do not delete frame if not explicitly set.\n",
      "\t\tthis.frame = frame;\n",
      "\t}\n",
      "\t\n",
      "\t/*\n",
      "\t * Test for a broken \"frame\" or \"newFrame\" if the object has not yet\n",
      "\t * been given a frame or new frame to load if it is already there.\n",
      "\t * For such cases, simply use the instance frame to set\n",
      "\t * the object's frame instead.\n",
      "\t */\n",
      "\t@Test\n",
      "\tpublic void setObjectFrame(JFrame newFrame) {\n",
      "\t\tframe = newFrame;\n",
      "\t}\n",
      "\t\n",
      "\t/**\n",
      "\t * Get the new frame object that this instance is based on.\n",
      "\t * This method is used to change the type of object it is based off of.\n",
      "\t */\n",
      "\t@Test\n",
      "\tpublic void setObjectType(JFrame newFrame) {\n",
      "\t\ttype = (JFrame)newFrame.getType();\n",
      "\n",
      "\t\tif (this instance == null) {\n",
      "\t\t\tassertFalse(\"No instance object was added to the object loader.\");\n",
      "\t\t\tnewFrame = null;\n",
      "\t\t}\n",
      "\t\t\n",
      "\t\tsuper.setObjectType(newFrame);\n",
      "\t}\n",
      "\t\n",
      "\t@Test\n",
      "\tpublic void setObjectFrame(JComponent newFrame) {\n",
      "\t\tframe = newFrame;\n",
      "\t}\n",
      "\t\n",
      "\t/**\n",
      "\t * Check if we should load a new instance object from the loading manager as one more\n",
      "\t * instance if we have it with a frame loader. The object loader should load\n",
      "\t * this instance from the instance to save.\n",
      "\t */\n",
      "\t@Test\n",
      "\tpublic void setObjectFrameworkLoader(JConfiguration cfg) {\n",
      "\t\tif (cfg == null) {\n",
      "\t\t\tassertFalse(\"If we want to load from our instance's loading manager, only this instance is loaded as a frame\");\n",
      "\t\t\tCFG.loadingManagerManager.loadInstance(cfg, newFrame, null);\n",
      "\t\t}\n",
      "\t\t\n",
      "\t\tsuper.setObjectFrameworkLoader(cfg);\n",
      "\t}\n",
      "\t\n",
      "\t/**\n",
      "\t * Set the initial loading manager to the new instance.\n",
      "\t * \n",
      "\t * @param fg This is the frame loader.\n",
      "\t * @param newFrame This is the new instance frame.\n",
      "\t * @param newFrameType If type is null, load the frame as if it was a new frame.\n",
      "\t * @param newFrameTypeCode To load if type is null; load all frames as if they were new frames.\n",
      "\t */\n",
      "\tpublic void setNewFrameworkLoader(JComponent newFrameworkLoader,\n",
      "\t\t\tJFrame newFrame) {\n",
      "\t\tif (this instance == null) {\n",
      "\t\t\tassertFalse(\"No instance object was added to the object loader.\");\n",
      "\t\t\tnewFrame = null;\n",
      "\t\t}\n",
      "\t\t\n",
      "\t\tsuper.setNewFrameworkLoader(newFrameworkLoader, newFrame, newFrameTypeCodeCode);\n",
      "\t}\n",
      "\t\n",
      "\t/**\n",
      "\t * LoadFrame(File) loads the given content into the instance.\n",
      "\t */\n",
      "\tprivate void loadFrame(File f) {\n",
      "\t�File f2 = new File(f.getName());\n",
      "\t\tdouble jc = -3D.0f;\n",
      "\t\tthis.instanceFuncName = new File(f.getName());\n",
      "\t\tthis.newInstanceName = new File(f.getName());\n",
      "\t\ttry {\n",
      "\t\t\tthis.instanceFunc = new File(f.getName() + \"/frame.bcf\");\n",
      "\t\t\tthis.frame = f;\n",
      "\t\t} catch (ClassCastException e) {\n",
      "\t\t\tassertTrue(\"Unsupported property at \" + newInstanceName + \"/ instance. Could not load frame\");\n",
      "\t\t\tassertError(\"Unexpected parameter \"-1 (in file \"-1.bcf) or \"+file+\": 'File should be \" + newInstanceName +\".bcf'\" + e);\n",
      "\t\t\tassertTrue(\"Frame \" + file +\": \" + instanceFuncName + \" not available; '\" + file +\" should be used instead\");\n",
      "\t\t}\n",
      "\t\tthis.objectLoader = loadNewInstanceLoader(instanceFuncName + \"/\", newInstanceName, newInstanceName,\n",
      "\t\t\t\tf, jc);\n",
      "\t\ttry {\n",
      "\t\t\tthis.objectLoader = loadInstanceLoader(instanceFuncName, newInstanceName, f, jc);\n",
      "\t\t} catch (InvalidOperationException e) {\n",
      "\t\t\tassertTrue(\"Unable to load\n",
      "\n",
      "[4200 | 5250.02] loss=0.71 avg=0.85\n",
      "[4201 | 5251.62] loss=0.82 avg=0.85\n",
      "[4202 | 5253.21] loss=1.03 avg=0.85\n",
      "[4203 | 5254.80] loss=0.64 avg=0.85\n",
      "[4204 | 5256.39] loss=0.93 avg=0.85\n",
      "[4205 | 5257.98] loss=0.99 avg=0.85\n",
      "[4206 | 5259.56] loss=0.87 avg=0.85\n",
      "[4207 | 5261.15] loss=1.05 avg=0.86\n",
      "[4208 | 5262.74] loss=1.05 avg=0.86\n",
      "[4209 | 5264.33] loss=0.60 avg=0.85\n",
      "[4210 | 5265.92] loss=0.76 avg=0.85\n",
      "[4211 | 5267.51] loss=1.04 avg=0.86\n",
      "[4212 | 5269.10] loss=1.37 avg=0.86\n",
      "[4213 | 5270.69] loss=0.69 avg=0.86\n",
      "[4214 | 5272.28] loss=0.65 avg=0.86\n",
      "[4215 | 5273.86] loss=1.89 avg=0.87\n",
      "[4216 | 5275.45] loss=1.14 avg=0.87\n",
      "[4217 | 5277.05] loss=1.09 avg=0.87\n",
      "[4218 | 5278.64] loss=0.48 avg=0.87\n",
      "[4219 | 5280.23] loss=0.67 avg=0.87\n",
      "[4220 | 5281.82] loss=1.04 avg=0.87\n",
      "[4221 | 5283.41] loss=1.35 avg=0.87\n",
      "[4222 | 5285.01] loss=0.59 avg=0.87\n",
      "[4223 | 5286.60] loss=0.85 avg=0.87\n",
      "[4224 | 5288.20] loss=0.28 avg=0.86\n",
      "[4225 | 5289.79] loss=0.97 avg=0.86\n",
      "[4226 | 5291.39] loss=1.52 avg=0.87\n",
      "[4227 | 5292.99] loss=0.28 avg=0.87\n",
      "[4228 | 5294.59] loss=1.21 avg=0.87\n",
      "[4229 | 5296.19] loss=0.84 avg=0.87\n",
      "[4230 | 5297.79] loss=0.94 avg=0.87\n",
      "[4231 | 5299.39] loss=0.67 avg=0.87\n",
      "[4232 | 5300.99] loss=0.84 avg=0.87\n",
      "[4233 | 5302.59] loss=0.87 avg=0.87\n",
      "[4234 | 5304.19] loss=1.01 avg=0.87\n",
      "[4235 | 5305.80] loss=1.44 avg=0.87\n",
      "[4236 | 5307.40] loss=0.55 avg=0.87\n",
      "[4237 | 5308.99] loss=0.78 avg=0.87\n",
      "[4238 | 5310.59] loss=0.62 avg=0.87\n",
      "[4239 | 5312.19] loss=0.92 avg=0.87\n",
      "[4240 | 5313.79] loss=0.36 avg=0.86\n",
      "[4241 | 5315.38] loss=0.91 avg=0.86\n",
      "[4242 | 5316.98] loss=1.11 avg=0.87\n",
      "[4243 | 5318.57] loss=1.30 avg=0.87\n",
      "[4244 | 5320.17] loss=1.57 avg=0.88\n",
      "[4245 | 5321.77] loss=1.13 avg=0.88\n",
      "[4246 | 5323.36] loss=0.92 avg=0.88\n",
      "[4247 | 5324.95] loss=0.76 avg=0.88\n",
      "[4248 | 5326.55] loss=1.18 avg=0.88\n",
      "[4249 | 5328.15] loss=0.81 avg=0.88\n",
      "[4250 | 5329.74] loss=0.26 avg=0.88\n",
      "[4251 | 5331.34] loss=0.77 avg=0.87\n",
      "[4252 | 5332.93] loss=1.48 avg=0.88\n",
      "[4253 | 5334.53] loss=0.78 avg=0.88\n",
      "[4254 | 5336.12] loss=1.24 avg=0.88\n",
      "[4255 | 5337.72] loss=0.64 avg=0.88\n",
      "[4256 | 5339.31] loss=1.10 avg=0.88\n",
      "[4257 | 5340.90] loss=0.89 avg=0.88\n",
      "[4258 | 5342.49] loss=0.88 avg=0.88\n",
      "[4259 | 5344.08] loss=1.29 avg=0.89\n",
      "[4260 | 5345.67] loss=0.90 avg=0.89\n",
      "[4261 | 5347.26] loss=1.09 avg=0.89\n",
      "[4262 | 5348.85] loss=1.23 avg=0.89\n",
      "[4263 | 5350.44] loss=0.52 avg=0.89\n",
      "[4264 | 5352.04] loss=0.33 avg=0.88\n",
      "[4265 | 5353.63] loss=0.58 avg=0.88\n",
      "[4266 | 5355.22] loss=0.27 avg=0.87\n",
      "[4267 | 5356.82] loss=0.40 avg=0.87\n",
      "[4268 | 5358.41] loss=0.95 avg=0.87\n",
      "[4269 | 5360.01] loss=0.87 avg=0.87\n",
      "[4270 | 5361.60] loss=0.82 avg=0.87\n",
      "[4271 | 5363.20] loss=0.91 avg=0.87\n",
      "[4272 | 5364.80] loss=1.35 avg=0.87\n",
      "[4273 | 5366.39] loss=0.85 avg=0.87\n",
      "[4274 | 5367.99] loss=0.55 avg=0.87\n",
      "[4275 | 5369.58] loss=1.27 avg=0.88\n",
      "[4276 | 5371.17] loss=0.82 avg=0.87\n",
      "[4277 | 5372.77] loss=0.97 avg=0.88\n",
      "[4278 | 5374.37] loss=0.70 avg=0.87\n",
      "[4279 | 5375.96] loss=0.47 avg=0.87\n",
      "[4280 | 5377.56] loss=0.74 avg=0.87\n",
      "[4281 | 5379.16] loss=0.92 avg=0.87\n",
      "[4282 | 5380.76] loss=1.12 avg=0.87\n",
      "[4283 | 5382.36] loss=0.93 avg=0.87\n",
      "[4284 | 5383.96] loss=1.46 avg=0.88\n",
      "[4285 | 5385.55] loss=0.40 avg=0.87\n",
      "[4286 | 5387.14] loss=1.05 avg=0.87\n",
      "[4287 | 5388.73] loss=0.64 avg=0.87\n",
      "[4288 | 5390.32] loss=0.93 avg=0.87\n",
      "[4289 | 5391.91] loss=1.10 avg=0.88\n",
      "[4290 | 5393.51] loss=1.20 avg=0.88\n",
      "[4291 | 5395.11] loss=0.31 avg=0.87\n",
      "[4292 | 5396.71] loss=0.94 avg=0.87\n",
      "[4293 | 5398.30] loss=1.24 avg=0.88\n",
      "[4294 | 5399.89] loss=0.10 avg=0.87\n",
      "[4295 | 5401.49] loss=1.21 avg=0.87\n",
      "[4296 | 5403.09] loss=1.43 avg=0.88\n",
      "[4297 | 5404.68] loss=1.31 avg=0.88\n",
      "[4298 | 5406.28] loss=1.19 avg=0.89\n",
      "[4299 | 5407.87] loss=0.65 avg=0.88\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      " */ return this.getObject(new Object[2]); }\n",
      "    public boolean isBoolean()\n",
      "    {\n",
      "        boolean result = false;\n",
      "        boolean[] b;\n",
      "        for (int i = 0; i < 2; i++) {\n",
      "             boolean value = (boolean)(getString(i) == value ? 1 : 0);\n",
      "            Object element = mData.getBounds(i);\n",
      "            for (int j = 0; j < 1; j++) {\n",
      "                 value = mData.getString(i, j);\n",
      "                 result = (boolean[]) value.add(value);\n",
      "            }\n",
      "            if (result == null)\n",
      "                result = result + 1;\n",
      "            else\n",
      "                result = (boolean[]) value;\n",
      "        }\n",
      "        return result;\n",
      "    }\n",
      "    public String toString() {\n",
      "        return String.format(\"{ }.\", this.getObject(this.getString(this.getBounds(this.getBounds()));\n",
      "\n",
      "    }\n",
      "    /* Getters for setters. */\n",
      "    public boolean[] getBoolean2() {\n",
      "        String s = \"\"; for (int i = 0; i < 2; i++) {\n",
      "            return {\n",
      "                  super.getBoolean(i),\n",
      "                      super.getString(i),\n",
      "                       0,\n",
      "              }\n",
      "        }\n",
      "        return new Boolean[2];\n",
      "    }\n",
      "\n",
      "    /**\n",
      "     * Gets the first child element of the container. If this child is not in the root path\n",
      "     * and has a <code>null</code> then <code>true</code> is returned.\n",
      "     *\n",
      "     * @return true for the setter for the first child element of this container\n",
      "     */\n",
      "    public boolean[] getBoolean2(boolean a,\n",
      "     boolean b) {\n",
      "       return this.getObject(null, b);\n",
      "    }\n",
      "    private void setBoolean(boolean a) {\n",
      "       if (a==null) return ;\n",
      "      mString = \"\";\n",
      "      for (int k = 0; k < this.getObject(a).toString(); k++) {\n",
      "          if (!a.equals(this.getObject(a)))\n",
      "            return ;\n",
      "         if (a==null) return ;\n",
      "         mString = a.substring(k++,a.length()-1);\n",
      "      }\n",
      "      }\n",
      "      if (this.getObject(this.getString(this.getBounds(this.getBounds())))==null) return ;\n",
      "      int length = this.size();\n",
      "      int length2 = this.size();\n",
      "\n",
      "      for (int k = 0; k < this.size(); k++) {\n",
      "         int j = 0;\n",
      "         int j2 = 0;\n",
      "        mString = \"\";\n",
      "        int j2 = 0;\n",
      "    \n",
      "\n",
      "[4300 | 5431.82] loss=0.76 avg=0.88\n",
      "[4301 | 5433.42] loss=0.86 avg=0.88\n",
      "[4302 | 5435.01] loss=0.53 avg=0.88\n",
      "[4303 | 5436.60] loss=1.67 avg=0.89\n",
      "[4304 | 5438.20] loss=0.76 avg=0.89\n",
      "[4305 | 5439.80] loss=0.83 avg=0.88\n",
      "[4306 | 5441.39] loss=1.15 avg=0.89\n",
      "[4307 | 5442.99] loss=1.56 avg=0.89\n",
      "[4308 | 5444.58] loss=1.12 avg=0.90\n",
      "[4309 | 5446.17] loss=1.02 avg=0.90\n",
      "[4310 | 5447.77] loss=0.79 avg=0.90\n",
      "[4311 | 5449.36] loss=0.74 avg=0.89\n",
      "[4312 | 5450.95] loss=0.67 avg=0.89\n",
      "[4313 | 5452.54] loss=1.13 avg=0.90\n",
      "[4314 | 5454.12] loss=0.92 avg=0.90\n",
      "[4315 | 5455.71] loss=0.37 avg=0.89\n",
      "[4316 | 5457.30] loss=0.58 avg=0.89\n",
      "[4317 | 5458.89] loss=0.61 avg=0.88\n",
      "[4318 | 5460.48] loss=1.05 avg=0.89\n",
      "[4319 | 5462.07] loss=0.78 avg=0.88\n",
      "[4320 | 5463.66] loss=0.87 avg=0.88\n",
      "[4321 | 5465.25] loss=0.63 avg=0.88\n",
      "[4322 | 5466.84] loss=0.63 avg=0.88\n",
      "[4323 | 5468.43] loss=0.76 avg=0.88\n",
      "[4324 | 5470.02] loss=1.43 avg=0.88\n",
      "[4325 | 5471.61] loss=1.00 avg=0.89\n",
      "[4326 | 5473.20] loss=1.11 avg=0.89\n",
      "[4327 | 5474.80] loss=0.46 avg=0.88\n",
      "[4328 | 5476.39] loss=0.80 avg=0.88\n",
      "[4329 | 5477.98] loss=0.73 avg=0.88\n",
      "[4330 | 5479.57] loss=1.03 avg=0.88\n",
      "[4331 | 5481.16] loss=0.28 avg=0.88\n",
      "[4332 | 5482.75] loss=0.80 avg=0.88\n",
      "[4333 | 5484.34] loss=0.88 avg=0.88\n",
      "[4334 | 5485.93] loss=0.99 avg=0.88\n",
      "[4335 | 5487.52] loss=0.77 avg=0.88\n",
      "[4336 | 5489.11] loss=0.99 avg=0.88\n",
      "[4337 | 5490.70] loss=1.30 avg=0.88\n",
      "[4338 | 5492.29] loss=1.37 avg=0.89\n",
      "[4339 | 5493.88] loss=1.04 avg=0.89\n",
      "[4340 | 5495.48] loss=0.83 avg=0.89\n",
      "[4341 | 5497.08] loss=1.17 avg=0.89\n",
      "[4342 | 5498.67] loss=1.23 avg=0.89\n",
      "[4343 | 5500.27] loss=0.99 avg=0.89\n",
      "[4344 | 5501.86] loss=1.57 avg=0.90\n",
      "[4345 | 5503.46] loss=0.81 avg=0.90\n",
      "[4346 | 5505.06] loss=1.10 avg=0.90\n",
      "[4347 | 5506.66] loss=0.73 avg=0.90\n",
      "[4348 | 5508.26] loss=0.75 avg=0.90\n",
      "[4349 | 5509.86] loss=0.52 avg=0.90\n",
      "[4350 | 5511.46] loss=0.94 avg=0.90\n",
      "[4351 | 5513.06] loss=0.76 avg=0.89\n",
      "[4352 | 5514.65] loss=1.73 avg=0.90\n",
      "[4353 | 5516.26] loss=1.13 avg=0.90\n",
      "[4354 | 5517.85] loss=1.33 avg=0.91\n",
      "[4355 | 5519.45] loss=1.00 avg=0.91\n",
      "[4356 | 5521.05] loss=0.42 avg=0.91\n",
      "[4357 | 5522.65] loss=1.29 avg=0.91\n",
      "[4358 | 5524.24] loss=0.55 avg=0.91\n",
      "[4359 | 5525.84] loss=0.71 avg=0.90\n",
      "[4360 | 5527.44] loss=0.72 avg=0.90\n",
      "[4361 | 5529.04] loss=0.64 avg=0.90\n",
      "[4362 | 5530.64] loss=0.79 avg=0.90\n",
      "[4363 | 5532.24] loss=1.43 avg=0.90\n",
      "[4364 | 5533.83] loss=0.86 avg=0.90\n",
      "[4365 | 5535.42] loss=0.63 avg=0.90\n",
      "[4366 | 5537.02] loss=0.93 avg=0.90\n",
      "[4367 | 5538.62] loss=1.22 avg=0.90\n",
      "[4368 | 5540.21] loss=1.12 avg=0.91\n",
      "[4369 | 5541.81] loss=0.88 avg=0.91\n",
      "[4370 | 5543.40] loss=0.48 avg=0.90\n",
      "[4371 | 5545.00] loss=1.25 avg=0.90\n",
      "[4372 | 5546.59] loss=0.63 avg=0.90\n",
      "[4373 | 5548.19] loss=1.23 avg=0.91\n",
      "[4374 | 5549.79] loss=1.08 avg=0.91\n",
      "[4375 | 5551.39] loss=1.07 avg=0.91\n",
      "[4376 | 5552.98] loss=0.51 avg=0.90\n",
      "[4377 | 5554.57] loss=0.59 avg=0.90\n",
      "[4378 | 5556.17] loss=0.72 avg=0.90\n",
      "[4379 | 5557.76] loss=0.62 avg=0.90\n",
      "[4380 | 5559.35] loss=0.93 avg=0.90\n",
      "[4381 | 5560.95] loss=0.98 avg=0.90\n",
      "[4382 | 5562.55] loss=0.20 avg=0.89\n",
      "[4383 | 5564.14] loss=1.56 avg=0.90\n",
      "[4384 | 5565.74] loss=0.40 avg=0.89\n",
      "[4385 | 5567.33] loss=1.23 avg=0.90\n",
      "[4386 | 5568.93] loss=0.40 avg=0.89\n",
      "[4387 | 5570.53] loss=1.04 avg=0.89\n",
      "[4388 | 5572.13] loss=0.72 avg=0.89\n",
      "[4389 | 5573.72] loss=0.45 avg=0.89\n",
      "[4390 | 5575.32] loss=1.18 avg=0.89\n",
      "[4391 | 5576.92] loss=1.21 avg=0.89\n",
      "[4392 | 5578.52] loss=1.30 avg=0.90\n",
      "[4393 | 5580.12] loss=0.51 avg=0.89\n",
      "[4394 | 5581.72] loss=1.22 avg=0.90\n",
      "[4395 | 5583.32] loss=1.39 avg=0.90\n",
      "[4396 | 5584.91] loss=1.13 avg=0.90\n",
      "[4397 | 5586.51] loss=0.45 avg=0.90\n",
      "[4398 | 5588.11] loss=0.99 avg=0.90\n",
      "[4399 | 5589.70] loss=0.99 avg=0.90\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "            assertNotNull(\"isIn(\"+isInString));\n",
      "                assertNotNull(\"isIn(\"+isInByte));\n",
      "                assertNotNull \"isIn(\"+isInByte));\n",
      "                assertNotNull \"isIn(\"+isInLong));\n",
      "               assertNotNull \"isIn(\"+isInNb));\n",
      "               assertNotNull \"isIn(\"+isInCharArray[]));\n",
      "               assertNotNull \"id();\");\n",
      "              assertNotNull \"newChar().isIn(\"+isInCharArray\");\n",
      "               assertNotNull  \", $;\");\n",
      "               assertNotNull \"isIn(\"+isInString[]);\n",
      "   }\n",
      "   \n",
      "   public void testSet() throws Exception {\n",
      "         return newArray(3);\n",
      "    }\n",
      "    public String[]    TestArraySizes() {\n",
      "         return newArray(5);\n",
      "    }\n",
      "}\n",
      "<|endoftext|>/*\n",
      " * Copyright (c) 1998-2003, Red Hat Inc. and others.\n",
      " * All rights reserved. This program and the accompanying materials\n",
      " * are made available under the terms of the Eclipse Public License v1.0\n",
      " * which accompanies this distribution, and is available at\n",
      " * http://www.eclipse.org/legal/epl-v10.html.\n",
      " *\n",
      " * Contributors:\n",
      " *    Red Hat Inc. - initial API and implementation\n",
      " * $Revision: 197579\n",
      " ******************************************************************************/\n",
      "package org.eclipse.ecf.webgui.core.runtime;\n",
      "\n",
      "import javax.swing.*;\n",
      "import javax.swing.ActionFrame;\n",
      "import javax.swing.JOptionPane;\n",
      "import javax.swing.JPanel;\n",
      "import javax.swing.JPanelGroup;\n",
      "import javax.swing.JPanelSource;\n",
      "import javax.swing.JPanelStateContainer;\n",
      "import javax.swing.JPanelTask;\n",
      "import org.eclipse.ecf.WebGUI;\n",
      "\n",
      "\n",
      "/**\n",
      " * This extends the Viewer, giving better controls for\n",
      " * debugging and profiling.\n",
      " *\n",
      " * @since 2.2.0\n",
      " */\n",
      "public class DebugTask extends AbstractViewerTask {\n",
      "\n",
      "\u0019private final final ActionFrame frame;\n",
      "\n",
      "\tprivate final ActionPane task;\n",
      "\n",
      "\tprivate JFile sourceFile;\n",
      "\n",
      "\tprivate final JFileListener listener;\n",
      "\n",
      "\tpublic DebugTask() {\n",
      "\t\tframe = new ActionFrame();\n",
      "\t\ttask = new JFileListener() {\n",
      "\t\t\tpublic void show(String message) { task.show(message); }\n",
      "\t\t\tpublic void showFromFile(String message) { task.showFromFile(message); }\n",
      "\t\t\t@Override\n",
      "\t\t\tpublic void addTask(String task) {\n",
      "\t\t\t\ttask.addToTask(task);\n",
      "\t\t\t\ttask.setEnabled(false);\n",
      "\t\t\t}\n",
      "\t\t\tpublic void removeTask(String task) {\n",
      "\t\t\t\ttask.removeFromTask(task);\n",
      "\t\t\t}\n",
      "\n",
      "\t\t\tfinal JPanelLayout layout = new JPanelLayout();\n",
      "\t\t\tlayout.add(JFileWidget.newWindowLayout(sourceFile, task, new JPanelContentPane()));\n",
      "\t\t\ttask = new JPanelTask(layout);\n",
      "\t\t}\n",
      "\n",
      "\t\t@Override\n",
      "\t\tpublic void setTask(String task, JPanel panel) {\n",
      "\t\t\ttask.setFocusInterval(200);\n",
      "\t\t}\n",
      "\n",
      "\t\tpublic void showFromJFile(String message) {\n",
      "\t\t\ttask.showFromJFile(message);\n",
      "\t\t}\n",
      "\n",
      "\t\tprivate final JDialog jDialogPane;\n",
      "\t\tprivate final JFileListener listener;\n",
      "\n",
      "\t\tpublic synchronized ViewerTask(JFrame frame) {\n",
      "\t\t\twindow = frame;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "}\n",
      "<|endoftext|>/*\n",
      " * Copyright (c\n",
      "\n",
      "[4400 | 5613.43] loss=0.85 avg=0.90\n",
      "[4401 | 5615.02] loss=1.35 avg=0.90\n",
      "[4402 | 5616.62] loss=1.64 avg=0.91\n",
      "[4403 | 5618.21] loss=0.94 avg=0.91\n",
      "[4404 | 5619.80] loss=1.18 avg=0.91\n",
      "[4405 | 5621.39] loss=0.81 avg=0.91\n",
      "[4406 | 5622.98] loss=1.20 avg=0.92\n",
      "[4407 | 5624.57] loss=0.82 avg=0.92\n",
      "[4408 | 5626.17] loss=0.56 avg=0.91\n",
      "[4409 | 5627.76] loss=0.78 avg=0.91\n",
      "[4410 | 5629.35] loss=1.26 avg=0.91\n",
      "[4411 | 5630.94] loss=0.57 avg=0.91\n",
      "[4412 | 5632.54] loss=1.23 avg=0.91\n",
      "[4413 | 5634.12] loss=1.24 avg=0.92\n",
      "[4414 | 5635.72] loss=0.69 avg=0.92\n",
      "[4415 | 5637.31] loss=0.63 avg=0.91\n",
      "[4416 | 5638.90] loss=1.32 avg=0.92\n",
      "[4417 | 5640.49] loss=1.43 avg=0.92\n",
      "[4418 | 5642.09] loss=0.95 avg=0.92\n",
      "[4419 | 5643.68] loss=1.32 avg=0.93\n",
      "[4420 | 5645.28] loss=0.87 avg=0.93\n",
      "[4421 | 5646.86] loss=1.09 avg=0.93\n",
      "[4422 | 5648.46] loss=1.13 avg=0.93\n",
      "[4423 | 5650.06] loss=0.70 avg=0.93\n",
      "[4424 | 5651.66] loss=1.15 avg=0.93\n",
      "[4425 | 5653.26] loss=0.44 avg=0.92\n",
      "[4426 | 5654.85] loss=0.97 avg=0.92\n",
      "[4427 | 5656.46] loss=0.48 avg=0.92\n",
      "[4428 | 5658.05] loss=0.64 avg=0.92\n",
      "[4429 | 5659.65] loss=0.66 avg=0.91\n",
      "[4430 | 5661.24] loss=1.39 avg=0.92\n",
      "[4431 | 5662.85] loss=0.95 avg=0.92\n",
      "[4432 | 5664.44] loss=1.42 avg=0.92\n",
      "[4433 | 5666.04] loss=0.16 avg=0.92\n",
      "[4434 | 5667.64] loss=0.25 avg=0.91\n",
      "[4435 | 5669.23] loss=1.24 avg=0.91\n",
      "[4436 | 5670.83] loss=0.65 avg=0.91\n",
      "[4437 | 5672.42] loss=0.69 avg=0.91\n",
      "[4438 | 5674.02] loss=1.10 avg=0.91\n",
      "[4439 | 5675.61] loss=0.75 avg=0.91\n",
      "[4440 | 5677.20] loss=0.84 avg=0.91\n",
      "[4441 | 5678.80] loss=0.68 avg=0.91\n",
      "[4442 | 5680.40] loss=0.62 avg=0.90\n",
      "[4443 | 5682.00] loss=1.04 avg=0.90\n",
      "[4444 | 5683.59] loss=0.31 avg=0.90\n",
      "[4445 | 5685.19] loss=0.98 avg=0.90\n",
      "[4446 | 5686.80] loss=0.72 avg=0.90\n",
      "[4447 | 5688.40] loss=0.24 avg=0.89\n",
      "[4448 | 5690.00] loss=0.70 avg=0.89\n",
      "[4449 | 5691.60] loss=0.82 avg=0.89\n",
      "[4450 | 5693.20] loss=0.81 avg=0.89\n",
      "[4451 | 5694.79] loss=1.12 avg=0.89\n",
      "[4452 | 5696.39] loss=0.79 avg=0.89\n",
      "[4453 | 5697.97] loss=0.53 avg=0.89\n",
      "[4454 | 5699.57] loss=1.21 avg=0.89\n",
      "[4455 | 5701.17] loss=0.91 avg=0.89\n",
      "[4456 | 5702.77] loss=0.58 avg=0.89\n",
      "[4457 | 5704.37] loss=1.29 avg=0.89\n",
      "[4458 | 5705.96] loss=0.98 avg=0.89\n",
      "[4459 | 5707.56] loss=1.57 avg=0.90\n",
      "[4460 | 5709.15] loss=0.90 avg=0.90\n",
      "[4461 | 5710.74] loss=0.58 avg=0.89\n",
      "[4462 | 5712.34] loss=1.32 avg=0.90\n",
      "[4463 | 5713.93] loss=0.83 avg=0.90\n",
      "[4464 | 5715.52] loss=1.38 avg=0.90\n",
      "[4465 | 5717.12] loss=1.11 avg=0.91\n",
      "[4466 | 5718.71] loss=0.97 avg=0.91\n",
      "[4467 | 5720.31] loss=0.55 avg=0.90\n",
      "[4468 | 5721.90] loss=0.87 avg=0.90\n",
      "[4469 | 5723.49] loss=0.23 avg=0.90\n",
      "[4470 | 5725.09] loss=0.78 avg=0.89\n",
      "[4471 | 5726.69] loss=0.93 avg=0.89\n",
      "[4472 | 5728.29] loss=1.15 avg=0.90\n",
      "[4473 | 5729.89] loss=1.55 avg=0.90\n",
      "[4474 | 5731.48] loss=0.80 avg=0.90\n",
      "[4475 | 5733.08] loss=1.05 avg=0.90\n",
      "[4476 | 5734.68] loss=0.79 avg=0.90\n",
      "[4477 | 5736.27] loss=0.95 avg=0.90\n",
      "[4478 | 5737.87] loss=1.35 avg=0.91\n",
      "[4479 | 5739.47] loss=1.15 avg=0.91\n",
      "[4480 | 5741.07] loss=1.35 avg=0.91\n",
      "[4481 | 5742.66] loss=1.11 avg=0.92\n",
      "[4482 | 5744.26] loss=1.17 avg=0.92\n",
      "[4483 | 5745.85] loss=1.34 avg=0.92\n",
      "[4484 | 5747.45] loss=0.87 avg=0.92\n",
      "[4485 | 5749.05] loss=1.05 avg=0.92\n",
      "[4486 | 5750.64] loss=1.31 avg=0.93\n",
      "[4487 | 5752.24] loss=1.11 avg=0.93\n",
      "[4488 | 5753.84] loss=1.04 avg=0.93\n",
      "[4489 | 5755.43] loss=0.61 avg=0.93\n",
      "[4490 | 5757.03] loss=0.98 avg=0.93\n",
      "[4491 | 5758.63] loss=1.10 avg=0.93\n",
      "[4492 | 5760.23] loss=1.05 avg=0.93\n",
      "[4493 | 5761.82] loss=0.81 avg=0.93\n",
      "[4494 | 5763.42] loss=1.12 avg=0.93\n",
      "[4495 | 5765.02] loss=0.41 avg=0.93\n",
      "[4496 | 5766.61] loss=0.69 avg=0.92\n",
      "[4497 | 5768.21] loss=1.39 avg=0.93\n",
      "[4498 | 5769.81] loss=1.02 avg=0.93\n",
      "[4499 | 5771.40] loss=0.93 avg=0.93\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "              setInterval(1000, 5, 200);\n",
      "                  int numHits = hit.countHits();\n",
      "                  try { hit.setInterval(1000, 5);\n",
      "              } catch (InterruptedException e) {\n",
      "                  e.printStackTrace();\n",
      "              }\n",
      "            }\n",
      "            if (numHits == hit.countHits() ) {\n",
      "                 if (hit.getInterval() > 5) {\n",
      "                     // wait till next tick\n",
      "                     if (numHits >= 5) {\n",
      "                         hit.updateCursor(hit);\n",
      "                     }\n",
      "                 }\n",
      "             } else if (numHits >= 5) {\n",
      "                   hit.updateCursor(hit);\n",
      "               } else {\n",
      "                   hit.setInterval(5, hit.countLines());\n",
      "               }\n",
      "            }\n",
      "            hit = new Hit(hit, numHits);\n",
      "            hit.setDamage(20);\n",
      "            hit.setId(1);\n",
      "            hit.setStart(0);\n",
      "            hit.setEnd(0);\n",
      "            hit.setHit(hit);\n",
      "            hit.setId(hit.getId());\n",
      "            hit.setStart(0);\n",
      "            hit.setEnd(0);\n",
      "            throw new BlockException(\n",
      "               \"Hit was invalid,\",\n",
      "               hit,\n",
      "               \"numHits=\\\"\" + hit.getNumLines() +\n",
      "             \"}, hit=\\\"\" + hit.getEndOffset());\n",
      "            throw new BlockException(\n",
      "               \"Hit cannot be null\");\n",
      "         }\n",
      "         switch (hit.getInterval()) {\n",
      "            case 5:\n",
      "                hit.setLines(hit.getLines());\n",
      "                break;\n",
      "             case 10:\n",
      "                hit.setLines(hit.getLen());\n",
      "                break;\n",
      "             case 15:\n",
      "                hit.setLines(hit.getLen());\n",
      "                break;\n",
      "         case 0:\n",
      " \n",
      "\n",
      "[4500 | 5794.85] loss=1.05 avg=0.93\n",
      "[4501 | 5796.45] loss=0.44 avg=0.93\n",
      "[4502 | 5798.04] loss=0.89 avg=0.93\n",
      "[4503 | 5799.63] loss=0.80 avg=0.92\n",
      "[4504 | 5801.22] loss=1.32 avg=0.93\n",
      "[4505 | 5802.81] loss=0.70 avg=0.93\n",
      "[4506 | 5804.40] loss=1.14 avg=0.93\n",
      "[4507 | 5806.00] loss=0.79 avg=0.93\n",
      "[4508 | 5807.58] loss=0.73 avg=0.92\n",
      "[4509 | 5809.17] loss=0.90 avg=0.92\n",
      "[4510 | 5810.76] loss=0.76 avg=0.92\n",
      "[4511 | 5812.35] loss=0.89 avg=0.92\n",
      "[4512 | 5813.94] loss=0.36 avg=0.92\n",
      "[4513 | 5815.52] loss=0.54 avg=0.91\n",
      "[4514 | 5817.11] loss=1.21 avg=0.92\n",
      "[4515 | 5818.70] loss=1.25 avg=0.92\n",
      "[4516 | 5820.29] loss=0.18 avg=0.91\n",
      "[4517 | 5821.87] loss=0.44 avg=0.91\n",
      "[4518 | 5823.47] loss=0.97 avg=0.91\n",
      "[4519 | 5825.06] loss=1.59 avg=0.91\n",
      "[4520 | 5826.65] loss=0.65 avg=0.91\n",
      "[4521 | 5828.24] loss=1.60 avg=0.92\n",
      "[4522 | 5829.90] loss=0.27 avg=0.91\n",
      "[4523 | 5831.49] loss=0.65 avg=0.91\n",
      "[4524 | 5833.09] loss=1.01 avg=0.91\n",
      "[4525 | 5834.68] loss=0.82 avg=0.91\n",
      "[4526 | 5836.27] loss=0.54 avg=0.91\n",
      "[4527 | 5837.87] loss=0.43 avg=0.90\n",
      "[4528 | 5839.46] loss=0.82 avg=0.90\n",
      "[4529 | 5841.06] loss=1.23 avg=0.90\n",
      "[4530 | 5842.65] loss=1.24 avg=0.91\n",
      "[4531 | 5844.24] loss=1.38 avg=0.91\n",
      "[4532 | 5845.83] loss=0.68 avg=0.91\n",
      "[4533 | 5847.43] loss=1.28 avg=0.91\n",
      "[4534 | 5849.02] loss=1.13 avg=0.92\n",
      "[4535 | 5850.60] loss=0.48 avg=0.91\n",
      "[4536 | 5852.19] loss=1.06 avg=0.91\n",
      "[4537 | 5853.79] loss=0.16 avg=0.90\n",
      "[4538 | 5855.38] loss=1.55 avg=0.91\n",
      "[4539 | 5856.98] loss=0.60 avg=0.91\n",
      "[4540 | 5858.57] loss=1.09 avg=0.91\n",
      "[4541 | 5860.17] loss=0.94 avg=0.91\n",
      "[4542 | 5861.76] loss=1.22 avg=0.91\n",
      "[4543 | 5863.36] loss=1.13 avg=0.92\n",
      "[4544 | 5864.96] loss=0.32 avg=0.91\n",
      "[4545 | 5866.55] loss=0.82 avg=0.91\n",
      "[4546 | 5868.15] loss=1.80 avg=0.92\n",
      "[4547 | 5869.75] loss=1.32 avg=0.92\n",
      "[4548 | 5871.34] loss=0.65 avg=0.92\n",
      "[4549 | 5872.94] loss=1.03 avg=0.92\n",
      "[4550 | 5874.54] loss=1.22 avg=0.92\n",
      "[4551 | 5876.13] loss=0.61 avg=0.92\n",
      "[4552 | 5877.73] loss=0.38 avg=0.91\n",
      "[4553 | 5879.32] loss=0.44 avg=0.91\n",
      "[4554 | 5880.92] loss=0.63 avg=0.91\n",
      "[4555 | 5882.52] loss=1.09 avg=0.91\n",
      "[4556 | 5884.11] loss=0.86 avg=0.91\n",
      "[4557 | 5885.71] loss=0.90 avg=0.91\n",
      "[4558 | 5887.30] loss=0.35 avg=0.90\n",
      "[4559 | 5888.89] loss=0.94 avg=0.90\n",
      "[4560 | 5890.49] loss=0.98 avg=0.90\n",
      "[4561 | 5892.08] loss=0.52 avg=0.90\n",
      "[4562 | 5893.68] loss=0.80 avg=0.90\n",
      "[4563 | 5895.28] loss=0.77 avg=0.90\n",
      "[4564 | 5896.88] loss=1.33 avg=0.90\n",
      "[4565 | 5898.47] loss=0.77 avg=0.90\n",
      "[4566 | 5900.07] loss=0.28 avg=0.89\n",
      "[4567 | 5901.67] loss=0.89 avg=0.89\n",
      "[4568 | 5903.27] loss=0.60 avg=0.89\n",
      "[4569 | 5904.86] loss=0.54 avg=0.89\n",
      "[4570 | 5906.45] loss=1.01 avg=0.89\n",
      "[4571 | 5908.05] loss=0.91 avg=0.89\n",
      "[4572 | 5909.65] loss=0.56 avg=0.89\n",
      "[4573 | 5911.25] loss=0.61 avg=0.88\n",
      "[4574 | 5912.85] loss=1.25 avg=0.89\n",
      "[4575 | 5914.45] loss=1.53 avg=0.89\n",
      "[4576 | 5916.04] loss=0.44 avg=0.89\n",
      "[4577 | 5917.63] loss=0.69 avg=0.89\n",
      "[4578 | 5919.22] loss=1.61 avg=0.89\n",
      "[4579 | 5920.83] loss=0.90 avg=0.89\n",
      "[4580 | 5922.42] loss=0.85 avg=0.89\n",
      "[4581 | 5924.02] loss=0.97 avg=0.89\n",
      "[4582 | 5925.62] loss=0.98 avg=0.90\n",
      "[4583 | 5927.20] loss=1.33 avg=0.90\n",
      "[4584 | 5928.80] loss=0.36 avg=0.89\n",
      "[4585 | 5930.39] loss=0.68 avg=0.89\n",
      "[4586 | 5931.98] loss=0.48 avg=0.89\n",
      "[4587 | 5933.57] loss=1.40 avg=0.89\n",
      "[4588 | 5935.17] loss=1.04 avg=0.89\n",
      "[4589 | 5936.76] loss=1.42 avg=0.90\n",
      "[4590 | 5938.35] loss=0.92 avg=0.90\n",
      "[4591 | 5939.95] loss=0.70 avg=0.90\n",
      "[4592 | 5941.55] loss=0.79 avg=0.90\n",
      "[4593 | 5943.15] loss=1.06 avg=0.90\n",
      "[4594 | 5944.74] loss=0.75 avg=0.90\n",
      "[4595 | 5946.34] loss=0.68 avg=0.89\n",
      "[4596 | 5947.94] loss=0.76 avg=0.89\n",
      "[4597 | 5949.53] loss=0.46 avg=0.89\n",
      "[4598 | 5951.13] loss=0.79 avg=0.89\n",
      "[4599 | 5952.73] loss=1.15 avg=0.89\n",
      "Generating samples...\n",
      "======== SAMPLE 1 ========\n",
      "      out.write(fName);\n",
      "\n",
      "         out.flush();\n",
      "         return out.toString();\n",
      "    }\n",
      "\n",
      "    public int get(int i) {\n",
      "        float fNum = new float(20 / (3 * 1000 / 24))) / 10.0;\n",
      "        return newfloat(Math.pow(fNum, 1.0f), 0.9f);\n",
      "    }\n",
      "    @Override\n",
      "    public static double get(double fM) {\n",
      "        double fMnum = new double(mNum * fM);\n",
      "        fMnum.reset();\n",
      "        fMnum = new double(mNum * 100 * fM);\n",
      "        fMnum.reset();\n",
      "        fMnum = new double(mNum * 24 * fM);\n",
      "        fMnum.reset();\n",
      "        fMnum = new double(mNum * 100 * fM);\n",
      "        fMnum.reset();\n",
      "        return fMnum;\n",
      "    }\n",
      "    public double get(double fM) {\n",
      "        double fMnum = new double(mNum * fM);\n",
      "        fMnum.reset();\n",
      "        fMnum = new double(mNum * 100 * fM);\n",
      "         fMnum.reset();\n",
      "        return fMnum;\n",
      "    }\n",
      "    public double get(double fMnum) {\n",
      "        double fMatn = new double(mNum * fMatn);\n",
      "        fMatn.reset();\n",
      "        fMatn = new double(mNum * 100 * fMatn);\n",
      "        fMatn.reset();\n",
      "        fMatn = new double(mNum * 24 * fMatn);\n",
      "        fMatn.reset();\n",
      "        return fMatn;\n",
      "    }\n",
      "    public Long get(long fM) {\n",
      "        return fM.reset();\n",
      "    }\n",
      "\n",
      "    public String get(String fName) throws IndexOutOfBoundsException {\n",
      "        String sName = get(fName);\n",
      "        // If name is invalid or we have no name, we'll give the default of a plain\n",
      "        // string\n",
      "        if (!fName.equalsIgnoreCase(sName)) {\n",
      "         throw new IllegalArgumentException();\n",
      "        } else {\n",
      "         throw new IndexOutOfBoundsException();\n",
      "        }\n",
      "        return sName;\n",
      "    }\n",
      "\n",
      "    public String get(String fName ) throws IndexOutOfBoundsException {\n",
      "        if (fName.equalsIgnoreCase(sName)) {\n",
      "           throw new IndexOutOfBoundsException();\n",
      "        } else {\n",
      "           throw new IndexBackwardException();\n",
      "        }\n",
      "        return sName;\n",
      "    }\n",
      "\n",
      "    public Long get(long fM) throws IndexOutOfBoundsException {\n",
      "        if (fM.eqName()) {\n",
      "           throw new IndexBackwardException();\n",
      "        } else {\n",
      "           throw new IndexInaccessibleException();\n",
      "        }\n",
      "        return fM.reset();\n",
      "    }\n",
      "\n",
      "\n",
      "    public int get(int i) {\n",
      "        return new int(i * 100000) // 1000001000 <= max\n",
      " \n",
      "\n",
      "[4600 | 5976.15] loss=0.82 avg=0.89\n",
      "[4601 | 5977.74] loss=1.38 avg=0.90\n",
      "[4602 | 5979.33] loss=0.81 avg=0.89\n",
      "[4603 | 5980.93] loss=0.88 avg=0.89\n",
      "[4604 | 5982.51] loss=0.59 avg=0.89\n",
      "[4605 | 5984.10] loss=0.20 avg=0.88\n",
      "[4606 | 5985.70] loss=1.03 avg=0.89\n",
      "[4607 | 5987.29] loss=0.55 avg=0.88\n",
      "[4608 | 5988.88] loss=0.84 avg=0.88\n",
      "[4609 | 5990.47] loss=1.42 avg=0.89\n",
      "[4610 | 5992.06] loss=0.15 avg=0.88\n",
      "[4611 | 5993.65] loss=1.56 avg=0.89\n",
      "[4612 | 5995.24] loss=0.77 avg=0.89\n",
      "[4613 | 5996.84] loss=1.20 avg=0.89\n",
      "[4614 | 5998.43] loss=1.14 avg=0.89\n",
      "[4615 | 6000.02] loss=1.38 avg=0.90\n",
      "[4616 | 6001.62] loss=0.57 avg=0.89\n",
      "[4617 | 6003.21] loss=0.79 avg=0.89\n",
      "[4618 | 6004.79] loss=1.14 avg=0.89\n",
      "[4619 | 6006.38] loss=1.21 avg=0.90\n",
      "[4620 | 6007.97] loss=1.41 avg=0.90\n",
      "[4621 | 6009.56] loss=1.18 avg=0.91\n",
      "[4622 | 6011.15] loss=0.62 avg=0.90\n",
      "[4623 | 6012.75] loss=1.36 avg=0.91\n",
      "[4624 | 6014.34] loss=0.73 avg=0.91\n",
      "[4625 | 6015.93] loss=1.32 avg=0.91\n",
      "[4626 | 6017.52] loss=0.76 avg=0.91\n",
      "[4627 | 6019.12] loss=1.07 avg=0.91\n",
      "[4628 | 6020.71] loss=1.11 avg=0.91\n",
      "[4629 | 6022.30] loss=0.89 avg=0.91\n",
      "[4630 | 6023.89] loss=1.48 avg=0.92\n",
      "[4631 | 6025.49] loss=0.66 avg=0.91\n",
      "[4632 | 6027.08] loss=0.75 avg=0.91\n",
      "[4633 | 6028.67] loss=0.96 avg=0.91\n",
      "[4634 | 6030.26] loss=0.77 avg=0.91\n",
      "[4635 | 6031.85] loss=0.85 avg=0.91\n",
      "[4636 | 6033.44] loss=0.57 avg=0.91\n",
      "[4637 | 6035.03] loss=0.88 avg=0.91\n",
      "[4638 | 6036.63] loss=0.47 avg=0.90\n",
      "[4639 | 6038.22] loss=1.14 avg=0.91\n",
      "[4640 | 6039.82] loss=0.55 avg=0.90\n",
      "[4641 | 6041.41] loss=0.96 avg=0.90\n",
      "[4642 | 6043.01] loss=0.60 avg=0.90\n",
      "[4643 | 6044.60] loss=0.84 avg=0.90\n",
      "[4644 | 6046.19] loss=0.96 avg=0.90\n",
      "[4645 | 6047.79] loss=0.82 avg=0.90\n",
      "[4646 | 6049.39] loss=0.81 avg=0.90\n",
      "[4647 | 6050.99] loss=1.17 avg=0.90\n",
      "[4648 | 6052.58] loss=0.48 avg=0.90\n",
      "[4649 | 6054.19] loss=0.33 avg=0.89\n",
      "[4650 | 6055.79] loss=1.01 avg=0.89\n",
      "[4651 | 6057.38] loss=0.53 avg=0.89\n",
      "[4652 | 6058.98] loss=0.85 avg=0.89\n",
      "[4653 | 6060.58] loss=1.12 avg=0.89\n",
      "[4654 | 6062.17] loss=0.81 avg=0.89\n",
      "[4655 | 6063.78] loss=1.11 avg=0.89\n",
      "[4656 | 6065.38] loss=0.79 avg=0.89\n",
      "[4657 | 6066.98] loss=1.38 avg=0.90\n",
      "[4658 | 6068.57] loss=0.85 avg=0.89\n",
      "[4659 | 6070.17] loss=1.50 avg=0.90\n",
      "[4660 | 6071.77] loss=0.62 avg=0.90\n",
      "[4661 | 6073.36] loss=0.70 avg=0.90\n",
      "[4662 | 6074.96] loss=0.50 avg=0.89\n",
      "interrupted\n",
      "Saving checkpoint/run1/model-4663\n"
     ]
    }
   ],
   "source": [
    "!PYTHONPATH=src ./train.py --dataset ../java_projects --model_name '345M'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vS1RJJDFOPnb"
   },
   "source": [
    "Save our checkpoints to start training again later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2328,
     "status": "ok",
     "timestamp": 1559092458087,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "JretqG1zOXdi",
    "outputId": "17ccb4c5-cfa0-4b8d-fa71-439787dc1ba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot create directory '/content/drive/My Drive/': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/ /content/drive/My\\ Drive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6D-i7vERWbNS"
   },
   "source": [
    "Load your trained model for use in sampling below (117M or 345M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VeETvWvrbKga"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "np0r6qfXBeUX"
   },
   "outputs": [],
   "source": [
    "!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/345M/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmnSrXqtfRbq"
   },
   "source": [
    "Generate conditional samples from the model given a prompt you provide -  change top-k hyperparameter if desired (default is 40),  if you're using 345M, add \"--model-name 345M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7500
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1045982,
     "status": "ok",
     "timestamp": 1559093703237,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 240
    },
    "id": "utJj-iY4gHwE",
    "outputId": "b43f4461-040a-4281-9cce-218663bc6ef4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-29 01:17:42.237182: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
      "2019-05-29 01:17:42.237511: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x33f0680 executing computations on platform Host. Devices:\n",
      "2019-05-29 01:17:42.237554: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2019-05-29 01:17:42.432852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2019-05-29 01:17:42.433377: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x33effa0 executing computations on platform CUDA. Devices:\n",
      "2019-05-29 01:17:42.433421: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2019-05-29 01:17:42.433828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
      "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
      "pciBusID: 0000:00:04.0\n",
      "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
      "2019-05-29 01:17:42.433856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
      "2019-05-29 01:17:42.907987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2019-05-29 01:17:42.908054: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
      "2019-05-29 01:17:42.908066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
      "2019-05-29 01:17:42.908317: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2019-05-29 01:17:42.908361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14115 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "Model prompt >>> /**\n",
      "2019-05-29 01:18:41.333115: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
      "======================================== SAMPLE 1 ========================================\n",
      " * A map from one of the input strings to the corresponding key/value pairs. \n",
      " * @param string a key/value pair.\n",
      " * @param keyValue a string value.\n",
      " * @inheritDoc\n",
      " * @see org.apache.tapestry5.util.ListEntry#containsKey(org.jooq.annotations.List)\n",
      " * @since 1.1.0\n",
      " */\n",
      "public static final class ListEntry<B> implements Entry<String> {\n",
      "\n",
      "\tprivate final Map<String, String> keyValue;\n",
      "\n",
      "\tprivate final List<B> keyList;\n",
      "\n",
      "\tprivate final String[] key;\n",
      "\n",
      "\t/**\n",
      "\t * The key values.  If the key values do not match, null will be returned.\n",
      "\t * If all the key values are null, an empty set will be returned.\n",
      "\t */\n",
      "\tpublic final List<B> getKeyValue(final String[] key) throws KeyException {\n",
      "\t\tif (keyValue == null) {\n",
      "\t\t\treturn null;\n",
      "\t�}\n",
      "\n",
      "\t\tlistValues = new ArrayList<B>();\n",
      "\t\tfor (final String value : valueString.toLowerCase()) {\n",
      "\t\t\tlistValues.addAll(entryValues);\n",
      "\t\t}\n",
      "\n",
      "\t\t/*\n",
      "\t\t * The value.  If the key value does not match the value from the\n",
      "\t\t * KeyEntryEntry.\n",
      "\t\t * If all the value values are null, an empty set will be returned.\n",
      "\t\t */\n",
      "\t\tfinal Object value = entryValues.iterator();\n",
      "\t\tif (value instanceof String) {\n",
      "\t\t\tnullValue = value;\n",
      "\t\t\tvalue = entryValues.nextObject();\n",
      "\t\t\tif (value instanceof StringArray) {\n",
      "\t\t\t\tvalue = entryValues.nextLong();\n",
      "\t\t\t\tresult.remove(value);\n",
      "\t\t\t\tvalue = entryValues.nextByteArray();\n",
      "\t\t\t\tresult.remove(value);\n",
      "\t\t\t\tsetValue(value, value.toURI());\n",
      "\t\t\t}\n",
      "\t\t\tvalues.addObject(value);\n",
      "\t\t\tnewValue = entryValues.nextObject();\n",
      "\t\t\tif (newValue instanceof StringArray) {\n",
      "\t\t\t\tvalue = newValue;\n",
      "\t\t\t}\n",
      "\t\t\tnewValue\n",
      "================================================================================\n",
      "Model prompt >>> /* This method setup an HTTP connection\n",
      "======================================== SAMPLE 1 ========================================\n",
      ", the use of this method is recommended to prevent a race condition.\n",
      "//  // JSR-347: [RFC5223:SOCK]\n",
      "//  //  * The \"http\" method returns the address, not the protocol number.\n",
      "//  //  *\n",
      "//  // If the \"http\" method returns \"http://\" because it is used to retrieve information about a file or directory,\n",
      "//   // you should use a <tt>http://\" </tt> alternative instead.\n",
      "// \n",
      "//  /** The 'http.'\n",
      "//   * Return the http address for the connection. This is the address of the\n",
      "//   * http server. For example the HTTP:// address returned by the 'http'\n",
      "//   * method returns \"http://127.0.0.1:3000\". This parameter should\n",
      "//   * not be used to query the http server for a specific port number.\n",
      "//   */\n",
      "//  /** Return the <tt>protocol</tt>\n",
      "//   * Return the protocol number for the file or directory and the protocol\n",
      "//   * version number for the transport.\n",
      "//   * For example the HTTP:///path/to/file/ returns \"1.2.3.4.5\". A\n",
      "//   * <tt><tt>protocol</tt></tt> in a URL can be\n",
      "//   * represented by\n",
      "//   * the string \"<tt>http://127.0.0.1/path/>\".\n",
      "//   * If the protocol parameter has the specified value, return this\n",
      "//   * value, not <tt>protocol</tt> if it has been used to retrieve the\n",
      "//   * protocol. If the protocol parameter is not specified when using the\n",
      "//   * <tt>http://</tt> method, all of the information returned in response to the request is returned in the HTTP response. The method <tt>http://</tt> returns that information only when the method <tt>http://</tt> returns information. When using this method the method <tt>http://</tt> returns information in the HTTP response that will be returned. When using this method the method <tt>http:///usr/share/doc/spring-junit4/spring-junit4-core/java/util/regex/regex.re\n",
      "================================================================================\n",
      "Model prompt >>> /* This method takes in an ArrayList and sorts it\n",
      "======================================== SAMPLE 1 ========================================\n",
      " using the default order of the List items\n",
      " * rather than using the most recently\n",
      " * appended first.  That is, because the item can't have the same order in the\n",
      " * array, the item is not appended to first, but it is appended to\n",
      " * a list of previously appended items. \n",
      " * If the user specifies items they can sort in the default order, they must\n",
      " * specify the following sort operation:\n",
      " */\n",
      "public static final int DEFAULT_SORT_OPTION; /* The default is to sort items in the\n",
      " * same order that the List items are appended, so item can't be\n",
      " * appended to the list first or second. Also, to allow users to sort in the\n",
      " * default order, they need to specify items in this order first, so the\n",
      " * order of an appended list of items in this array is the same\n",
      " * as an ordered list of items in the List array. */\n",
      "public static final int DEFAULT; /* The default is the list order.  This defaults\n",
      " * to the order of items which are most recently appended to the\n",
      " * ArrayList.  (Note that the order can be changed by the user if they want to\n",
      " * create an ordered list.)  If the user specifies items, they need to specify\n",
      " * the following sort operation:\n",
      " */\n",
      "public static final int EXTENT_SORT_OPTION; /* A greater or equal integer\n",
      " * value, indicating sorting in the most recent order.\n",
      " */\n",
      "public static final int EXTENT; /* This is the last sorted array item, which\n",
      " * has the most recently appended items appended to it.\n",
      " */\n",
      "public static final int ENTTYPES = ArrayList.fromIterator(M_BAR_EMPTY_ROWS);\n",
      "}\n",
      "<|endoftext|>package gmp.gmp.common.extract.xmldata;\n",
      "\n",
      "import java.util.concurrent.atomic.AtomicInteger;\n",
      "\n",
      "import gmp.gmp.common.extract.Extractor;\n",
      "\n",
      "public class Cursor {\n",
      "\n",
      "virtual void clear();\n",
      "\n",
      "\n",
      "virtual void add(final AtomicInteger item);\n",
      "\n",
      "\n",
      "virtual void clear();\n",
      "\n",
      "\n",
      "virtual void set(final AtomicInteger nextItem);\n",
      "\n",
      "\n",
      "virtual void set(final AtomicInteger lastItem);\n",
      "\n",
      "\n",
      "virtual void clear(); // this\n",
      "\n",
      "virtual void increment();\n",
      "\n",
      "\n",
      "protected static final long serial\n",
      "================================================================================\n",
      "Model prompt >>> public class Bob\n",
      "======================================== SAMPLE 1 ========================================\n",
      " is User(boolean is_owner) {\n",
      "\n",
      "\t\tprivate User owner;\n",
      "\n",
      "\t\tpublic Bob getOwner() {\n",
      "\n",
      "\t\t\treturn owner;\n",
      "\t\t}\n",
      "\n",
      "\t\tpublic boolean isOwner() {\n",
      "\t\t\treturn owner;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\tpublic void initialize(Boelectric bob) {\n",
      "\t\towner = ((boolean)bob instanceof User)) ? userManager .get(owner) : null;\n",
      "\n",
      "\t\tthis.owner = owner;\n",
      "\n",
      "\t}\n",
      "\n",
      "\tpublic void removeOwner() {\n",
      "\t\towner = null;\n",
      "\t}\n",
      "\n",
      "\tpublic boolean getOwner() {\n",
      "\t\treturn owner;\n",
      "\t}\n",
      "\n",
      "\tpublic boolean isOwner() {\n",
      "\t\treturn owner;\n",
      "\t}\n",
      "\t\n",
      "\tpublic class Bob is User(boolean is_owner) {\n",
      "\n",
      "\t\tprivate User owner;\n",
      "\n",
      "\t\tpublic Bob getOwner() {\n",
      "\t\t\treturn owner;\n",
      "\t\t}\n",
      "\t}\n",
      "\tpublic void initialize() {\n",
      "\t\towner = (boolean)bob instanceof User);\n",
      "\t\townerManager.put(Bob, owner);\n",
      "\t\tuserManager.get(Bob, ownerManager.get(OwnerManager.class));\n",
      "\t\townerManager.remove(OwnerManager.class);\n",
      "\t\townerManager.set(OwnerManager.class,OwnerManager.class);\n",
      "\t\tbob = new OwnerManager();\n",
      "\t\tbobManager.add(Bob, ownerManager.get(0));\n",
      "\t\tbobManager.remove(OwnerManager.class);\n",
      "\t\townerManager.get(1).add(Bob);\n",
      "\t\townerManager.get(8).remove(OwnerManager.class);\n",
      "\t\townerManager.get(30).add(Bob);\n",
      "\t\tthis.owner = ownerManager.get(ownerManager.get(OwnerManager.class));\n",
      "\t\townerManager.get(31).add(Bob);\n",
      "\t\treturn ownerManager.get(ownerManager.get(1));\n",
      "\t}\n",
      "\tpublic void destroy() {\n",
      "\t\tOwnerManager.destroyOwner(ownerManager,OwnerManager.class);\n",
      "\t\tOwnerManager.getOwnerManager().removeOwner(ownerManager,OwnerManager.class);\n",
      "\t\townerManager.getOwnerManager().removeOwner(ownerManager,OwnerManager.class);\n",
      "\t\townerManager.get\n",
      "================================================================================\n",
      "Model prompt >>> import com.java.Sky\n",
      "======================================== SAMPLE 1 ========================================\n",
      "pe.SkypeMessageDialog.DialogItem);\n",
      "\n",
      "     /** An error message when an invalid number is requested. This message is shown when the client has selected another user. */\n",
      "    private void setUser(MessageDialogItem user) {\n",
      "         user.setErrorMessage(MessageDialogItem.ERROR);\n",
      "    }\n",
      "\n",
      "    private void setProgressBar(ProgressBar pb) {\n",
      "         pb.setProgressBar(pb.getMainFrame().getProgressBar());\n",
      "    }\n",
      "\n",
      "    private void setMessageDialogItem(com.java.Skype.SkypeMessageDialog messages,\n",
      "                MessageDialogItem messageDialog) {\n",
      "         this.messageDialog = messages;\n",
      "    }\n",
      "\n",
      "    private void setProgressBar(ProgressBar progressBar) {\n",
      "         progressBar.setProgressBar(progressBar.getMainFrame().getProgressBar());\n",
      "    }\n",
      "\n",
      "    private void setMessageDialogItem(com.java.Skype.SkypeMessageDialog dialog,\n",
      "               MessageDialogItem messageDialogTo, MessageDialogItem message) {\n",
      "         dialog.setProgressBar(messageDialogTo.checkProgressBar());\n",
      "    }\n",
      "}\n",
      "<|endoftext|>package com.java.Skype.SkypeMessageDialog;\n",
      "\n",
      "import java.util.*;\n",
      "\n",
      "public class ErrorMessage extends MessageDialog {\n",
      "\n",
      "public ErrorMessage(com.java.Skype.SkypeMessageDialog dialog) {\n",
      "         msg = Dialog.createMessageDialog(dialog);\n",
      "        msg.setError(MessageDialogItem.ERROR);\n",
      "    }\n",
      "\n",
      "    private void setErrorMessage(ErrorMessage msg) {\n",
      "        msg.setError(MessageDialogItem.ERROR);\n",
      "    }\n",
      "\n",
      "    private void setProgressBar(ProgressBar progressBar) {\n",
      "        progressBar.setProgressBar(progressBar.getMainFrame().getProgressBar());\n",
      " \n",
      "================================================================================\n",
      "Model prompt >>> import java.Bob.Henry\n",
      "======================================== SAMPLE 1 ========================================\n",
      "\n",
      "\t.*;\n",
      "\t/*\n",
      "quickShip * @author Michael T. Hager - @junit\n",
      "\t */\n",
      "\tpublic class Bob {\n",
      "\t\t/**\n",
      "\t\t * Initialize the field\n",
      "\t\t * @param fieldName\n",
      "\t\t */\n",
      "\t\tBob(Field(FieldName.BIGFOOT))\n",
      "\t\t{\n",
      "\t\t\tthis.fieldName = fieldName;}\n",
      "\t\tBob(Field(T.BIGFOOT))\n",
      "\t\t{\n",
      "\t\t\tthis.fieldName = fieldName;}\n",
      "\t\tBag.addFieldWithField(this, fieldName, new Bob());\n",
      "\t\treturn (this.fieldName);\n",
      "\t}\n",
      "\t\n",
      "\tfinal void unset(String fieldName) {\n",
      "\t\tthis.fieldName = fieldName;}\n",
      "\n",
      "\tfinal void set(String fieldName, String value) {\n",
      "\t\tthis.fieldName = fieldName;\n",
      "\t\tthis.fieldName = value;\n",
      "\t\tthis.fieldName = value;\n",
      "\t\treturn;\n",
      "\t}\n",
      "\n",
      "\t@Override\n",
      "\tpublic boolean equals(Object o0, Object o1) {\n",
      "\t\treturn o0 == o1 ? (o0 == o1) : ((o0 == o1) ? o1.getClass() : (o0 == null));\n",
      "\t}\n",
      "\n",
      "\t@Override\n",
      "\tpublic Object hashCode() {\n",
      "\t\tfor (int i = 0; i < fields.size(); i++) {\n",
      "\t\t\tobject valueObj0 = fields.get(i);\n",
      "\t\t\tobject valueObj1 = fields.get(i);\n",
      "\t\t\tif (valueObj0 instanceof BigField) {\n",
      "\t\t\t\tBigField valueObj01 = fields.get(i);\n",
      "\t\t\t\tif (valueObj1 instanceof BigField) {\n",
      "\t\t\t\t\tT.BIGFOOT valueObj1 = fields.get(i);\n",
      "\t\t\t\t\tBag.saveField(valueObj0, valueObj1);\n",
      "\t\t\t\t\treturn valueObj1 = valueObj01;\n",
      "\t\t\t\t\tvalueObj0 = fields.get(i);\n",
      "\t\t\t\t\tvalueObj1 = fields.get(i);\n",
      "\t\t\t\t\tvalueObj0 = fields.get(i);\n",
      "\t\t\t\t\tvalueObj1\n",
      "================================================================================\n",
      "Model prompt >>> In the night, you heard the sounds of\n",
      "======================================== SAMPLE 1 ========================================\n",
      " the night.\"\n",
      "\n",
      "Hudson, J. R., & Williams, D. S. (1993). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 151, 621-623.\n",
      "Hudson, J. R., & Williamson, C. V. (1993). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 151, 623-626.\n",
      "Hudson, J. R., & Williamson, C. V. (1995). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 154, 626-626.\n",
      "Hudson, J. R., & Williamson, C. V. (1995). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 154, 627-627.\n",
      "Hudson, J. R., & Williamson, C. V. (1997a). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 156, 628-628.\n",
      "Hudson, J. R., & Williamson, C. V. (1997b). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 156, 629-635.\n",
      "Hudson, J. R., & Williamson, C. V. (1998). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 160, 636-637.\n",
      "Hudson, J. R., & Williamson, C. V. (1998a). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 160, 638-642.\n",
      "Hudson, J. R., & Williamson, C. V. (1998b). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 160, 643-645.\n",
      "Hudson, J. R., & Williamson, C. V. (1999). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 166, 646-647.\n",
      "Hudson, J. R., & Williamson, C. V. (1999). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 166, 648-650.\n",
      "Hudson, M. B., & Williamson, U. M. (1999). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 169, 651-661.\n",
      "Hudson, J. R., & Williamson, C. V. (1999a). On the auditory perception of spectral frequencies. American Journal of Psychiatry, 169,\n",
      "================================================================================\n",
      "Model prompt >>> You heard a noise come from the basement. You knew there was only one thing that could be down there, but it was dead, no way it could be making sounds. The door to the basement slammed open and in the doorway appeared\n",
      "======================================== SAMPLE 1 ========================================\n",
      " an extremely ugly looking woman. She was obviously pregnant and had a huge ugly belly. She had green eyes and had a large nose. She also had a large bulge in her belly and two large blue eyes were shining brightly. She had long gray hair hanging out her back, just barely covering her breasts.\"\n",
      "*\n",
      "*Note that in the source code, the words \"pregnant\"? were added as such. \n",
      "*\n",
      "* This is my first time writing a game. \n",
      "* \n",
      "* I really hope to be good in it. \n",
      "*\n",
      "* The game is called \"Pregnancy\" and \n",
      "* The characters are called \"The Mother\" and \"The Baby\".\n",
      "*\n",
      "* The characters are all named after \n",
      "* the female members of The Church of Satan.\n",
      "*\n",
      "* I really really need your help.\n",
      "*\n",
      "* If I didn't want to let you know about my game it would just take me another few hours\n",
      "* and I wouldn't know for sure, I might say something like I have done a lot of work on it. If I am still interested\n",
      "* in the project and I can continue writing, I might give you my code.\n",
      "*\n",
      "* (The game is called in English as \"The Mother\")\n",
      "*\n",
      "\n",
      "  * Copyright (c) 1999 to present by Karel van der Marel , all rights reserved \n",
      " *\n",
      "* This program and the accompanying materials are made available under the terms of\n",
      "* the Eclipse Public License v1.0 which accompanies this distribution,\n",
      "* which can be found in the project root file epl-1.0-GPL.txt \n",
      "*\n",
      "* Contributors:\n",
      "* \n",
      "* Karel van der Marel - initial API and game implementation.  All rights\n",
      "* reserved\n",
      "*\n",
      "*\n",
      "* @author karel@jle.nl\n",
      "*/\n",
      "package org.jle.core.player.item.;\n",
      "\n",
      "/**\n",
      " * A player is represented \n",
      "* by a number of items which can represent the individual \n",
      "* player who wears the items. Each item has a unique \n",
      "* number associated with \n",
      "* it.\n",
      "* \n",
      "* @author karel@jle.nl\n",
      "*/\n",
      "public class PlayerItem extends\n",
      "*  Player\n",
      "* {\n",
      "* private static final int numberOfItems;\n",
      "* private int numberOfItemsPerGame;\n",
      "*\n",
      "* /**\n",
      "* \n",
      "* @\n",
      "================================================================================\n",
      "Model prompt >>> Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
      "    yield g\n",
      "  File \"src/interactive_conditional_samples.py\", line 71, in interact_model\n",
      "    raw_text = input(\"Model prompt >>> \")\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"src/interactive_conditional_samples.py\", line 89, in <module>\n",
      "    fire.Fire(interact_model)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 127, in Fire\n",
      "    component_trace = _Fire(component, args, context, name)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 366, in _Fire\n",
      "    component, remaining_args)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/fire/core.py\", line 542, in _CallCallable\n",
      "    result = fn(*varargs, **kwargs)\n",
      "  File \"src/interactive_conditional_samples.py\", line 86, in interact_model\n",
      "    print(\"=\" * 80)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
      "    tf_session.TF_CloseSession(self._session)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python3 src/interactive_conditional_samples.py --top_k 40 --model_name \"345M\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LeDhY97XMDXn"
   },
   "source": [
    "To check flag descriptions, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pBaj2L_KMAgb"
   },
   "outputs": [],
   "source": [
    "!python3 src/interactive_conditional_samples.py -- --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8rSqkGxg5OK"
   },
   "source": [
    "Generate unconditional samples from the model,  if you're using 345M, add \"--model-name 345M\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LaQUEnRxWc3c"
   },
   "outputs": [],
   "source": [
    "!python3 src/generate_unconditional_samples.py --model_name \"345M\" | tee /tmp/samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VM1Hag-JL3Bt"
   },
   "source": [
    "To check flag descriptions, use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sdxfye-SL66I"
   },
   "outputs": [],
   "source": [
    "!python3 src/generate_unconditional_samples.py -- --help"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "fine_tune_GPT-2.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/ak9250/gpt-2-colab/blob/master/GPT_2.ipynb",
     "timestamp": 1558992642605
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
