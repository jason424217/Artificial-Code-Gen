{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 1.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.5/dist-packages (from keras) (1.16.3)\n",
      "Collecting scipy>=0.14 (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/49/8f13fa215e10a7ab0731cc95b0e9bb66cf83c6a98260b154cfbd0b55fb19/scipy-1.3.0-cp35-cp35m-manylinux1_x86_64.whl (25.1MB)\n",
      "\u001b[K     |████████████████████████████████| 25.1MB 4.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyyaml (from keras)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/837fefac7475963d1eccf4aa684c23b95aa6c1d033a2c5965ccb11e22623/PyYAML-5.1.1.tar.gz (274kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 1.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from keras) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from keras) (1.0.7)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from keras) (2.9.0)\n",
      "Building wheels for collected packages: pyyaml\n",
      "  Building wheel for pyyaml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/16/27/a1/775c62ddea7bfa62324fd1f65847ed31c55dadb6051481ba3f\n",
      "Successfully built pyyaml\n",
      "Installing collected packages: scipy, pyyaml, keras\n",
      "Successfully installed keras-2.2.4 pyyaml-5.1.1 scipy-1.3.0\n",
      "\u001b[33mWARNING: You are using pip version 19.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 59.5M  100 59.5M    0     0  3728k      0  0:00:16  0:00:16 --:--:-- 4113k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   227  100   227    0     0    373      0 --:--:-- --:--:-- --:--:--   373\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.7M  100 10.7M    0     0  2698k      0  0:00:04  0:00:04 --:--:-- 2698k      0  0:00:05  0:00:02  0:00:03 2006k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 61.6M  100 61.6M    0     0  3687k      0  0:00:17  0:00:17 --:--:-- 3988k   0     0  3653k      0  0:00:17  0:00:15  0:00:02 3731k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   229  100   229    0     0    322      0 --:--:-- --:--:-- --:--:--   322\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 11.1M  100 11.1M    0     0  2600k      0  0:00:04  0:00:04 --:--:-- 2992k\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 13.7M  100 13.7M    0     0  2856k      0  0:00:04  0:00:04 --:--:-- 3232k72136      0  0:03:20  0:00:01  0:03:19 72093\n"
     ]
    }
   ],
   "source": [
    "#@title Download the pretrained keras model files\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_model.h5 -o ./data/cnn_wiki_tox_v3_model.h5\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_hparams.h5 -o ./data/cnn_wiki_tox_v3_hparams.h5\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_wiki_tox_v3_tokenizer.pkl -o ./data/cnn_wiki_tox_v3_tokenizer.pkl\n",
    "\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_model.h5 -o ./data/cnn_debias_tox_v3_model.h5\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_hparams.h5 -o ./data/cnn_debias_tox_v3_hparams.h5\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/cnn_debias_tox_v3_tokenizer.pkl -o ./data/cnn_debias_tox_v3_tokenizer.pkl\n",
    "\n",
    "!curl -L https://storage.googleapis.com/what-if-tool-resources/computefest2019/wiki_test.csv -o ./data/wiki_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#@title Load the keras models\n",
    "import sys\n",
    "from keras.models import load_model\n",
    "from six.moves import cPickle as pkl\n",
    "\n",
    "def pkl_load(f):\n",
    "  return pkl.load(f) if sys.version_info < (3, 0) else pkl.load(\n",
    "    f, encoding='latin1')\n",
    "\n",
    "model1 = load_model('data/cnn_wiki_tox_v3_model.h5')\n",
    "with open('data/cnn_wiki_tox_v3_tokenizer.pkl', 'rb') as f:\n",
    "  tokenizer1 = pkl_load(f)\n",
    "tokenizer1.oov_token = None # quick fix for version issues\n",
    "\n",
    "model2 = load_model('data/cnn_debias_tox_v3_model.h5')\n",
    "with open('data/cnn_debias_tox_v3_tokenizer.pkl', 'rb') as f:\n",
    "  tokenizer2 = pkl_load(f)\n",
    "tokenizer2.oov_token = None # quick fix for version issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define custom prediction functions so that WIT infers using keras models\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set up model helper functions:\n",
    "PADDING_LEN = 250\n",
    "\n",
    "# Get raw string out of tf.Example and prepare it for keras model input\n",
    "def examples_to_model_in(examples, tokenizer):\n",
    "  texts = [ex.features.feature['comment'].bytes_list.value[0] for ex in examples]\n",
    "  if sys.version_info >= (3, 0):\n",
    "    texts = [t.decode('utf-8') for t in texts]\n",
    "  # Tokenize string into fixed length sequence of integer based on tokenizer \n",
    "  # and model padding\n",
    "  text_sequences = tokenizer.texts_to_sequences(texts)\n",
    "  model_ins = pad_sequences(text_sequences, maxlen=PADDING_LEN)\n",
    "  return model_ins\n",
    "\n",
    "# WIT predict functions:\n",
    "def custom_predict_1(examples_to_infer):\n",
    "  model_ins = examples_to_model_in(examples_to_infer, tokenizer1)\n",
    "  preds = model1.predict(model_ins)\n",
    "  return preds\n",
    "\n",
    "def custom_predict_2(examples_to_infer):\n",
    "  model_ins = examples_to_model_in(examples_to_infer, tokenizer2)\n",
    "  preds = model2.predict(model_ins)\n",
    "  return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Define helper functions for dataset conversion from csv to tf.Examples\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Converts a dataframe into a list of tf.Example protos.\n",
    "def df_to_examples(df, columns=None):\n",
    "  examples = []\n",
    "  if columns == None:\n",
    "    columns = df.columns.values.tolist()\n",
    "  for index, row in df.iterrows():\n",
    "    example = tf.train.Example()\n",
    "    for col in columns:\n",
    "      if df[col].dtype is np.dtype(np.int64):\n",
    "        example.features.feature[col].int64_list.value.append(int(row[col]))\n",
    "      elif df[col].dtype is np.dtype(np.float64):\n",
    "        example.features.feature[col].float_list.value.append(row[col])\n",
    "      elif row[col] == row[col]:\n",
    "        example.features.feature[col].bytes_list.value.append(row[col].encode('utf-8'))\n",
    "    examples.append(example)\n",
    "  return examples\n",
    "\n",
    "# Converts a dataframe column into a column of 0's and 1's based on the provided test.\n",
    "# Used to force label columns to be numeric for binary classification using a TF estimator.\n",
    "def make_label_column_numeric(df, label_column, test):\n",
    "  df[label_column] = np.where(test(df[label_column]), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Read the dataset from CSV and process it for model {display-mode: \"form\"}\n",
    "import pandas as pd\n",
    "\n",
    "# Set the path to the CSV containing the dataset to train on.\n",
    "csv_path = 'data/wiki_test.csv'\n",
    "\n",
    "# Set the column names for the columns in the CSV. If the CSV's first line is a header line containing\n",
    "# the column names, then set this to None.\n",
    "csv_columns = None\n",
    "\n",
    "# Read the dataset from the provided CSV and print out information about it.\n",
    "df = pd.read_csv(csv_path, names=csv_columns, skipinitialspace=True)\n",
    "df = df[['is_toxic', 'comment']]\n",
    "\n",
    "# Remove non ascii characters\n",
    "comments = df['comment'].values\n",
    "proc_comments = []\n",
    "for c in comments:\n",
    "  try:\n",
    "    if sys.version_info >= (3, 0):\n",
    "      c = bytes(c, 'utf-8')\n",
    "    c = c.decode('unicode_escape')\n",
    "    if sys.version_info < (3, 0):\n",
    "      c = c.encode('ascii', 'ignore')\n",
    "    proc_comments.append(c.strip())\n",
    "  except:\n",
    "    proc_comments.append('')\n",
    "\n",
    "df = df.assign(comment=proc_comments)\n",
    "\n",
    "label_column = 'is_toxic'\n",
    "make_label_column_numeric(df, label_column, lambda val: val)\n",
    "\n",
    "examples = df_to_examples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc268cc7f6974e09af9e2c71a6f7fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WitWidget(config={'label_vocab': [], 'model_name_2': '2', 'model_type': 'classification', 'inference_address':…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@title Invoke What-If Tool for the data and two models (Note that this step may take a while due to prediction speed of the toxicity model){display-mode: \"form\"}\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder\n",
    "num_datapoints = 1000  #@param {type: \"number\"}\n",
    "tool_height_in_px = 720  #@param {type: \"number\"}\n",
    "\n",
    "# Setup the tool with the test examples and the trained classifier\n",
    "config_builder = WitConfigBuilder(examples[:num_datapoints]).set_custom_predict_fn(\n",
    "  custom_predict_1).set_compare_custom_predict_fn(custom_predict_2)\n",
    "\n",
    "wv = WitWidget(config_builder, height=tool_height_in_px)\n",
    "wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploration ideas\n",
    "\n",
    "- Organize datapoints by setting X-axis scatter to \"inference score 1\" and Y-axis scatter to \"inference score 2\" to see how each datapoint differs in score between the original model (1) and debiased model (2). Points off the diagonal have differences in results between the two models.\n",
    "  - Are there patterns of which datapoints don't agree between the two models?\n",
    "  - If you set the ground truth feature dropdown in the \"Performance + Fairness\" tab to \"is_toxic\", then you can color or bin the datapoints by \"inference correct 1\" or \"inference correct 2\". Are there patterns of which datapoints are incorrect for model 1? For model 2?\n",
    "\n",
    "You may want to focus on terms listed [here](https://github.com/conversationai/unintended-ml-bias-analysis/blob/master/unintended_ml_bias/bias_madlibs_data/adjectives_people.txt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
