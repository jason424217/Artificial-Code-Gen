{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily export jupyter cells to python module\n",
    "https://github.com/fastai/course-v3/blob/master/nbs/dl2/notebook2script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted evaluation.ipynb to exp/nb_evaluation.py\r\n"
     ]
    }
   ],
   "source": [
    "! python /tf/src/scripts/notebook2script.py evaluation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1562072276551,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "NqSTZm5UR9NS",
    "outputId": "5afa5e70-35ca-48cf-b255-fa6d12694551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/src/data/gpt-2\n"
     ]
    }
   ],
   "source": [
    "cd /tf/src/data/gpt-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19101,
     "status": "ok",
     "timestamp": 1562072297626,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "_wONoY04SGgL",
    "outputId": "eccda4fe-0849-4d91-879f-edc5ceac48a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fire>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (0.2.1)\n",
      "Collecting regex==2017.4.5 (from -r requirements.txt (line 2))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
      "\u001b[K     |████████████████████████████████| 604kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.21.0 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 11.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.31.1 (from -r requirements.txt (line 4))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 15.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toposort==1.5 (from -r requirements.txt (line 5))\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.11.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2.6)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K     |████████████████████████████████| 143kB 13.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.25,>=1.21.1 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/11/525b02e4acc0c747de8b6ccdab376331597c569c42ea66ab0a1dbd36eca2/urllib3-1.24.3-py2.py3-none-any.whl (118kB)\n",
      "\u001b[K     |████████████████████████████████| 122kB 13.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17 (from requests==2.21.0->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K     |████████████████████████████████| 163kB 13.2MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: regex\n",
      "  Building wheel for regex (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
      "Successfully built regex\n",
      "Installing collected packages: regex, chardet, urllib3, certifi, requests, tqdm, toposort\n",
      "  Found existing installation: regex 2019.6.8\n",
      "    Uninstalling regex-2019.6.8:\n",
      "      Successfully uninstalled regex-2019.6.8\n",
      "  Found existing installation: tqdm 4.32.2\n",
      "    Uninstalling tqdm-4.32.2:\n",
      "      Successfully uninstalled tqdm-4.32.2\n",
      "Successfully installed certifi-2019.6.16 chardet-3.0.4 regex-2017.4.5 requests-2.21.0 toposort-1.5 tqdm-4.31.1 urllib3-1.24.3\n",
      "\u001b[33mWARNING: You are using pip version 19.1.1, however version 19.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2219,
     "status": "ok",
     "timestamp": 1562072364186,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "v-FFfIovWj1P",
    "outputId": "9e48829f-e15d-4adb-96d8-0d91a34c4fd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-beta1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fire\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from statistics import median\n",
    "import argparse\n",
    "import time\n",
    "import tqdm\n",
    "from tensorflow.core.protobuf import rewriter_config_pb2\n",
    "import glob\n",
    "import pickle\n",
    "import h5py\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bQ3d7jgiXVFR"
   },
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO819gXNXG9-"
   },
   "outputs": [],
   "source": [
    "\"\"\"Byte pair encoding utilities\"\"\"\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "        return text\n",
    "\n",
    "def get_encoder(model_name, models_dir):\n",
    "    with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r') as f:\n",
    "        encoder = json.load(f)\n",
    "    with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "        bpe_data = f.read()\n",
    "    bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "    return Encoder(\n",
    "        encoder=encoder,\n",
    "        bpe_merges=bpe_merges,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y_aIf7Q7XHTy"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "61cFgIMfamTx"
   },
   "outputs": [],
   "source": [
    "class HParams():\n",
    "  n_vocab=50257\n",
    "  n_ctx=1024\n",
    "  n_embd=768\n",
    "  n_head=12\n",
    "  n_layer=12\n",
    "  \n",
    "  def __init__(self, n_vocab, n_ctx, n_embd, n_head, n_layer):\n",
    "    self.n_vocab = n_vocab\n",
    "    self.n_ctx = n_ctx\n",
    "    self.n_embd = n_embd\n",
    "    self.n_head = n_head\n",
    "    self.n_layer = n_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jpBqRQiuQRd4"
   },
   "outputs": [],
   "source": [
    "def default_hparams():\n",
    "    return HParams(\n",
    "        n_vocab=50257,\n",
    "        n_ctx=1024,\n",
    "        n_embd=768,\n",
    "        n_head=12,\n",
    "        n_layer=12,\n",
    "    )\n",
    "\n",
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(input=x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "    \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        n_state = x.shape[-1]\n",
    "        g = tf.compat.v1.get_variable('g', [n_state], initializer=tf.compat.v1.constant_initializer(1), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [n_state], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        u = tf.reduce_mean(input_tensor=x, axis=axis, keepdims=True)\n",
    "        s = tf.reduce_mean(input_tensor=tf.square(x-u), axis=axis, keepdims=True)\n",
    "        x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "        x = x*g + b\n",
    "        return x\n",
    "\n",
    "def split_states(x, n):\n",
    "    \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "    *start, m = shape_list(x)\n",
    "    return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "def merge_states(x):\n",
    "    \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "    *start, a, b = shape_list(x)\n",
    "    return tf.reshape(x, start + [a*b])\n",
    "\n",
    "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        *start, nx = shape_list(x)\n",
    "        w = tf.compat.v1.get_variable('w', [1, nx, nf], initializer=tf.compat.v1.random_normal_initializer(stddev=w_init_stdev), use_resource=False)\n",
    "        b = tf.compat.v1.get_variable('b', [nf], initializer=tf.compat.v1.constant_initializer(0), use_resource=False)\n",
    "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "        return c\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "\n",
    "def attn(x, scope, n_state, *, past, hparams):\n",
    "    assert x.shape.ndims == 3  # Should be [batch, sequence, features]\n",
    "    assert n_state % hparams.n_head == 0\n",
    "    if past is not None:\n",
    "        assert past.shape.ndims == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "    def split_heads(x):\n",
    "        # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "        return tf.transpose(a=split_states(x, hparams.n_head), perm=[0, 2, 1, 3])\n",
    "\n",
    "    def merge_heads(x):\n",
    "        # Reverse of split_heads\n",
    "        return merge_states(tf.transpose(a=x, perm=[0, 2, 1, 3]))\n",
    "\n",
    "    def mask_attn_weights(w):\n",
    "        # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "        _, _, nd, ns = shape_list(w)\n",
    "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "        b = tf.reshape(b, [1, 1, nd, ns])\n",
    "        w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "        return w\n",
    "\n",
    "    def multihead_attn(q, k, v):\n",
    "        # q, k, v have shape [batch, heads, sequence, features]\n",
    "        w = tf.matmul(q, k, transpose_b=True)\n",
    "        w = w * tf.math.rsqrt(tf.cast(v.shape[-1], w.dtype))\n",
    "\n",
    "        w = mask_attn_weights(w)\n",
    "        w = tf.nn.softmax(w, axis=-1)\n",
    "        a = tf.matmul(w, v)\n",
    "        return a\n",
    "\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        c = conv1d(x, 'c_attn', n_state*3)\n",
    "        q, k, v = map(split_heads, tf.split(c, 3, axis=2))\n",
    "        present = tf.stack([k, v], axis=1)\n",
    "        if past is not None:\n",
    "            pk, pv = tf.unstack(past, axis=1)\n",
    "            k = tf.concat([pk, k], axis=-2)\n",
    "            v = tf.concat([pv, v], axis=-2)\n",
    "        a = multihead_attn(q, k, v)\n",
    "        a = merge_heads(a)\n",
    "        a = conv1d(a, 'c_proj', n_state)\n",
    "        return a, present\n",
    "\n",
    "\n",
    "def mlp(x, scope, n_state, *, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "        h2 = conv1d(h, 'c_proj', nx)\n",
    "        return h2\n",
    "\n",
    "def block(x, scope, *, past, hparams):\n",
    "    with tf.compat.v1.variable_scope(scope):\n",
    "        nx = x.shape[-1]\n",
    "        a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "        x = x + a\n",
    "        m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "        x = x + m\n",
    "        return x, present\n",
    "\n",
    "def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "    return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "def expand_tile(value, size):\n",
    "    \"\"\"Add a new axis of given size.\"\"\"\n",
    "    value = tf.convert_to_tensor(value=value, name='value')\n",
    "    ndims = value.shape.ndims\n",
    "    return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "def positions_for(tokens, past_length):\n",
    "    batch_size = tf.shape(input=tokens)[0]\n",
    "    nsteps = tf.shape(input=tokens)[1]\n",
    "    return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "\n",
    "def model(hparams, X, past=None, scope='model', reuse=tf.compat.v1.AUTO_REUSE):\n",
    "    with tf.compat.v1.variable_scope(scope, reuse=reuse):\n",
    "        results = {}\n",
    "        batch, sequence = shape_list(X)\n",
    "\n",
    "        wpe = tf.compat.v1.get_variable('wpe', [hparams.n_ctx, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.01), use_resource=False)\n",
    "        wte = tf.compat.v1.get_variable('wte', [hparams.n_vocab, hparams.n_embd],\n",
    "                             initializer=tf.compat.v1.random_normal_initializer(stddev=0.02), use_resource=False)\n",
    "        past_length = 0 if past is None else tf.shape(input=past)[-2]\n",
    "        h = tf.gather(wte, X) + tf.gather(wpe, positions_for(X, past_length))\n",
    "\n",
    "        # Transformer\n",
    "        presents = []\n",
    "        pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "        assert len(pasts) == hparams.n_layer\n",
    "        for layer, past in enumerate(pasts):\n",
    "            h, present = block(h, 'h%d' % layer, past=past, hparams=hparams)\n",
    "            presents.append(present)\n",
    "        results['present'] = tf.stack(presents, axis=1)\n",
    "        h = norm(h, 'ln_f')\n",
    "\n",
    "        # Language model loss.  Do tokens <n predict token n?\n",
    "        h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "        logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "        logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "        results['logits'] = logits\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A_rmLotVXbbw"
   },
   "source": [
    "# Sample from Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "45t7syAbXaPb"
   },
   "outputs": [],
   "source": [
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.compat.v1.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       pred=tf.equal(k, 0),\n",
    "       true_fn=lambda: logits,\n",
    "       false_fn=lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, past=None, temperature=1, top_k=0):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model(hparams=hparams, X=tokens, past=past, reuse=tf.compat.v1.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents\n",
    "        }\n",
    "\n",
    "    def body(past, prev, output, embedding):\n",
    "        next_outputs = step(hparams, prev, past=past)\n",
    "        logits = next_outputs['logits'][:, -1, :]  / tf.cast(temperature, dtype=tf.float32)\n",
    "        logits = top_k_logits(logits, k=top_k)\n",
    "        samples = tf.random.categorical(logits=logits, num_samples=1, dtype=tf.int32)\n",
    "        return [\n",
    "            next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "            samples,\n",
    "            tf.concat([output, samples], axis=1),\n",
    "            logits,\n",
    "        ]\n",
    "\n",
    "    past, prev, output, logprobs = body(past, context, context, context)\n",
    "\n",
    "    def cond(*args):\n",
    "        return True\n",
    "\n",
    "#     _, _, tokens, embedding = tf.while_loop(\n",
    "#         cond=cond, body=body,\n",
    "#         maximum_iterations=length - 1,\n",
    "#         loop_vars=[\n",
    "#             past,\n",
    "#             prev,\n",
    "#             output,\n",
    "#             embedding\n",
    "#         ],\n",
    "#         shape_invariants=[\n",
    "#             tf.TensorShape(past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([batch_size, None]),\n",
    "#             tf.TensorShape([None, None, 768]),\n",
    "#         ],\n",
    "#         back_prop=False,\n",
    "#     )\n",
    "\n",
    "    probs = tf.math.softmax(logprobs)\n",
    "    return output, past, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2FqjqTMksna"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def load_dataset(enc, path, combine):\n",
    "    paths = []\n",
    "    if os.path.isfile(path):\n",
    "        # Simple file\n",
    "        paths.append(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # Directory\n",
    "        for i, (dirpath, _, fnames) in enumerate(os.walk(path)):\n",
    "            if i % 1000 == 0:\n",
    "                print(i)\n",
    "            for fname in fnames:\n",
    "                paths.append(os.path.join(dirpath, fname))\n",
    "                \n",
    "            if i == 10000:\n",
    "                print(\"Breaking\")\n",
    "                break\n",
    "    else:\n",
    "        # Assume glob\n",
    "        paths = glob.glob(path)\n",
    "\n",
    "    token_chunks = []\n",
    "    raw_text = ''\n",
    "    for i, path in enumerate(tqdm.tqdm(paths)):\n",
    "#         if 'after.java' not in path:\n",
    "#             continue\n",
    "\n",
    "        try:\n",
    "            with open(path, 'r') as fp:\n",
    "                raw_text += fp.read()\n",
    "#             if len(raw_text) > 35000: continue\n",
    "            tokens = raw_text# np.stack(enc.encode(raw_text))\n",
    "            token_chunks.append(tokens)\n",
    "            raw_text = ''\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if i >= 1000:\n",
    "            break\n",
    "    return token_chunks\n",
    "\n",
    "def binary_search(f, lo, hi):\n",
    "    if f(lo) or not f(hi):\n",
    "        return None\n",
    "    while hi > lo + 1:\n",
    "        mid = (lo + hi) // 2\n",
    "        if f(mid):\n",
    "            hi = mid\n",
    "        else:\n",
    "            lo = mid\n",
    "    return hi\n",
    "\n",
    "\n",
    "class Sampler(object):\n",
    "    \"\"\"Fairly samples a slice from a set of variable sized chunks.\n",
    "\n",
    "    'Fairly' means that the distribution is the same as sampling from one concatenated chunk,\n",
    "    but without crossing chunk boundaries.\"\"\"\n",
    "\n",
    "    def __init__(self, chunks, seed=None):\n",
    "        self.chunks = chunks\n",
    "        self.total_size = sum(chunk.shape[0] for chunk in chunks)\n",
    "        self.boundaries = [0]\n",
    "        for i in range(len(chunks)):\n",
    "            self.boundaries.append(self.boundaries[-1] + chunks[i].shape[0])\n",
    "        self.rs = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample(self, length):\n",
    "        assert length < self.total_size // len(\n",
    "            self.chunks\n",
    "        ), \"Dataset files are too small to sample {} tokens at a time\".format(\n",
    "            length)\n",
    "        while True:\n",
    "            index = self.rs.randint(0, self.total_size - length - 1)\n",
    "            i = binary_search(lambda j: self.boundaries[j] > index, 0,\n",
    "                              len(self.boundaries) - 1) - 1\n",
    "            if self.boundaries[i + 1] > index + length:\n",
    "                within_chunk = index - self.boundaries[i]\n",
    "                return self.chunks[i][within_chunk:within_chunk + length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLkRBQSysTKq"
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self, dataset, model_name, combine, batch_size, learning_rate, optimizer, noise, top_k, top_p, run_name, sample_every, sample_length, sample_num, save_every, val_dataset, val_batch_size, val_batch_count, val_every, pretrained, iterations):\n",
    "        self.dataset = dataset\n",
    "        self.model_name = model_name\n",
    "        self.combine = combine\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.noise = noise\n",
    "        self.top_k = top_k\n",
    "        self.top_p = top_p\n",
    "        self.run_name = run_name\n",
    "        self.sample_every = sample_every\n",
    "        self.sample_length = sample_length\n",
    "        self.sample_num = sample_num\n",
    "        self.save_every = save_every\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.val_batch_count = val_batch_count\n",
    "        self.val_every = val_every\n",
    "        self.pretrained = pretrained\n",
    "        self.iterations = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "                dataset=\"/tf/src/data/methods/DATA00M_[god-r]/train\",\n",
    "                model_name=\"117M\",\n",
    "                combine=50000,\n",
    "                batch_size=1, # DO NOT TOUCH. INCREASING THIS WILL RAIN DOWN HELL FIRE ONTO YOUR COMPUTER.\n",
    "                learning_rate=0.00002,\n",
    "                optimizer=\"sgd\",\n",
    "                noise=0.0,\n",
    "                top_k=1,\n",
    "                top_p=0.0,\n",
    "                run_name=\"run1\",\n",
    "                sample_every=100,\n",
    "                sample_length=1023,\n",
    "                sample_num=1,\n",
    "                save_every=1000,\n",
    "                val_dataset=None,\n",
    "                val_batch_size=1,\n",
    "                val_batch_count=40,\n",
    "                val_every=100,\n",
    "                pretrained=True,\n",
    "                iterations=200000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/972771 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1001"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = get_encoder(args.model_name, \"models\")\n",
    "data_set = load_dataset(enc, args.dataset, args.combine)\n",
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_SIZE = len(data_set)\n",
    "TRN_SET_SIZE = int(DATA_SET_SIZE * 0.8)\n",
    "VAL_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "TST_SET_SIZE = int(DATA_SET_SIZE * 0.1)\n",
    "\n",
    "trn_set = data_set[:TRN_SET_SIZE]\n",
    "val_set = data_set[TRN_SET_SIZE:TRN_SET_SIZE + VAL_SET_SIZE]\n",
    "tst_set = data_set[-TST_SET_SIZE:]\n",
    "len(trn_set), len(val_set), len(tst_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probs(chkpt_path, ):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Supervised Pre-Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 705262,
     "status": "error",
     "timestamp": 1562073894102,
     "user": {
      "displayName": "Nathan Cooper",
      "photoUrl": "",
      "userId": "15284233239426922637"
     },
     "user_tz": 300
    },
    "id": "cfjs2UHNkN5J",
    "outputId": "0a2ea262-c6af-4ac5-b102-80e1e417b19f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "'lang' 'java' 0.055041924\n",
      "'.' '.' 0.9661884\n",
      "'lang' 'lang' 0.7470425\n",
      "'.' '.' 0.9985688\n",
      "'String' 'Override' 0.53475684\n",
      "').' '\\n' 0.012239664\n",
      "' ' 'public' 0.9716119\n",
      "' void' ' void' 0.32525918\n",
      "' on' ' leaf' 0.556629\n",
      "'Det' 'Set' 0.02382625\n",
      "'(' 'Entry' 0.55445194\n",
      "'(' 'Node' 0.78121585\n",
      "'(' '(' 0.7361973\n",
      "'java' 'final' 0.84545255\n",
      "' java' ' java' 0.7972387\n",
      "'.' '.' 0.99978083\n",
      "'lang' 'lang' 0.91412145\n",
      "'.' '.' 0.9995895\n",
      "'Boo' 'Object' 0.010253676\n",
      "' cell' ' value' 0.0041451175\n",
      "')' ')' 0.8313284\n",
      "' throws' ' throws' 0.25968435\n",
      "' java' ' java' 0.8929419\n",
      "'.' '.' 0.9993957\n",
      "'lang' 'io' 0.9405779\n",
      "'.' '.' 0.9993592\n",
      "'Output' 'IO' 0.0079289535\n",
      "'Exception' 'Exception' 0.99953914\n",
      "',' ' {' 0.30821943\n",
      "'\\n' '\\n' 0.9941788\n",
      "'public' ' ' 0.2600353\n",
      "' ' ' ' 0.99484235\n",
      "' ' ' ' 0.99009025\n",
      "' android' ' final' 0.0068402616\n",
      "' java' ' org' 0.86953694\n",
      "'.' '.' 0.99505275\n",
      "'spring' 'op' 0.03596117\n",
      "'itan' 'end' 0.006950536\n",
      "'em' 'ay' 0.031906605\n",
      "'.' 'light' 0.8163279\n",
      "'.' '.' 0.9655592\n",
      "'com' 'yang' 0.025405843\n",
      "'a' 'tools' 0.0051768916\n",
      "'.' '.' 0.99439764\n",
      "'op' 'yang' 0.026211433\n",
      "'t' '.' 0.0058424366\n",
      "'tools' 'model' 0.10272535\n",
      "'.' '.' 0.9862708\n",
      "'model' 'api' 0.10472913\n",
      "'.' '.' 0.9811937\n",
      "'Tree' 'Le' 0.015551819\n",
      "'af' 'af' 0.98209524\n",
      "'Node' 'List' 0.2843696\n",
      "'Entry' 'Sche' 0.24838793\n",
      "'ma' 'ma' 0.87793386\n",
      "' table' 'Node' 0.06920708\n",
      "' table' ' schema' 0.045254502\n",
      "' =' ' =' 0.54317826\n",
      "' new' ' tracker' 0.54830915\n",
      "'Group' '.' 0.0026402096\n",
      "'get' 'leaf' 0.59909135\n",
      "'set' 'Set' 0.06481014\n",
      "'Entry' 'Entry' 0.7761947\n",
      "'Node' 'Node' 0.9718563\n",
      "'().' '();' 0.022574067\n",
      "'\\n' '\\n' 0.99940133\n",
      "' ' ' ' 0.9996698\n",
      "' ' ' ' 0.9999465\n",
      "' ' ' ' 0.99422354\n",
      "' for' ' final' 0.09399663\n",
      "' org' ' org' 0.8383184\n",
      "'.' '.' 0.99801314\n",
      "'op' 'op' 0.9551861\n",
      "'end' 'end' 0.99764484\n",
      "'ay' 'ay' 0.9999126\n",
      "'light' 'light' 0.9998031\n",
      "'.' '.' 0.99817896\n",
      "'yang' 'yang' 0.99958783\n",
      "'tools' 'tools' 0.9794168\n",
      "'.' '.' 0.9977888\n",
      "'yang' 'yang' 0.9999471\n",
      "'.' '.' 0.9696895\n",
      "'model' 'data' 0.9909991\n",
      "'.' '.' 0.99192446\n",
      "'Base' 'cod' 0.014131812\n",
      "'ec' 'ec' 0.57352537\n",
      "'s' '.' 0.16168736\n",
      "'Data' 'g' 0.09242006\n",
      "'ad' 'son' 0.02308551\n",
      "'.' '.' 0.9504918\n",
      "'cod' 'JSON' 0.013941695\n",
      "'Field' 'Cod' 0.039072994\n",
      "'ec' 'ec' 0.9937488\n",
      "'Cod' '<' 0.0071145906\n",
      "'String' 'java' 0.08206381\n",
      "'.' '.' 0.99359703\n",
      "'sql' 'lang' 0.00051028415\n",
      "'.' '.' 0.9978909\n",
      "'Integer' 'Object' 0.06097368\n",
      "'>' '>' 0.6527497\n",
      "' data' ' codec' 0.35168073\n",
      "'Info' ' =' 0.000990848\n",
      "' tracker' ' codec' 0.00931855\n",
      "'s' 's' 0.069893755\n",
      "'.' '.' 0.9546365\n",
      "'get' 'cod' 0.6802062\n",
      "'ec' 'ec' 0.970234\n",
      "'S' 'For' 0.009320935\n",
      "'Object' '(' 0.009212245\n",
      "'sche' 'sche' 0.053536452\n",
      "'ma' 'ma' 0.9795257\n",
      "'.' ');' 0.04821047\n",
      "'\\n' '\\n' 0.99984014\n",
      "' ' ' ' 0.9998031\n",
      "' ' ' ' 0.99999475\n",
      "' ' ' ' 0.9987852\n",
      "' final' ' context' 0.50189614\n",
      "'.' '.' 0.6580162\n",
      "'write' 'em' 0.020386077\n",
      "'it' 'itting' 0.7556467\n",
      "'(' 'Child' 0.4405764\n",
      "'Node' '(' 0.019693585\n",
      "'context' 'cod' 0.27136138\n",
      "'ec' 'ec' 0.99873155\n",
      "',' 's' 0.15640718\n",
      "'.' '.' 0.6882514\n",
      "'data' 'get' 0.053674936\n",
      "'Entry' 'Sche' 0.04670604\n",
      "'ma' 'ma' 0.9775702\n",
      "'From' 'Context' 0.01058348\n",
      "'().' '(),' 0.4996738\n",
      "' encoding' ' writer' 0.0064900825\n",
      "');' ');' 0.35504517\n",
      "'\\n' '\\n' 0.9995153\n",
      "' ' ' ' 0.999846\n",
      "' ' ' ' 0.99999094\n",
      "' ' ' ' 0.9882008\n",
      "' final' ' write' 0.29390442\n",
      "'();' 'Value' 0.016549878\n",
      "'(' '(' 0.5681136\n",
      "'context' 'value' 0.3567323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████     | 1/2 [00:05<00:05,  5.75s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "');' ',' 0.40805316\n",
      "' value' ' codec' 0.02073386\n",
      "');' ');' 0.5127268\n",
      "'\\n' '\\n' 0.9982705\n",
      "' ' '}' 0.9998529\n",
      "'Context' ' void' 0.008007119\n",
      "' close' ' release' 0.009753885\n",
      "'();' 'All' 0.19108842\n",
      "'State' '()' 0.011319834\n",
      "' throws' ' {' 0.22381838\n",
      "'\\n' '\\n' 0.9381627\n",
      "' ' ' ' 0.9982497\n",
      "' ' ' ' 0.9985544\n",
      "' ' ' ' 0.9871023\n",
      "' ' ' if' 0.87490314\n",
      "' (' ' ((' 0.529652\n",
      "'android' 'm' 0.04637747\n",
      "')' 'Sensor' 0.01310583\n",
      "'.' 'Manager' 0.4004432\n",
      "'State' ')' 0.0072070207\n",
      "' !=' ' !=' 0.552459\n",
      "' null' ' null' 0.9357989\n",
      "')' ')' 0.9536101\n",
      "' {' ' {' 0.93773955\n",
      "'\\n' '\\n' 0.9957931\n",
      "' ' ' ' 0.99968266\n",
      "' ' ' ' 0.999961\n",
      "' ' ' ' 0.9999411\n",
      "' ' ' ' 0.99937195\n",
      "' ' ' ' 0.98011506\n",
      "' ' ' ' 0.9574576\n",
      "' throw' ' ' 0.002757033\n",
      "' if' ' m' 0.07120687\n",
      "'Sensor' 'Sensor' 0.89869046\n",
      "'Manager' 'Manager' 0.9726068\n",
      "'.' '.' 0.8053988\n",
      "'release' 'un' 0.3811006\n",
      "'lock' 'register' 0.49093664\n",
      "'Image' 'Listener' 0.0030364108\n",
      "'(' '(' 0.6747008\n",
      "'m' 'm' 0.5059085\n",
      "'Activity' 'Sensor' 0.0033035572\n",
      "'Manager' 'Event' 0.97064304\n",
      "'.' 'Listener' 0.15983365\n",
      "',' ');' 0.20015302\n",
      "'\\n' '\\n' 0.99897146\n",
      "' ' ' ' 0.9997825\n",
      "' ' ' ' 0.9999267\n",
      "' ' ' ' 0.9988587\n",
      "' ' ' }' 0.8103274\n",
      "'\\n' '\\n' 0.8099117\n",
      "' ' ' ' 0.9992099\n",
      "' ' ' ' 0.9996612\n",
      "' }' ' ' 0.039082985\n",
      "' m' ' if' 0.2504233\n",
      "' ((' ' ((' 0.7701111\n",
      "'m' 'm' 0.78072745\n",
      "'F' 'Sensor' 0.0051318426\n",
      "'Manager' 'Manager' 0.7817701\n",
      "')' ')' 0.82629955\n",
      "' !=' ' !=' 0.874866\n",
      "' null' ' null' 0.991492\n",
      "')' ')' 0.9924549\n",
      "' {' '\\n' 0.9435337\n",
      "' ' ' ' 0.9981481\n",
      "' ' ' ' 0.9999944\n",
      "' ' ' ' 0.99998844\n",
      "' ' ' ' 0.9885091\n",
      "' ' ' ' 0.8527997\n",
      "' ' ' ' 0.55408806\n",
      "' ' ' ' 0.15822248\n",
      "' org' ' m' 0.016517991\n",
      "'Sensor' 'Sensor' 0.9803033\n",
      "'Manager' 'Manager' 0.9784761\n",
      "'.' ' =' 0.9784944\n",
      "' null' ' null' 0.18204334\n",
      "';' ';' 0.9960097\n",
      "'\\n' '\\n' 0.99917704\n",
      "' ' ' ' 0.9996302\n",
      "' ' ' ' 0.9998683\n",
      "' ' ' ' 0.9949091\n",
      "' ' ' ' 0.508482\n",
      "'}' '\\n' 0.0016630129\n",
      "' ' ' ' 0.9995683\n",
      "' ' ' ' 0.9999193\n",
      "' ' ' ' 0.89870703\n",
      "' m' ' if' 0.23948671\n",
      "' (' ' ((' 0.15721096\n",
      "'m' 'm' 0.8601795\n",
      "'Sensor' 'Camera' 0.8923804\n",
      "'View' ')' 0.052279796\n",
      "' !=' ' !=' 0.9366099\n",
      "' null' ' null' 0.99666506\n",
      "')' ')' 0.9843338\n",
      "'\\n' ' {' 0.73811764\n",
      "'\\n' '\\n' 0.98700327\n",
      "' ' ' ' 0.9999168\n",
      "' ' ' ' 0.99999225\n",
      "' ' ' ' 0.99998987\n",
      "' ' ' ' 0.99998045\n",
      "' ' ' ' 0.99993837\n",
      "' ' ' ' 0.9997993\n",
      "' ' ' ' 0.9985898\n",
      "' ' ' m' 0.6506966\n",
      "'Camera' 'Camera' 0.97541225\n",
      "' =' '.' 0.8014618\n",
      "'un' 'set' 0.11617431\n",
      "'C' 'Preview' 0.038427204\n",
      "'Count' 'Callback' 0.0054713134\n",
      "'(' '(' 0.8089076\n",
      "'new' 'null' 0.15786943\n",
      "');' ');' 0.7912845\n",
      "'\\n' '\\n' 0.9992617\n",
      "' ' ' ' 0.99987614\n",
      "' ' ' ' 0.9999571\n",
      "' ' ' ' 0.9990044\n",
      "' }' ' ' 0.2667322\n",
      "' ' ' ' 0.24020697\n",
      "' ' ' ' 0.41807175\n",
      "' if' ' ' 0.049901523\n",
      "' m' ' m' 0.7775028\n",
      "'Camera' 'Camera' 0.973766\n",
      "'.' '.' 0.82729363\n",
      "'set' 'release' 0.73639697\n",
      "'(' '();' 0.02898813\n",
      "'\\n' '\\n' 0.99878424\n",
      "' ' ' ' 0.9998122\n",
      "' ' ' ' 0.9999479\n",
      "' ' ' ' 0.9989177\n",
      "' }' ' ' 0.5820958\n",
      "' }' ' ' 0.71405184\n",
      "' }' ' ' 0.619904\n",
      "' }' ' ' 0.4643289\n",
      "' m' ' m' 0.585228\n",
      "'Camera' 'Camera' 0.9353935\n",
      "'.' ' =' 0.8209086\n",
      "' null' ' null' 0.25443617\n",
      "';' ';' 0.99699056\n",
      "'\\n' '\\n' 0.999102\n",
      "' ' ' ' 0.9996389\n",
      "' ' ' ' 0.9998952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.50s/it]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' ' ' ' 0.99779534\n",
      "' }' ' }' 0.3917499\n",
      "'\\n' '\\n' 0.92069364\n",
      "' ' '}' 0.99941397\n",
      "dict_keys(['.', 'lang', ' void', '(', ' java', ')', ' throws', 'Exception', '\\n', ' ', 'af', 'ma', ' =', 'Entry', 'Node', ' org', 'op', 'end', 'ay', 'light', 'yang', 'tools', 'ec', '>', 's', 'sche', ');', ' !=', ' null', ' {', 'Sensor', 'Manager', 'm', ' ((', ';', 'Camera', ' m', ' }'])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "MAX_CHUNK = 1024\n",
    "\n",
    "def interact_model(\n",
    "    model_name='117M',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=0,\n",
    "    models_dir='models',\n",
    "    ds=[]\n",
    "):\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = get_encoder(\"117M\", \"models\")\n",
    "    hparams = default_hparams()\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams.n_ctx // 2\n",
    "    elif length > hparams.n_ctx:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        output, past, logits = sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            past=None,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "        \n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "        \n",
    "        mean_probs = {}\n",
    "        print(mean_probs)   \n",
    "        for method in tqdm.tqdm(ds[:2]):\n",
    "            enc_meth = enc.encode(method)\n",
    "            rshft = enc_meth[:-1]\n",
    "            lshft = enc_meth[1:]\n",
    "            context_tokens = []\n",
    "            for i, tok in enumerate(rshft):\n",
    "                context_tokens.append(tok)\n",
    "                if len(context_tokens) == MAX_CHUNK + 1:\n",
    "                    context_tokens.pop(0)\n",
    "               \n",
    "                out, p, probs = sess.run([output, past, logits], feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                }, options = tf.compat.v1.RunOptions(report_tensor_allocations_upon_oom = True))\n",
    "                out = out[:, -1]\n",
    "                \n",
    "                print(repr(enc.decode([out[0]])), repr(enc.decode([lshft[i]])), probs[0][out[0]])\n",
    "                if out[0] == lshft[i]:\n",
    "                    if enc.decode([out[0]]) in mean_probs:\n",
    "                        prob, count = mean_probs[enc.decode([out[0]])]\n",
    "                        mean_probs[enc.decode([out[0]])] = (prob + probs[0][out[0]], count + 1)\n",
    "                    else:\n",
    "                        mean_probs[enc.decode([out[0]])] = (probs[0][out[0]], 1)\n",
    "#                 probs = \n",
    "        print(mean_probs.keys())\n",
    "\n",
    "interact_model(model_name='run1',\n",
    "    seed=None,\n",
    "    nsamples=1,\n",
    "    batch_size=1,\n",
    "    length=None,\n",
    "    temperature=1,\n",
    "    top_k=40,\n",
    "    models_dir='checkpoint', ds=data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'private void releaseAll() {\\n    if ((mSensorManager) != null) {\\n        mSensorManager.unregisterListener(mSensorEventListener);\\n    }\\n    if ((mSensorManager) != null)\\n        mSensorManager = null;\\n    \\n    if ((mCamera) != null) {\\n        mCamera.setPreviewCallback(null);\\n        mCamera.release();\\n        mCamera = null;\\n    }\\n}'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_probs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c2a080eb6b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_probs' is not defined"
     ]
    }
   ],
   "source": [
    "print(mean_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s'] 1\n",
      "['s', 't'] 2\n",
      "['s', 't', 'u'] 3\n",
      "['t', 'u', 'f'] 3\n"
     ]
    }
   ],
   "source": [
    "MAX_CHUNK = 3\n",
    "method = 'stuff'\n",
    "rshft = method[:-1] # do this on the actual encoding, not plaintext\n",
    "lshft = method[1:]  # do this on the actual encoding, not plaintext\n",
    "\n",
    "# enc_meth = enc.encode(method)\n",
    "context = []\n",
    "for tok in rshft:\n",
    "    context.append(tok)\n",
    "    if len(context) == MAX_CHUNK + 1:\n",
    "        context.pop(0)\n",
    "    print(context, len(context))\n",
    "        # pred, probs <= model(context)\n",
    "        # if pred equals lshft[something] then update dict with prob for pred tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s']\n",
      "['s', 't']\n",
      "st\n",
      "['t']\n",
      "['t', 'u']\n",
      "tu\n",
      "['u']\n",
      "['u', 'f']\n",
      "uf\n"
     ]
    }
   ],
   "source": [
    "MAX_CHUNK = 2\n",
    "method = 'stuff'\n",
    "rshft = method[:-1] # do this on the actual encoding, not plaintext\n",
    "lshft = method[1:]  # do this on the actual encoding, not plaintext\n",
    "\n",
    "# enc_meth = enc.encode(method)\n",
    "for i in range(len(rshft)):\n",
    "    chunk = rshft[i:MAX_CHUNK + i]\n",
    "    context = []\n",
    "    for tok in chunk:\n",
    "        context.append(tok)\n",
    "        print(context)\n",
    "        # pred, probs <= model(context)\n",
    "        # if pred equals lshft[something] then update dict with prob for pred tok\n",
    "    print(chunk)\n",
    "    if MAX_CHUNK + i >= len(rshft): break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API look\n",
    "mean_probs = calc_probs(file_name)\n",
    "mean_probs[\"}\"] # => display mean probability of all correctly predicted } characters"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "gpt2_tf2_new.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
