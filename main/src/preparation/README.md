## Data Preparation

### Preface and Requirements
This project was built using Python 3.7.4, but is at times run on the W&M SciClone bora cluster using Python 3.6.1. If you are trying to run the code yourself, whenever you see `python3` you may need to substitute your own Python 3 reference.<br/>
All other necessary packages/modules/resources are installed within a virtual environment.


### Github API Calls

This program uses GitHub personal access tokens, which for security reasons are not uploaded to the repository. To create a personal auth token, following the steps found here: https://help.github.com/en/articles/creating-a-personal-access-token-for-the-command-line. To efficiently utilize the tokens, each should be generated by a different GitHub account.

The token only needs to have `public_repo` access. Other accesses may be enabled but that increases the risk of exposure to associated GitHub accounts should the tokens be leaked. Tokens should be stored in a .txt file in `src/preparation/` called `personal_tokens.txt`, one token per line. A minimum of 2 tokens are required for the code to operate, more can be added to increase performance up to a theoretical upper bound that we do not know. On various runs of the code we used between 5 and 9.


### Set-Up
The project runs in virtual environments created with venv.<br/>
To create the environment for this section run `python3 -m venv cg-venv`<br/>
To activate the environment run `source cg-venv/bin/activate`<br/>
To install all necessary packages run `pip install -r requirements.txt` from within the environment <br/>
If you get the `error: invalid command 'bdist wheel'` for any packages, cancel the install, run `pip install wheel` and then run `pip install -r requirements.txt` again.

After setup the environment can be activated at any time from `main/src/preparation/` with `source cg-venv/bin/activate` and exited with `exit`


### Data Collection
TODO: Add more info about help messages here
`multi_thread_repo_finder.py` sorts through public GitHub repositories and downloads files with relevant languages into a folder structure located relative to where the code is run from. Run the script with
```
python3 multi_thread_repo_finder.py
```


### Data Collection With HPC (W&M Specific)
There are multiple ways to run this code using HPC, here we'll outline our method.

SSH into `<USERNAME>@bora.sciclone.wm.edu`.<br/>
Clone the repository into `sciclone/home20/<USERNAME>/`.<br/>
Change directory to `/src/preparation/`.<br/>
Modify the output location for our data structure in `multi_thread_repo_finder.py` to something suitable for large, long-term storage like `sciclone/data10/<USERNAME>/`.<br/>
Run `chmod +x readjob` to make our job file an executable.<br/>
Run `qsub readjob` to submit the job.


### Create Method Level Data
#### Python, Java, JavaScript and C++
Four of the parsers (Python, Java, JavaScript and C++) are implemented based on Tree-sitter (we use the py-tree-sitter library). The compiled library is under `main/src/preparation/parsers/`. The library requires Python 3.7.4.
To run the parsers on the raw repo data, on command line pass the language-level output_file_directory_path:
```
python raw_<LANGUAGE>_parser.py <PATH_TO_LANGUAGE_FOLDER>
```
If the data has been already been partitioned into training, test, and validation folders, etc., use the partition parsers:
```
python partition_<LANGUAGE>_parser.py <PATH_TO_LANGUAGE_FOLDER>
```
Ensure that there is not a backslash at the end of the path.

The `methods` directory will be generated under each repo folder and the parsed function files will appear there.

### Create Testbeds
A testbed is a file filled with artificial noise by shuffling the lines in normal files.  To create a testbed of 100 files, on the command line pass the path to the test partition for that language from `main/src/preparation/parsers/`.
```
python testbed_generator_files.py <PATH_TO_LANGUAGE_FOLDER/test>
```
To create a testbed fo 100 methods, on the command line pass the path to the test partition for that language from `main/src/preparation/parsers/`.
```
python testbed_generator_methods.py <PATH_TO_LANGUAGE_FOLDER/test>
```
These testbeds, when given to the model, should return garbled and garbage source code because they are garbage.

### Construct Dataframes
Prepare the gathered and sorted data into 30 pandas dataframes using `src/preparation/pandas_dataframe_generator.py`. The program takes in 2 to 4 arguments. The first 2 are necessary for the code to run, the last 2 allow you to focus on a particular section of the data. This helps to catch mistakes early and helps accommodate computers with lower ram. The arguments are 1: the path to the partition data folder. 2: either 'file' or 'function', giving the level of data you want turned into dataframes. 3(optional): the name of a specific language folder you want the code to focus on. 4(optional): the name of a specific subset folder you want the code to focus on. The following two commands will construct all 30 dataframes.
```
python3 pandas_dataframe_generator.py <PATH_TO_PARTITION_FOLDER> file
```
```
python3 pandas_dataframe_generator.py <PATH_TO_PARTITION_FOLDER> function
```
The following two commands are examples that would process a particular language or language subset
```
python3 pandas_dataframe_generator.py <PATH_TO_PARTITION_FOLDER> function scala
```
```
python3 pandas_dataframe_generator.py <PATH_TO_PARTITION_FOLDER> function cpp validate
```
